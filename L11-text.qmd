---
title: "Machine learning with text"
author:
- affiliation: University of Pennsylvania
  email: gridge@upenn.edu
  name: Greg Ridgeway
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    theme: 
      dark: darkly
      light: default
    toc: true
    html-math-method: mathjax
  pdf:
    toc: true
    exclude: true
    prefer-html: true
number-sections: true
editor_options: 
  chunk_output_type: console
bibliography: G:/My Drive/docs/Greg/articles/mybib.bib
---

<!-- Since heavy plotly() use need this installed, no need to call-->
<!-- install.packages("webdriver") -->
<!-- webdriver::install_phantomjs() -->

<!-- quarto render L11-text.qmd  -->
<!-- quarto render L11-text.qmd --cache-refresh -->

<!-- git commit L11-* -m "commit message" -->
<!-- git status -->
<!-- git push -->

# Introduction

In this section we will use `text2vec` to explore the language used in a collection of police reports describing officer-involved shootings (OIS). These reports contain unstructured narrative text. Our goal is to transform that text into a format we can analyze using tools from natural language processing (NLP). We will walk through a typical text analysis process: tokenizing the reports, building a vocabulary, constructing a document-term matrix, and applying TF-IDF to highlight the most distinctive terms. Along the way, we will also examine co-occurrence patterns.

To start, we are going to need a couple of R packages to facilitate our work. `text2vec` will do most of the work converting the documents into a form of data that we can analyze.

```{r}
#| warning: false
#| message: false
library(kableExtra)

library(dplyr)
library(tidyr)
library(stringr)
library(crayon)
library(caret)

library(text2vec)
library(Matrix)

library(glmnet)
```

As for the source of our documents, the Philadelphia Police Department posts [reports](https://www.phillypolice.com/accountability/ois/) on each officer-involved shooting. I have pulled the data off their website and packaged it into an .RData file. Loading it will create the data frame `ois`. Details on how to pull the data off of the PPD website are part of my [R4crim collection](https://github.com/gregridgeway/R4crim?tab=readme-ov-file) of scripts.

```{r}
load("data/PPD OIS.RData")
ois <- ois |>
  mutate(fatal = grepl("1 Killed|^Killed$", subInjury) |> as.numeric())
ois |> select(-text) |> head()
```

The data include an incident ID, the date of the shooting, the address and coordinates where the shooting occurred, and a URL to the incident report. There is also a column called `text` containing the full text of the officer-involved shooting report. Some can be long, but here's one that hits close to home as an example.
```{r}
#| results: hide
ois |> filter(id=="16-30") |> select(text) |> unlist() |> cat()
```

::: {.callout-note title="Narrative from OIS Report 16-30"}
```{r, results = 'asis', echo = FALSE}
ois |>
  filter(id == "16-30") |>
  pull(text) |>
  cat()
```
:::

With this set of `r nrow(ois)` reports, we will use a variety of data cleaning methods and machine learning methods to try to make sense of these documents.

# Turning text into data with `text2vec`

To transform the text into a form that is better suited for analysis, we need to go through a number of steps. Part of the reason `text2vec` is popular is that it can handle large collections of documents. To make the tasks computational efficient there are a number of steps to work through in order to get a usable dataset.

We start by create a "tokenizer," a process that breaks raw text into individual units like words, phrases, or symbols—called (the "tokens"), the basic building blocks for text analysis. The `itoken()` function in the `text2vec` package creates an iterator over a collection of text documents, preparing them for efficient text processing. Instead of transforming all text at once, `itoken()` streams the documents one at a time, making it well-suited for handling large sets of documents. It "tokenizes" each document using =either a built-in default or a custom tokenizer (which we will do) and produces a structure that can be passed on to other functions that will tidy up the collection of tokens and convert them into a dataset. Because it does not store all tokenized text in memory, `itoken()` enables fast and memory-efficient text analysis workflows.

```{r}
# Create an iterator over tokens
#   tokens does not actually store data
#   just an efficient means for looping over documents
tokens <- itoken(ois$text,
                 progressbar = FALSE,
                 ids = ois$id)
# this gets the next batch of documents... for me around 14 documents
a <- tokens$nextElem()
a$ids
a$tokens |> sapply(head)
```

You can see that so far `itoken()` has pulled in `r length(a$tokens)` documents and chopped them up into individual words. Notice that the collection of words have some undesirable quirks. For example, there are

-   numbers that we probably do not really care about
-   unimportant words like "of" (known as "stop words")
-   Line feeds `\n` in between two words

`create_vocabulary()` and `prune_vocabulary()` help us to trim down the words to the ones that we actually care about. `create_vocabulary()` allows us to provide a list of stop words to remove. `stopwords("en")` is just such a list. Here are just a few of the 175 English stop words 
```{r}
stopwords::stopwords("en") |> head(20)
```
There are lists for several other languages as well, Italian, for example.
```{r}
stopwords::stopwords("it") |> head(20)
```
We can also ask consider pairs of words in addition to single words (`ngram=1:2`). This allows word phrases like "police officer" and "pit bull" to be considered as words.

`prune_vocabulary()` trims down words from our vocabulary that are probably not particularly useful 

-  words that few documents use (too rare)
-  words that too many documents use (too common) 

We can add in some other filters too, like only using words that are at least three letters and dropping any words that have numbers in them (like 3pm or 9mm).

```{r}
# reset to beginning
tokens <- itoken(ois$text,
                 progressbar = FALSE,
                 ids = ois$id)

# Build vocabulary
#   these are the collection of words that I care about
#   skip stopwords (the, of, in, ...)
#   include two word phrases (2-gram or bigram), 
#      "police officer", "full uniform", "black male", "drop weapon", "pit bull"
#   skip words that only show up in fewer than 10 documents
#   skip words that are in the majority of documents (police?, discharged?)
vocab <- tokens |>
  create_vocabulary(stopwords = stopwords::stopwords("en"),
                    ngram = 1:2) |>
  prune_vocabulary(term_count_min = 10,
                   doc_proportion_max = 0.5) |>
  filter(nchar(term) >= 3) |>
  filter(!grepl("[0-9]", term))

# space_tokenizer(), default, keeps a lot of punctuation
vocab
```

Let's make our own tokenizer instead of using the default. As we see, the default (`space_tokenizer()`) often retains punctuation, symbols, or other strange features that dilute or fragment our vocabulary. Customizing the tokenizer allows us to tailor the cleaning process to the structure and quirks of the officer-involved shooting reports. The function that we will create, `oisTokenizer()`, is a custom tokenizer designed to clean and standardize the raw text from officer-involved shooting reports before further text analysis. It converts the text to lowercase, removes common punctuation patterns (like those in abbreviations such as “3 p.m.”), strips out unusual or inconsistent symbols (such as smart quotes, parentheses, and hash symbols), and splits the text into individual tokens using whitespace as the delimiter. 

After tokenization, it will also apply "stemming". Stemming is a text preprocessing technique that reduces words to their root or base form by removing common suffixes. For example, "running", "runner", and "runs" might all be reduced to "run", allowing the model to treat these variations as the same underlying concept. The `SnowballC` package has a handy `wordStem()` function in it. Let's test it out on a few words.
```{r}
c("dog","dogs","office","officer","officers","police","policy","policies") |>
  SnowballC::wordStem(language = "en")
```
Conveniently, it makes both "dog" and "dogs" simply "dog". However, note that it also makes "office", "officer", and "officers" all simplified to "office"... maybe not ideal. Since our text will have a lot of "officer" and "officers" and probably very few, if any, "office", we will need to remember that this stemming has reduced our "police officers" to "polic_offic". You may see terms like "offic_offic" or "offend_offend". Typically this occurs because officer or offender was the last word in one sentence and the first non-stop word in the next sentence (e.g. "The suspect ran from the officer. The officer said 'stop!'").

```{r}
# our own custom tokenizer
oisTokenizer <- function(text)
{
  text |>
    tolower() |>
    # remove abbreviation punctuation (like 3 p.m.)
    gsub("([A-z])[,.]+", "\\1", x=_) |>
    # remove some weird symbols
    gsub("[“”()#]", "", x=_) |>
    # no smart quotes
    gsub("’", "'", x=_) |>
    # split any words with \n, \t, \r between them
    strsplit("\\s+") |>
    # stemming
    lapply(SnowballC::wordStem, language = "en")
}
```

Now we can rerun our documents through our new tokenizer.
```{r}
# reset to beginning
#    now using our oisTokenizer()
tokens <- itoken(ois$text,
                 tokenizer = oisTokenizer,
                 progressbar = TRUE,
                 ids = ois$id)

vocab <- tokens |>
  create_vocabulary(stopwords = stopwords::stopwords("en"),
                    ngram = 1:2) |>
  prune_vocabulary(term_count_min = 10,
                   doc_proportion_min = 0.05,
                   doc_proportion_max = 0.5) |>
  filter(nchar(term) >=3) |>
  filter(!grepl("[0-9]", term))  |>
  #   Drop some specific terms that are not useful (e.g. philadelphia)
  filter(!term %in% c("philadelphia_polic","philadelphia",
                     "inform_da","da_offic","incid_inform","inform_ppd",
                     "inform_post","post","officer-involv",
                     "officer-involv_shoot"))

vocab
```
Now we have a collection of words and word phrases gathered from our documents. Note that it includes so two word phrases (bigrams) with the two stemmed words combined with an underscore between them.

## Creating a document-term matrix (DTM)

Our next destination is to create a "document-term matrix" (DTM). A DTM is a matrix representation of a collection of text documents, where each row corresponds to a document and each column corresponds to a unique term (a word or phrase) from the collection of documents. The values in the matrix typically reflect the number of times each term appears in each document. A DTM transforms the unstructured text into a format that machine learning models can work with.

The first step to getting to a DTM with `text2vec` is to create a "vectorizer". A vectorizer translates tokenized text into a numeric matrix format, such as a DTM. `vocab_vectorizer()` creates a function that will take batches of documents, compare them to the vocabulary we built, and produce the associated components of the DTM.

```{r}
# Create a vectorizer
#   helper function to convert streams of text into DTM
vectorizer <- vocab_vectorizer(vocab)
# Let's see what this function looks like!
vectorizer
```
It is a little difficult to interpret, but we can see that it is going to take in a iterator over our tokenized documents and produce something that will (hopefully!) be useful. Let's give it a try.
```{r}
# Create the document-term matrix (DTM)
#   row represents a document
#   column represents a unique term (word or phrase)
#   cell contains the count (or weight) of that term in the document
oisDTM <- create_dtm(tokens, vectorizer)
oisDTM[65:74, 415:424] |> as.matrix() |> t()
```
We have a DTM! I have picked a few interesting rows and columns. I also transposed the DTM so it is more readable, but typically the rows are documents and columns are terms. You can see a few non-zero counts in this matrix. These indicate which documents include these terms and how many times that term appears in the document.

Let's explore further.
```{r}
# number of documents and words
dim(oisDTM)
# rows represent individual OIS shooting reports
rownames(oisDTM)[1:5]
# columns are the words/phrases
colnames(oisDTM)[1:10]     # feature names
# how many vocab words in document?
rowSums(oisDTM)
# how many documents have these words?
colSums(oisDTM)[1:20]

# Most common words?
colSums(oisDTM) |>
  sort(decreasing = TRUE) |>
  head(10)
```

## Term Frequency–Inverse Document Frequency

While raw term counts in a document-term matrix tell us how often each word appears, they do not account for how informative or distinctive those words are across the entire collection of documents. Common words like "officer" or "incident" may appear frequently in every report, but they are not useful for distinguishing one document from another. Term frequency-inverse document frequency (TF-IDF) improves on this by weighting terms based on how frequently they appear in a specific document and how rare they are across all documents. This highlights terms that are both common within a document and uncommon elsewhere, making them more meaningful for identifying the unique content of each report.

Term Frequency-Inverse Document Frequency (TF-IDF) gives weights to words in a document in a way that balances:

1.  *Term Frequency* (TF): This word must be important in this document

-   The more a word appears in a document, the more likely it is to be relevant to the document's content
-   If the word "shooting" appears 12 times in a police report, it is probably central to that document

2.  *Inverse Document Frequency* (IDF): But if it appears in every document, it is not very informative

-   Common words like "officer", "incident", or "said" might appear everywhere
-   IDF *downweights* those high-frequency but low-discrimination terms
-   It prefers terms that help *distinguish* one document from others

The formula for TF-IDF for document $i$ and term $j$:

$$
\mathrm{tfidf}_{ij} = \mathrm{TF}_{ij}\log\frac{N}{\mathrm{DF}_j}
$$ 
where 

-  $\mathrm{TF}$ is the number of times term $j$ appears in document $i$. It measures the importance of the term within a document 
-  $N$ = total number of documents 
-  $\mathrm{DF}_j$ = number of documents containing term $j$

$\mathrm{IDF}_{ij}=\log\frac{N}{\mathrm{DF}_j}$ captures the rarity across documents. Note that if a word appears in all documents then $\mathrm{tfidf}_{ij} = 0$. The combination of $\mathrm{TF}$ and $\mathrm{IDF}$ gives a measure of relevance and distinctiveness. A high $\mathrm{tfidf}_{ij}$ means a term appears often in document $i$, but rarely in other documents. It gives you terms that define a document. These are the terms that are useful for classification, clustering, or topic modeling.

### Example

Assume there are $N=100$ documents.

| Term      | TF in Doc A | DF across corpus | IDF | TF-IDF |
|-----------|-------------|------------------|-----|--------|
| "weapon"  | 5           | 10               | 2.3 | 11.5   |
| "officer" | 6           | 95               | 0.1 | 0.3    |
| "said"    | 20          | 100              | 0   | 0      |

-   "weapon" gets a high score, specific and relevant
-   "officer" is common, downweighted
-   "said" is everywhere, zeroed out

```{r}
# TF-IDF: term frequency–inverse document frequency weights
#    downweights common words that appear in many documents
#    upweights rare words that are more informative or distinctive
# TF: How often a word appears in a document
# IDF: How rare that word is across all documents
#    TF-IDF = TF × log(N / DF)
#       N = total number of documents
#       DF = number of documents containing the term
tfidf_transformer <- TfIdf$new()
oisTFIDF <- tfidf_transformer$fit_transform(oisDTM)
```

Let's take a look at those same rows and columns that we did before for the DTM. The matrix looks largely the same, just everything scaled down.
```{r}
oisTFIDF[65:74, 415:424] |> as.matrix() |> round(2) |> t()
```

Let's compare the top features.
```{r}
# View top features by TF
colSums(oisDTM) %>%
  sort(decreasing = TRUE) %>%
  head(10)

# View top features by TF-IDF
colSums(oisTFIDF) %>%
  sort(decreasing = TRUE) %>%
  head(10)
```
The TF-IDF does change which terms make the top-10 list. We see "knife" and "driver" show up and "door" and "point" drop off.

<!-- Why does "resid" show up in this list? -->
<!-- ```{r} -->
<!-- oisDTM[,"resid"] -->

<!-- # highlight the word "post" in the report -->
<!-- ois |>  -->
<!--   filter(id=="20-20") |>  -->
<!--   pull(text) |>  -->
<!--   gsub("resid", bgYellow$black("resid"), x=_) |>  -->
<!--   cat() -->
<!-- ``` -->

# Term co-occurrence matrix (TCM)

There may be some concepts that are not limited to a single word or a few adjacent words. Term co-occurrence looks for words that tend to appear close to each other in text to possibly help you expand the vocabulary to additional phrases. A term co-occurrence matrix (TCM) captures how often pairs of words appear near each other within a given window of text, such as a sentence or a few neighboring words. Unlike a document-term matrix, which represents the relationship between documents and individual terms, a TCM focuses on the relationships between terms themselves. This is useful for uncovering word associations and identifying common phrases. In our case, we will use the TCM to explore how certain words, such as "officer," "suspect," or "weapon," tend to co-occur across police shooting reports, revealing patterns that might not be visible from frequency counts alone.

When scanning through each document, setting `skip_grams_window = 5` will treat any two terms that appear within a window of 5 tokens as co-occurring. For example, if the document has the phrase "the officer shot the suspect with a weapon" and we set `skip_grams_window = 5`, then for the word "shot" it will consider "the", "officer", "the", "suspect", "with" as co-occurring terms.

We will use `create_tcm()` to create a TCM. The $(i,j)$ element of the TCM will be the number of times term $i$ occurs within 5 terms of term $j$.
```{r}
# Create a co-occurrence matrix (Feature Co-occurrence Matrix)
oisTCM <- itoken(ois$text,
                 tokenizer = oisTokenizer,
                 progressbar = FALSE,
                 ids = ois$id) |>
  create_tcm(vocab_vectorizer(vocab), 
             skip_grams_window = 5)
```

This will be a little easier to visualize if we convert to a long (rather than wide) format.
```{r}
# Convert to triplet format and extract top co-occurring pairs
oisPairs <- Matrix::summary(oisTCM) |>
  filter(i != j) |>
  rename(feature1 = i, feature2 = j, weight = x) |>
  left_join(data.frame(feature1 = 1:nrow(oisTCM),
                       term1 = colnames(oisTCM)),
            by = join_by(feature1))  |>
  left_join(data.frame(feature2 = 1:nrow(oisTCM),
                       term2 = colnames(oisTCM)),
            by = join_by(feature2)) |>
  select(-feature1, -feature2) |>
  filter(term1 != term2) |>
  filter(!str_detect(term1, fixed(term2)) & 
           !str_detect(term2, fixed(term1)))

oisPairs |>
  arrange(desc(weight)) |>
  slice_head(n = 50)
```
Much of this co-occurrence is due to the template language describing where the department is in the investigation, referrals to the district attorney, and the report offers preliminary summary.

::: {.callout-note title="Template language on the report"}
*** Information posted in the original summary reflects a preliminary understanding of what occurred at the time of the incident. This information is posted shortly after the incident and may be updated as the investigation leads to new information. The District Attorney’s Office is provided all the information from the PPD’s investigation prior to their charging decision.
:::

Further on down the list some term pairs a more interesting.
```{r}
oisPairs |>
  filter(weight >= 8 & weight <=9) |>
  arrange(desc(weight))
```
  
  
# Singular value decomposition for text

We already explored how SVD can be used to [compress images and classify emojis](https://raw.githack.com/gregridgeway/ML4SocialScience/main/L7-svd.html). Now we will explore what SVD does for text.

Since TFIDF matrices can get large, we will use the SVD implementation in the IRLBA package (Implicitly Restarted Lanczos Bidiagonalization Algorithm). The IRLBA implementation of SVD allows you to limit the number of singular vectors to compute, ignoring and never computing the rest. 

```{r}
library(irlba)
oisSVD <- irlba(t(oisTFIDF), nv = 50)
# each run of SVD can switch the signs on U and V
#    this forces sign(v[1,]) = +1, so SVD is unique
oisSVD$u <- sweep(oisSVD$u, 2, sign(oisSVD$v[1,]), `*`)
oisSVD$v <- sweep(oisSVD$v, 2, sign(oisSVD$v[1,]), `*`)
```



Let's see how many singular vectors seem important. In @fig-SVDsingularvalues we see the first two or three singular values seem large, but then they decrease quite slowly from there.
```{r}
#| label: fig-SVDsingularvalues
#| fig-cap: "Singular values from SVD of `oisTFIDF`"
#| fig-width: 5
#| fig-height: 4
plot(oisSVD$d, 
     xlab="Index of the singular value", 
     ylab="Singular value",
     ylim=c(0, max(oisSVD$d)),
     pch=16)
```

The columns of $\mathbf{U}$ are our "eigendocuments," the fundamental building blocks that the actual documents blend to form their word collections. We will take a look at the first five eigendocuments, highlighting the 10 terms with the highest weight in the left singular vectors. Note that both large positive and large negative values are important to interpret. I have added headings to each left singular vector summarizing what kinds of incidents might heavily weight this column of $\mathbf{U}$.

```{r}
#| echo: false
if (knitr::is_latex_output()) {
  header_labels <- c("Term", "$u_1$", "Term", "$u_2$",
                   "Term", "$u_3$", "Term", "$u_4$",
                   "Term", "$u_5$")
} else {
  header_labels <- c("Term", "\\(u_1\\)", "Term", "\\(u_2\\)",
                     "Term", "\\(u_3\\)", "Term", "\\(u_4\\)",
                     "Term", "\\(u_5\\)")
}
```

```{r}
# Stack the top 10 terms from each component into one long table
a <- data.frame(term=colnames(oisTFIDF),
           u1=oisSVD$u[,1],
           u2=oisSVD$u[,2],
           u3=oisSVD$u[,3],
           u4=oisSVD$u[,4],
           u5=oisSVD$u[,5])

a <- bind_rows(a |>
                 select(term, u1) |> 
                 arrange(desc(abs(u1))) |> 
                 slice_head(n = 10) |> 
                 mutate(component = "u1", u = u1),
               a |> 
                 select(term, u2) |> 
                 arrange(desc(abs(u2))) |> 
                 slice_head(n = 10) |> 
                 mutate(component = "u2", u = u2),
               a |> 
                 select(term, u3) |> 
                 arrange(desc(abs(u3))) |> 
                 slice_head(n = 10) |> 
                 mutate(component = "u3", u = u3),
               a |> 
                 select(term, u4) |> 
                 arrange(desc(abs(u4))) |> 
                 slice_head(n = 10) |> 
                 mutate(component = "u4", u = u4),
               a |> 
                 select(term, u5) |> 
                 arrange(desc(abs(u5))) |> 
                 slice_head(n = 10) |> 
                 mutate(component = "u5", u = u5)) |> 
  select(component, term, u)
```

```{r}
#| echo: false
# Escape _ in all character columns
if (knitr::is_latex_output()) {
  a <- a |>
    mutate(across(where(is.character),
                  ~ str_replace_all(.x, "_", "\\\\_")))
}
```

```{r}
#| tbl-cap: "Top 10 Terms for First 5 SVD Components"
#| label: tbl-uterms
a |> 
  group_by(component) |> 
  mutate(rank = row_number()) |> 
  ungroup() |> 
  pivot_wider(names_from = component, 
              values_from = c(term, u),
              names_sep = "_") |>
  data.frame() |>
  select(term_u1, u_u1, term_u2, u_u2, term_u3, u_u3,
         term_u4, u_u4, term_u5, u_u5) |>
  kbl(align = "rrrrrrrrrr",
        col.names = header_labels,
        digits = 2,
        escape = FALSE,
        booktabs = TRUE) |>
  add_header_above(c("Offender and dog" = 2, 
                     "Dog attack, no offender" = 2,
                     "Suspect/SWAT" = 2,
                     "Knife attacks" = 2,
                     "?" = 2),
                   escape = FALSE) |>
  kable_styling(full_width = FALSE,
                latex_options = "scale_down")
```
Note that "dog" shows up with a large absolute value in the first three left singular vectors. Even if there is a large negative value, that still means the term is important. A report of shooting a pit bull will have a large negative value for its $v_2$, signalling a heavy presence of dog, pit bull, and attack terms, but the absence of the "offender" term. The first left singular vector measure the combined presence of "dog," "offender," and "suspect" (or the absence of all three if a document's value of $v_1$ is large and negative.

Remember that the columns of $\mathbf{V}$ tell us how a particular report mixes together the eigendocuments (columns of $\mathbf{U}$) to form their TFIDF weighted terms. I am curious to explore documents involving dog attacks ($\mathbf{u}_2$) and documents with knife attacks ($\mathbf{u}_4$). The code below creates an interactive 2D map of documents on their values of $v_2$ and $v_4$. For each document, the code extracts the 10 most heavily weighted TF-IDF terms to use as hover text, giving a quick sense of the content. It also assigns each document to a category, "Likely dog attack," "Likely knife attack," or "Other," based on thresholds of $v_2$ and $v_4$. `plotly()` produces an interactive scatterplot where each point is a document, colored by category and annotated with its key terms. This visualization allows us to explore thematic variation in the reports and visually distinguish different types of incidents based on the language used.
```{r}
#| label: fig-documentMap
#| fig-cap: Plot of how documents weigh 2nd and 4th left singular vectors
#| warning: false
#| message: false

library(plotly)

# collect the 10 terms with the highest weight in each document
i <- apply(oisTFIDF, 1, order, decreasing=TRUE)
hovertext <- apply(i, 2, 
                   function(j) 
                   {
                     colnames(oisTFIDF)[j[1:10]] |>
                       paste(collapse = "\n")
                   })

# label some document types
group <- case_when(oisSVD$v[,2] > 0.1 ~ "Likely dog attack",
                   oisSVD$v[,4] > 0.1 ~ "Likely knife attack",
                   TRUE ~ "Other")
groupCols <- c("Likely dog attack" = "red",
               "Likely knife attack" = "orange",
               "Other" = "steelblue")

# make a plot with hovertext
plot_ly(x = oisSVD$v[,2],
        y = oisSVD$v[,4],
        type = "scatter",
        mode = "markers",
        text = hovertext,
        hoverinfo = "text",
        color = group,
        colors = groupCols,
        marker = list(size = 6)) |>
  layout(xaxis = list(title = "V2 - Dog attack measure"),
         yaxis = list(title = "V4 - Knife attack measure"))
```


# Clustering documents with Hartigan's k-means clustering algorithm

Hartigan's k-means algorithm is an iterative method for partitioning data into $k$ clusters by minimizing the total within-cluster sum of squares. It begins with randomly chosen cluster centers (centroids) and alternates between two steps: 

1.  assigning each data point to the nearest centroid based on Euclidean distance
2.  updating each centroid to be the mean of the points currently assigned to it

These steps repeat until the assignments no longer change significantly, indicating convergence. The algorithm is greedy and locally optimal. That is, it always reduces the the within cluster sum of squares at each iteration, but it can converge to different solutions depending on the initial centroids. Multiple runs with different random starts are often used to find a better overall solution.

Let's look at a little 2D demonstration before we run this on documents, which can be a little abstract. I have created three clusters with centers at (0,0), (3,3), and (5,5). Then I have R code that reassigns points to their nearest centroids, recomputes the cluster centroids, plots the points colored by their cluster assignment, checks for convergence, and repeats if needed.

```{r}
#| label: fig-kmeans
#| fig-show: asis 
#| warning: false
#| message: false
#| fig-cap: Demonstration of Hartigan's k-means algorithm
#| fig-width: 5
#| fig-height: 4

set.seed(20250325)
library(ggplot2)

# Simulate data from 3 clusters
df <- data.frame(x=rnorm(180, mean=c(0,3,5)),
                 y=rnorm(180, mean=c(0,3,5)))

# find three clusters
k <- 3

# pick three random points to start
old_centroid <- df |>
  slice_sample(n=k) |>
  select(x,y)

# Function to assign points to nearest center
#   compare each point to each of the three centers
#   assign the point to its closest center
assign_clusters <- function(df, centers) 
  {
    apply(df, 1, function(point) 
    {
      which.min(colSums((t(centers) - point)^2))
    })
  }

iter <- 1
repeat 
{
  # Assign points to nearest cluster
  df$cluster <- assign_clusters(df[,c("x","y")], 
                                old_centroid)

  # get centroids of newly assigned clusters
  new_centroid <- df |>
    group_by(cluster) |>
    summarize(x=mean(x), y=mean(y))
  
  # plot showing old and new centroids
  plotKmeans <- ggplot(df, aes(x = x, y = y, color = factor(cluster))) +
    geom_point(size = 2) +
    geom_point(data = old_centroid, aes(x = x, y = y), 
               shape = 21, size = 5, stroke = 2, color = "black") +
    geom_point(data = new_centroid, aes(x = x, y = y, fill=factor(cluster)), 
               shape = 21, size = 5, stroke = 2, color = "black") +
    geom_segment(
      data = bind_cols(old_centroid, new_centroid),
      aes(x = x...1, y = y...2, xend = x...4, yend = y...5),
      arrow = arrow(length = unit(0.15, "inches")),
      color = "black"
    ) +
    ggtitle(paste("K-Means Iteration", iter)) +
    labs(color="Cluster") +
    guides(fill = "none") +
    theme_minimal()
  print(plotKmeans)
  
  new_centroid <- new_centroid |> select(-cluster)

  # check if converged
  if(all(abs(new_centroid - old_centroid) < 0.001))
  {
    break
  }
  
  old_centroid <- new_centroid
  iter <- iter + 1
}

```
After 8 iterations, the centroids of the clusters do not change. That means the cluster assignments for each point will no longer changed. The k-means algorithm has converged after 8 iterations. Indeed k-means seems to have figured out the three clusters that I simulated. In practice we never really know how many real clusters there are. We need to come up with some measures that help us decide whether the number of clusters and their centroids adequately capture the data points.

The plot in @fig-totSS illustrates the total sum of squares (TSS), which measures the overall variability in the dataset. It shows each data point connected to the grand centroid, the average of all points in the data. The total sum of squares is calculated by summing the squared distances from each point to this overall centroid. This serves as a baseline measure of dispersion before we run our clustering algorithm.
```{r}
#| label: fig-totSS
#| fig-cap: Total sum of squares calculation. TSS sums all the squared distances between each point and the single centroid of the entire dataset
#| warning: false
#| message: false
tot_centroid <- df |> 
  select(x, y) |> 
  colMeans() |> 
  t() |> 
  data.frame()
plotTotCentroid <- ggplot(df, aes(x = x, y = y, 
                                  color = factor(cluster))) +
  geom_point(size = 2) +
  geom_segment(
    data = bind_cols(tot_centroid |>
                       slice(rep(1,180)), 
                     df |>
                       select(x,y)),
    aes(x = x...1, y = y...2, xend = x...3, yend = y...4),
    color = "black"
  ) +
  labs(color="Cluster") +
  theme_minimal()
print(plotTotCentroid)
```

In contrast, @fig-withinSS depicts the within-cluster sum of squares (WCSS). Each point is now connected to its respective cluster centroid, rather than the overall centroid. The sum of squared distances from each point to its assigned cluster center quantifies how compact each cluster is.
```{r}
#| label: fig-withinSS
#| fig-cap: Within cluster sum of squares calculation. WCSS sums the squared distances between each point and the center of its associated centroid
new_centroid$cluster <- 1:3

plotWithinCentroid <- ggplot(df, aes(x = x, y = y, 
                                  color = factor(cluster))) +
  geom_point(size = 2) +
  geom_segment(
    data = df |> 
      left_join(new_centroid, by=join_by(cluster)),
    aes(x = x.x, y = y.x, xend = x.y, yend = y.y),
    color = "black"
  ) +
  labs(color="Cluster") +
  theme_minimal()
print(plotWithinCentroid)
```
A useful measure of clustering quality is the proportion of variance explained by the clustering, computed as $R^2 = 1-\frac{WCSS}{TSS}$. This tells us how much of the total variability has been accounted for by the clustering structure. A higher value indicates better clustering. If this proportion is low, it may suggest that the three clusters is insufficient to capture the underlying structure of the data.

Let's put all this to work on our officer-involved shooting reports. We will define documents by their first 7 right singular vectors (columns of $\mathbf{V}$). We will actually cluster on $\mathbf{V}\boldsymbol\Sigma$. Remember that SVD normalizes all the columns in $\mathbf{V}$ (and those in $\mathbf{U}$ as well) to have length 1. That has the effect of making differences between documents basedon their values of $v_{30}$ just as large as differences on $v_1$. The singular values in $\boldsymbol\Sigma$ tell us how to weight each column based how much variance it captures in the original TF-IDF matrix. Without this weighting, each dimension would contribute equally to distance calculations during clustering, even though some components may be far more informative than others. By using $\mathbf{V}\boldsymbol\Sigma$, our clustering algorithm will more accurately group documents based on their underlying thematic content.

R has a built-in `kmeans()` function in the default `stats` package that will do all the work for us. First, we need to decide on how many clusters we should use. The code below explores the relationship between the number of clusters, $k$, and $R^2$. This is a key part of the "elbow method," a common strategy for selecting an appropriate number of clusters. By running k-means repeatedly with increasing values of $k$ (from 1 to 40 in this case), and recording the corresponding $R^2$, we can visualize how much the clustering improves as $k$ increases. A plot of $R^2$ versus $k$ typically shows a steep increase initially and then levels off. The "elbow" point, where the rate of improvement sharply slows, is often a good choice for the number of clusters. It represents a balance between underfitting and overfitting the structure in the data. Another common method is to consider more and more clusters until $R^2$ reaches 0.80 (or 0.90).

```{r}
#| fig-cap: Relationship between $k$ and $R^2$
#| label: fig-kvsse
design <- data.frame(k=1:40, R2=NA)
for(i in 2:nrow(design))
{
  # multiply by the singular values to incorporate the importance of each 
  km <- kmeans(oisSVD$v[,1:7] %*% diag(oisSVD$d[1:7]), 
               centers = design$k[i], 
               nstart = 5)
  design$R2[i] <- ifelse(i==1, 1, 
                         1 - km$tot.withinss/km$totss)
}
plot(R2~k, data=design, pch=16,
     xlab=expression(k),
     ylab=expression(R^2),
     ylim=0:1)
abline(h=0.8, col="red")
kSelect <- design |>
  filter(R2 > 0.8) |>
  slice_min(R2) |>
  pull(k)
```
In @fig-kvsse I have marked where the $R^2$ reaches 80\%. That happens when we set $k$ to `r kSelect`. With $k$ set to `r kSelect`, the clustering captures 80\% of the variation in the first seven right singular vectors.
```{r}
# run kmeans on first 10 right singular vectors (scaled by Sigma)
#   clusters similar documents
set.seed(20250325)
oisKmeans <- kmeans(oisSVD$v[,1:10] %*% diag(oisSVD$d[1:10]), 
                    centers = kSelect,
                    nstart = 5) # try 5 random starting points
```

Let's add the cluster label to our original OIS data frame so we can see if themes are identifiable from their incident descriptions.
```{r}
# add the cluster labels to the dataset
ois$cluster <- oisKmeans$cluster
ois |>
  select(id,location,date,cluster) |>
  head()
```

We need to craft some labels or definitions for these clusters. For each cluster I will compute the average of their TFIDF weights for each term in the vocabulary. I will paste together the top ten terms as cluster labels.
```{r}
hovertext <- as.matrix(oisTFIDF) |>
  as.data.frame() |>
  mutate(cluster = ois$cluster) |>
  group_by(cluster) |>
  # average the TFIDF values within each cluster
  summarize(across(everything(), mean), .groups = "drop") |>
  select(-cluster) |>
  # find the top 10 TFIDF weighted terms by cluster
  apply(1, function(w)
  {
    i <- order(w, decreasing = TRUE)
    colnames(oisTFIDF)[i[1:10]]
  }) |>
  t() |>
  # paste together the top 10 terms
  apply(1, paste, collapse=", ") |> 
  data.frame(terms = _) |>
  cbind(size = oisKmeans$size,
        cluster = 1:kSelect) |>
  arrange(desc(size))
```

Let's examine the top 10 terms for these clusters.
```{r}
hovertext |>
  kable(escape = FALSE, 
        col.names = c("Top Terms", "Size", "Cluster")) |>
  kable_styling(full_width = FALSE, 
                position = "left", 
                bootstrap_options = c("striped", "hover"))
```

The first row describes the largest cluster with `r hovertext$size[1]` documents (Cluster \#`r hovertext$cluster[1]`). It describes shootings involving a car (driver, passenger, door). The second largest cluster (Cluster \#`r hovertext$cluster[2]`) seems to describe a prototypical officer-involved shooting involving an offender, a marked police vehicle, and some mention of charges. Clusters \#`r hovertext$cluster[3]` and \#`r hovertext$cluster[4]` are the SWAT (Special Weapons and Tactics) incidents. Cluster \#`r hovertext$cluster[5]` clearly seems to be shootings of dogs, commonly pit bulls. You can pick up other themes in the remaining clusters such as knife attacks and the involvement of plainclothes police officers.

Let's plot these out based on the values of each document's first two right singular vectors ($v_1$ and $v_2$). I will color them based on their clustering and add the top 10 terms as hovertext so we have some idea about the themes. Explore the clusters in the plot and see if you can extra some of those themes.
```{r}
#| label: fig-clusteredDocuments
#| fig-cap: Plot of document by first two right singular vectors, colored by cluster
plot_ly(
  x = oisSVD$v[,1],
  y = oisSVD$v[,2],
  type = "scatter",
  mode = "markers",
  color = ois$cluster,
  hoverinfo = "text", # don't show the coordinates, only hoverinfo
  text = hovertext$terms[ois$cluster] |>
    gsub(", ", "\n", x=_), 
  marker = list(size = 6)
) |>
  layout(
    xaxis = list(title = "SVD Dimension 1"),
    yaxis = list(title = "SVD Dimension 2"),
    title = "K-Means Clustering of OIS Reports"
  )
```


# Document classification

We can now explore whether the text in the officer-involved shooting narratives helps us predict whether the subject was fatally shot in the incident. We will base our predictions directly on the DTM, the word frequency count in the text. I will use `glmnet()` to fit an L1 regularized logistic regression model, a logistic regression model with a penalty on the sum of the absolute values of the coefficients. The intention is that this will select out those particular terms that signal a fatal shooting.
```{r}
#| label: fittingLasso
#| cache: true
# Fit logistic regression with L1 penalty
set.seed(20250329)
oisLasso <- cv.glmnet(as.matrix(oisDTM), 
                      ois$fatal, 
                      family = "binomial",
                      alpha = 1,
                      nfolds = 10)
```

@tbl-lassocoef shows the terms with the largest coefficients.
```{r}
#| tbl-cap: "Top 5 terms  with the largest coefficients"
#| label: tbl-lassocoef
coef(oisLasso, s = "lambda.min") |>
  as.matrix() |>
  as.data.frame() |>
  tibble::rownames_to_column("Term") |>
  arrange(desc(s1)) |>
  slice_head(n=10) |>
  kable(digits = 2, 
        col.names = c("Term", "LASSO Coefficient")) %>%
  kable_styling(full_width = FALSE, 
                bootstrap_options = c("striped", "hover"), 
                position = "left")
```
Quite sensibly we see terms associated with the subject being "pronounced deceased" and is variations. Only `r -1 + coef(oisLasso, s = "lambda.min") |> sign() |> abs() |> sum()` terms have non-zero coefficients.

Let's see how it performs in terms of misclassification. I will use 10-fold cross-validation to get out-of-fold predicted probabilities for each document.
```{r}
set.seed(20250329)
iFold <- rep(1:10, length.out=nrow(ois)) |> sample()
oofPred <- rep(NA, nrow(ois))

for (i in 1:10) 
{
  fit <- cv.glmnet(oisDTM[iFold!=i,], 
                   ois$fatal[iFold!=i], 
                   family = "binomial", 
                   alpha = 1,
                   nfolds = 10)
  oofPred[iFold==i] <- predict(fit, 
                               newx = oisDTM[iFold==i,],
                               s = "lambda.min", 
                               type = "response")
}

# Let's use a 0.25 probability cut-off (about 28% are fatal)
table(oofPred>0.25, ois$fatal)
```
Overall predictive performance seems reasonably good. There is a lot of information in the terms to separate most fatal incidents from the non-fatal ones.
