<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Greg Ridgeway">
<meta name="dcterms.date" content="2025-02-10">

<title>L3 Prediction, Bias, Variance, and Noise</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="L3-prediction-bias-variance_files/libs/clipboard/clipboard.min.js"></script>
<script src="L3-prediction-bias-variance_files/libs/quarto-html/quarto.js"></script>
<script src="L3-prediction-bias-variance_files/libs/quarto-html/popper.min.js"></script>
<script src="L3-prediction-bias-variance_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="L3-prediction-bias-variance_files/libs/quarto-html/anchor.min.js"></script>
<link href="L3-prediction-bias-variance_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L3-prediction-bias-variance_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="L3-prediction-bias-variance_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="L3-prediction-bias-variance_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L3-prediction-bias-variance_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L3-prediction-bias-variance_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="L3-prediction-bias-variance_files/libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview-of-prediction-models" id="toc-overview-of-prediction-models" class="nav-link active" data-scroll-target="#overview-of-prediction-models"><span class="header-section-number">1</span> Overview of prediction models</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example"><span class="header-section-number">2</span> Example</a>
  <ul class="collapse">
  <li><a href="#noise" id="toc-noise" class="nav-link" data-scroll-target="#noise"><span class="header-section-number">2.1</span> Noise</a></li>
  <li><a href="#variance" id="toc-variance" class="nav-link" data-scroll-target="#variance"><span class="header-section-number">2.2</span> Variance</a></li>
  <li><a href="#bias" id="toc-bias" class="nav-link" data-scroll-target="#bias"><span class="header-section-number">2.3</span> Bias</a></li>
  <li><a href="#prediction-error-is-noise-variance-squared-bias" id="toc-prediction-error-is-noise-variance-squared-bias" class="nav-link" data-scroll-target="#prediction-error-is-noise-variance-squared-bias"><span class="header-section-number">2.4</span> Prediction error is noise + variance + squared bias</a></li>
  </ul></li>
  <li><a href="#k-nearest-neighbor-regression" id="toc-k-nearest-neighbor-regression" class="nav-link" data-scroll-target="#k-nearest-neighbor-regression"><span class="header-section-number">3</span> <span class="math inline">\(k\)</span>-nearest neighbor regression</a></li>
  <li><a href="#sec-biasVarDecomp" id="toc-sec-biasVarDecomp" class="nav-link" data-scroll-target="#sec-biasVarDecomp"><span class="header-section-number">4</span> Bias/variance decomposition</a></li>
  <li><a href="#predicting-high-school-dropout-using-the-national-education-longitudinal-study-of-1988-nels88" id="toc-predicting-high-school-dropout-using-the-national-education-longitudinal-study-of-1988-nels88" class="nav-link" data-scroll-target="#predicting-high-school-dropout-using-the-national-education-longitudinal-study-of-1988-nels88"><span class="header-section-number">5</span> Predicting high school dropout using the National Education Longitudinal Study of 1988 (NELS88)</a>
  <ul class="collapse">
  <li><a href="#background-on-nels88" id="toc-background-on-nels88" class="nav-link" data-scroll-target="#background-on-nels88"><span class="header-section-number">5.1</span> Background on NELS88</a></li>
  <li><a href="#a-1000-nearest-neighbor-classifier" id="toc-a-1000-nearest-neighbor-classifier" class="nav-link" data-scroll-target="#a-1000-nearest-neighbor-classifier"><span class="header-section-number">5.2</span> A <span class="math inline">\(1000\)</span>-nearest neighbor classifier?</a></li>
  <li><a href="#how-many-nearest-neighbors-for-k-nearest-neighbor-classifier" id="toc-how-many-nearest-neighbors-for-k-nearest-neighbor-classifier" class="nav-link" data-scroll-target="#how-many-nearest-neighbors-for-k-nearest-neighbor-classifier"><span class="header-section-number">5.3</span> How many nearest neighbors for <span class="math inline">\(k\)</span>-nearest neighbor classifier?</a></li>
  </ul></li>
  <li><a href="#leave-one-out-cross-validation" id="toc-leave-one-out-cross-validation" class="nav-link" data-scroll-target="#leave-one-out-cross-validation"><span class="header-section-number">6</span> Leave-one-out cross-validation</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">7</span> Summary</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="L3-prediction-bias-variance.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">L3 Prediction, Bias, Variance, and Noise</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Greg Ridgeway <a href="mailto:gridge@upenn.edu" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            University of Pennsylvania
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 10, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<!-- In terminal -->
<!-- quarto render L3-prediction-bias-variance.qmd -->
<!-- quarto render L3-prediction-bias-variance.qmd --cache-refresh  -->
<!-- git commit L3* -m "commit message" -->
<!-- git status -->
<!-- git push -->
<p>Having explored the naïve Bayes classifier, how it learns from data, and how to evaluate its performance, we now turn to the general problem of supervised machine learning. “Supervised” machine learning is the area of machine learning in which we have existing data with known outcomes that we are trying to predict. In the case of the Georgia parolees, we observed their reoffense status within three years.</p>
<section id="overview-of-prediction-models" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Overview of prediction models</h1>
<p>Refer to <span class="citation" data-cites="Hast:Tibs:2001">Hastie, Tibshirani, and Friedman (<a href="#ref-Hast:Tibs:2001" role="doc-biblioref">2001</a>)</span> Chapters 2.4 and 3.2.</p>
<ol type="1">
<li><p>We observe <span class="math inline">\((y_i,\mathbf{x}_i), i=1,\ldots,N\)</span> drawn independently from some unknown distribution, <span class="math inline">\(P(y,\mathbf{x})\)</span>.</p></li>
<li><p>Examples <!-- Two spaces after first line, align second line to render correctly --></p>
<ol type="a">
<li><p><span class="math inline">\(\mathbf{x}\)</span>=characteristics of an adolescent entering drug treatment<br>
<span class="math inline">\(y\)</span>=passes a drug test 12 months after admission (a 0/1 outcome)</p></li>
<li><p><span class="math inline">\(\mathbf{x}\)</span>=characteristics of a firearm sale (purchaser, dealer, gun features)<br>
<span class="math inline">\(y\)</span>=time when the firearm is recovered in connection to a crime (a positive, possibly censored, continuous outcome)</p></li>
<li><p><span class="math inline">\(\mathbf{x}\)</span>=customer history of book purchases<br>
<span class="math inline">\(y\)</span>=title of the next book they will purchase (a very large multiclass outcome)</p></li>
<li><p><span class="math inline">\(\mathbf{x}\)</span>=demographics, ZIP code<br>
<span class="math inline">\(y\)</span>=will vote in next election</p></li>
<li><p><span class="math inline">\(\mathbf{x}\)</span>=all previous words in a conversation<br>
<span class="math inline">\(y\)</span>=next word produced in the conversation</p></li>
</ol></li>
<li><p>There is an unknown, noisy relationship between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(y\)</span> that we want to learn. That is, we want to find a function, <span class="math inline">\(\hat f(\mathbf{x})\)</span>, that minimizes a loss function of the form <span class="math display">\[
J(\hat f) = \mathbb{E} \ell(y,\hat f(\mathbf{x})) = \int\hspace{-5pt}\int \ell(y,\hat f(\mathbf{x})) P(y,\mathbf{x})\,d\mathbf{x}\, dy
\]</span></p>
<ol type="a">
<li><p><span class="math inline">\(J\)</span> is called a “functional”. Its argument is a function and <span class="math inline">\(J\)</span> of that function measures the performance of <span class="math inline">\(\hat f\)</span> at predicting the outcome</p></li>
<li><p><span class="math inline">\(\ell(y,\hat f(\mathbf{x}))\)</span> describes the cost of predicting <span class="math inline">\(\hat f(\mathbf{x})\)</span> when the true value is <span class="math inline">\(y\)</span>. Integrating over all <span class="math inline">\(y\)</span>s and <span class="math inline">\(\mathbf{x}\)</span>s yields the average loss or risk of using <span class="math inline">\(\hat f(\mathbf{x})\)</span> to make predictions</p></li>
<li><p>The true minimizer of <span class="math inline">\(J\)</span> is denoted <span class="math inline">\(f(\mathbf{x})\)</span>. <span class="math inline">\(\hat f(\mathbf{x})\)</span> is our best guess for <span class="math inline">\(f\)</span></p></li>
<li><p>Three concepts contribute to the magnitude of <span class="math inline">\(J\)</span></p>
<ol type="i">
<li><strong>Noise</strong>: Random or unpredictable variations in the data that obscure the underlying relationship between features and the outcome response</li>
<li><strong>Bias</strong>: Systematic error caused by restrictions of the model form or simplifying assumptions</li>
<li><strong>Variance</strong>: Sensitivity of a predictive model’s outputs to fluctuations in the training data</li>
</ol></li>
</ol></li>
<li><p>Example: Ordinary least squares (OLS) (<span class="citation" data-cites="Hast:Tibs:2001">Hastie, Tibshirani, and Friedman (<a href="#ref-Hast:Tibs:2001" role="doc-biblioref">2001</a>)</span> Chapter 2.3.1)</p>
<ol type="a">
<li><p>Let <span class="math inline">\(\ell(y,f(\mathbf{x})) = (y-f(\mathbf{x}))^2\)</span></p></li>
<li><p>Assume that <span class="math inline">\(f(x)=\beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_dx_d = \beta'\mathbf{x}\)</span> (or at least is closely approximated by a linear function of <span class="math inline">\(\mathbf{x}\)</span>)</p></li>
<li><p>Problem! We cannot compute <span class="math inline">\(J(f)\)</span> because we do not know the data distribution, <span class="math inline">\(P(y,\mathbf{x})\)</span>. We have a sample of <span class="math inline">\(n\)</span> observations from <span class="math inline">\(P\)</span> so we can approximate <span id="eq-Japprox"><span class="math display">\[
\begin{split}
\mathbb{E} \ell(y,f(\mathbf{x})) &amp;\approx \frac{1}{n}\sum_{i=1}^n (y_i-f(\mathbf{x}_i))^2 \\
   &amp;= \frac{1}{n}\sum_{i=1}^n (y_i - \beta'\mathbf{x}_i)^2 \\
\hat f(\mathbf{x}) &amp;= \mathbf{y}'\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x}
\end{split}
\tag{1}\]</span></span></p></li>
<li><p>If the approximation in the first line of (<a href="#eq-Japprox" class="quarto-xref">1</a>) is not very good then our model ends up not being very good</p></li>
<li><p>If we guess poorly at the form of <span class="math inline">\(f\)</span> then, regardless of how well we fare in (<a href="#eq-Japprox" class="quarto-xref">1</a>), our model may not be very good</p></li>
</ol></li>
<li><p>Predictions methods vary by how they address each of the following questions</p>
<ol type="a">
<li><p><strong>Loss function</strong>: How does the method penalize the difference between <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat f(\mathbf{x})\)</span>?</p></li>
<li><p><strong>Functional form</strong>: What are the restrictions on the form of <span class="math inline">\(\hat f(\mathbf{x})\)</span>?</p></li>
<li><p><strong>Optimization method</strong>: How does the method optimize <span class="math inline">\(J(f)\)</span>?</p></li>
</ol></li>
<li><p>Examples of triplets (loss function, functional form, optimization method)</p>
<ol type="a">
<li>OLS
<ol type="i">
<li>Loss function: Squared error, <span class="math inline">\(\ell(y,f(\mathbf{x})) = (y-f(\mathbf{x}))^2\)</span></li>
<li>Functional form: Any linear transformation of <span class="math inline">\(x\)</span>, <span class="math inline">\(\beta'\mathbf{x}\)</span></li>
<li>Optimization method: Use the sample average to estimate <span class="math inline">\(J\)</span> then there is a closed form solution, <span class="math inline">\(\hat f(\mathbf{x}) = \mathbf{y}'\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x}\)</span></li>
</ol></li>
<li>Logistic regression (for 0/1 outcomes)
<ol type="i">
<li>Loss function: Bernoulli log-likelihood, <span class="math inline">\(\ell(y,f(\mathbf{x})) = yf(\mathbf{x}) - \log\left(1+e^{f(\mathbf{x})}\right)\)</span></li>
<li>Functional form: Any linear transformation of <span class="math inline">\(x\)</span>, <span class="math inline">\(\beta'\mathbf{x}\)</span></li>
<li>Optimization method: Use the sample average to estimate <span class="math inline">\(J\)</span> then use the iteratively reweighted least squares (IRLS) algorithm (discussed in detail later)</li>
</ol></li>
<li>Single-layer neural network
<ol type="i">
<li>Loss function: Bernoulli log-likelihood, <span class="math inline">\(\ell(y,f(\mathbf{x})) = yf(\mathbf{x}) - \log\left(1+e^{f(\mathbf{x})}\right)\)</span></li>
<li>Functional form: <span class="math inline">\(\sigma(w_0+w_1\sigma(\beta_1'\mathbf{x})+\ldots+w_h\sigma(\beta_h'\mathbf{x}))\)</span>. Non-linear transformations of linear transformations of <span class="math inline">\(x\)</span></li>
<li>Optimization method: Gradient descent through backpropagation</li>
</ol></li>
</ol></li>
<li><p>Features of different methods. The loss function, functional form, and optimization method determine the prediction model. These choices have other side effects. I list them here roughly in order of the importance I attribute to them.</p>
<ol type="a">
<li><p><strong>Computability</strong>: Can you actually fit this model to data in a reasonable amount of time and with a reasonable amount of computing power?</p></li>
<li><p><strong>Accuracy</strong>: Does this model accurately describe the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(\mathbf{x}\)</span>? In particular, can it do so on future observations or generalize to other units not in the observed sample?</p></li>
<li><p><strong>Interpretability</strong>: Does the model help the analyst understand the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(\mathbf{x}\)</span> and which components of <span class="math inline">\(\mathbf{x}\)</span> are most important? Note that interpreting a model that does not accurately describe the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(\mathbf{x}\)</span> is absolutely pointless</p></li>
<li><p><strong>Stability</strong>: Does the model fit wildly change from sample to sample? Is it greatly affected by outliers?</p></li>
<li><p><strong>Missing data</strong>: Can the model utilize observations with missing values in <span class="math inline">\(\mathbf{x}\)</span>?</p></li>
<li><p><strong>Bayes risk consistent</strong>: As <span class="math inline">\(n\)</span> gets large does <span class="math inline">\(\hat f\)</span> approach <span class="math inline">\(f\)</span>?</p></li>
</ol></li>
</ol>
</section>
<section id="example" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Example</h1>
<p>This section will run through a simulation exploring noise, bias, and variance.</p>
<p>There is a true function out there, hidden from us, that we want to recover. That is the basic prediction model problem. In this case, we will simulate knowing what the true model is, then pretend we do not know what it is, and see how close we can get to matching its predictive performance.</p>
<p>The true model is <span class="math inline">\(f(x) = (x-1)^2+2\)</span>. We do not get to observe this directly, but instead only get to observe 100 <span class="math inline">\((x,y)\)</span> pairs where <span class="math inline">\(x_i\)</span> is randomly drawn on the interval <span class="math inline">\([0,3]\)</span> and <span class="math inline">\(y_i = f(x_i) + N(0,\frac{1}{4})\)</span>, the true model plus random noise generated from a normal distribution with mean 0 and variance <span class="math inline">\(\frac{1}{4}\)</span>. Here is some R code simulating the dataset, plotting the 100 points from the observed dataset, and overlaying the true model on top.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">3</span>) <span class="sc">|&gt;</span> <span class="fu">sort</span>()) <span class="sc">|&gt;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>     <span class="co"># here is the true f(x) (normally unknown to us)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">fx =</span> (x<span class="dv">-1</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span>) <span class="sc">|&gt;</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>     <span class="co"># add some noise unrelated to x,y</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>     <span class="co">#   rnorm(mean, std deviation)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">y =</span> fx <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.25</span>)))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y<span class="sc">~</span>x, <span class="at">data=</span>d)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(fx<span class="sc">~</span>x, <span class="at">data=</span>d, <span class="at">lwd=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-data" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L3-prediction-bias-variance_files/figure-html/fig-data-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Simulated dataset (points) with the true model, <span class="math inline">\(f(x)\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
<section id="noise" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="noise"><span class="header-section-number">2.1</span> Noise</h2>
<p><a href="#fig-data" class="quarto-xref">Figure&nbsp;1</a> shows the concept of <strong>noise</strong>. That is, even if we knew <span class="math inline">\(f(x)\)</span> exactly, if we make a prediction using it we will still not perfectly predict <span class="math inline">\(y\)</span>. In this simulation <span class="math display">\[
\mathrm{noise} = \mathbb{E}(y-f(x))^2 = \frac{1}{4}
\]</span> The average squared difference between the best prediction possible and <span class="math inline">\(y\)</span> is <span class="math inline">\(\frac{1}{4}\)</span>. Let’s check with our data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>d <span class="sc">|&gt;</span> <span class="fu">summarize</span>(<span class="at">noise =</span> <span class="fu">mean</span>((y<span class="sc">-</span>fx)<span class="sc">^</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      noise
1 0.2357729</code></pre>
</div>
</div>
<p>That is somewhat close to <span class="math inline">\(\frac{1}{4}\)</span>, but it is not exact because we just estimated that expected average using 100 simulated observations. A large sample will get us closer</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>nLarge <span class="ot">&lt;-</span> <span class="dv">100000000</span> <span class="co"># 100 million!</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">runif</span>(nLarge, <span class="dv">0</span>, <span class="dv">3</span>)) <span class="sc">|&gt;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">fx =</span> (x<span class="dv">-1</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">y =</span> fx <span class="sc">+</span> <span class="fu">rnorm</span>(nLarge, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.25</span>))) <span class="sc">|&gt;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">noise =</span> <span class="fu">mean</span>((y<span class="sc">-</span>fx)<span class="sc">^</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      noise
1 0.2499744</code></pre>
</div>
</div>
<p>Even that is an approximation. If we really want to exactly compute the noise we need to use an integral. Recall the law of large numbers.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Law of Large Numbers
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(y_1, \ldots, y_n\)</span> is a random sample from <span class="math inline">\(p(y)\)</span>, then as <span class="math inline">\(n \to \infty\)</span><br>
<span id="eq-lawoflargenumbers"><span class="math display">\[
\frac{1}{n}\sum_{i=1}^n g(y_i) \to \int_{-\infty}^\infty g(y)p(y)\,dy = \mathbb{E}[g(Y)].
\tag{2}\]</span></span></p>
</div>
</div>
<p><span class="math display">\[
\begin{split}
\mathrm{noise} &amp;= \mathbb{E}(y-f(x))^2 \\
&amp;= \int_0^3 \int_{-\infty}^\infty (y-f(x))^2\,P(x,y)\,dy\,dx \\
&amp;= \int_0^3 \int_{-\infty}^\infty (y-(x-1)^2-2)^2\,P(y|x)P(x)\,dy\,dx \\
&amp;= \int_0^3 \int_{-\infty}^\infty (y-(x-1)^2-2)^2\,
   \frac{1}{\sqrt{2\pi \cdot \frac{1}{4}}} e^{-\frac{1}{2}\left(\frac{y - (x-1)^2 - 2}{\frac{1}{2}}\right)^2}
   \frac{1}{3}\,dy\,dx \\
\end{split}
\]</span> Set <span class="math display">\[
\begin{split}
u &amp;= y-(x-1)^2-2 \\
du &amp;= dy
\end{split}
\]</span> <span class="math display">\[
\begin{split}
\mathrm{noise}  
&amp;= \frac{1}{3}\int_0^3 \color{orange}{\int_{-\infty}^\infty (u-0)^2\,
   \frac{1}{\sqrt{2\pi \cdot \frac{1}{4}}} e^{-\frac{1}{2}\left(\frac{u-0}{\frac{1}{2}}\right)^2}
   \,du} \, dx \\
\end{split}
\]</span> The term in orange is the definition of the variance of a <span class="math inline">\(N(0, \frac{1}{4})\)</span> random variable. <span class="math display">\[
\begin{split}
\mathrm{noise}  
&amp;= \frac{1}{3}\int_0^3 \frac{1}{4}\,dx \\
&amp;= \frac{1}{4}
\end{split}
\]</span> Now that was a rather long, and perhaps silly, computation to do. It relies on us knowing some key information, like the true form of <span class="math inline">\(f(x)\)</span> and the distribution <span class="math inline">\(P(x,y)\)</span>, things we will never know in practice. If we know (or assume) that <span class="math inline">\(\mathrm{Var}(y|x)=\sigma^2\)</span>, constant for all values of <span class="math inline">\(x\)</span>, then the calculation simplifies and concludes correctly that <span class="math inline">\(\mathrm{noise}=\sigma^2\)</span>. Still, real applications are never so kind to us. But I include this to show you to connect the simulated values to the concept of noise.</p>
</section>
<section id="variance" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="variance"><span class="header-section-number">2.2</span> Variance</h2>
<p>Next, let’s explore a model’s variance, not the variance of <span class="math inline">\(y\)</span>, but how our guess for a prediction model changes depending on the dataset. We return to the real world in which we do not know the true form of <span class="math inline">\(f(x)\)</span>. We will make an all-too-common assumption and consider only linear models of the form <span class="math inline">\(\hat f(x) = \beta_0+\beta_1x\)</span>. I will add to our plot the OLS estimate for the line of best fit.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y<span class="sc">~</span>x, <span class="at">data=</span>d)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>lm1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data=</span>d)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm1, <span class="at">lwd=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-var1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-var1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L3-prediction-bias-variance_files/figure-html/fig-var1-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-var1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: OLS estimate <span class="math inline">\(\hat f(x)\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
<p>This particular line’s y-intercept and slope depend on the specific collection of 100 simulated pairs <span class="math inline">\((x_i, y_i)\)</span>. There are many lines that we <em>could</em> have gotten from 100 randomly generated <span class="math inline">\(x\)</span>s and <span class="math inline">\(y\)</span>s. Let’s see what kind of lines we <em>could</em> have observed. I’ll generate 50 datasets and draw the OLS line for each of them in grey.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y<span class="sc">~</span>x, <span class="at">data=</span>d)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>nSim <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>coefs <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">ncol=</span><span class="dv">2</span>, <span class="at">nrow=</span>nSim)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nSim)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  dSim <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">3</span>)) <span class="sc">|&gt;</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">fx =</span> (x<span class="dv">-1</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span>) <span class="sc">|&gt;</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">y =</span> fx <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.25</span>)))</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  lmSim <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data=</span>dSim)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(lmSim, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">col=</span><span class="st">"grey"</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  coefs[i,] <span class="ot">&lt;-</span> <span class="fu">coef</span>(lmSim)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the average prediction line</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>meanfx <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(coefs)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(meanfx, <span class="at">lwd=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-var2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-var2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L3-prediction-bias-variance_files/figure-html/fig-var2-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-var2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Variation in <span class="math inline">\(\hat f(x)\)</span> from different simulated datasets
</figcaption>
</figure>
</div>
</div>
</div>
<p>All the lines are close to each other, but there is a range of predictions we might get, especially at the ends near 0 and 3. I have also plotted in a heavy black line the average of the 50 lines. This is an estimate of <span class="math inline">\(\mathbb{E}_D \hat f(\mathbf{x}|D)\)</span>, where <span class="math inline">\(D\)</span> represents a randomly constructed dataset. The notation <span class="math inline">\(\mathbb{E}_D\)</span> is meant to convey an averaging over the distribution of datasets we might observe. <a href="#fig-var2" class="quarto-xref">Figure&nbsp;3</a> shows the concept of <em>variance</em> of the model. We can formally define the variance of a prediction model as <span class="math display">\[
\mathrm{Var} \hat f(x_0) = \mathbb{E}_{D,Y|\mathbf{X}=\mathbf{x}_0} (\hat f(\mathbf{x}_0|D)-\mathbb{E}_D \hat f(\mathbf{x}_0|D))^2
\]</span> where <span class="math inline">\(x_0\)</span> is a specific value of <span class="math inline">\(x\)</span>.</p>
<p>Let’s use simulation to figure out <span class="math inline">\(\mathrm{Var} \hat f\left(\frac{1}{2}\right)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>nSim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>predfx0<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, nSim)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nSim)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  dSim <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">3</span>)) <span class="sc">|&gt;</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">fx =</span> (x<span class="dv">-1</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span>) <span class="sc">|&gt;</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">y =</span> fx <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.25</span>)))</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  lmSim <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data=</span>dSim)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  predfx0<span class="fl">.5</span>[i] <span class="ot">&lt;-</span> <span class="fu">predict</span>(lmSim, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">x=</span><span class="fl">0.5</span>))</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co"># compute average prediction E_D(fhat(x0)|D)</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>EDfx0<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">mean</span>(predfx0<span class="fl">.5</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># compute average squared difference (variance)</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>VarDfx0<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">mean</span>((predfx0<span class="fl">.5</span> <span class="sc">-</span> EDfx0<span class="fl">.5</span>)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>VarDfx0<span class="fl">.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.02094196</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># or more directly</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co">#   slight difference because var() divides by nSim-1 instead of nSim</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(predfx0<span class="fl">.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.02096293</code></pre>
</div>
</div>
<p>For a randomly selected dataset, we can expect our prediction at <span class="math inline">\(\frac{1}{2}\)</span> to have variance of about 0.02 (“about” because I’m using 1000 simulations to estimate this rather than doing a big integral calculation). Once again, in reality we do not know the distribution <span class="math inline">\(P(x,y)\)</span> so such simulations are never possible in reality. It turns out that we can get reasonable estimates of the variance from our original linear model fit to those 100 original observations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predict at x0 = 0.5 and ask to include standard error of prediction</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>fx0<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">predict</span>(lm1, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">x=</span><span class="fl">0.5</span>), <span class="at">se=</span><span class="cn">TRUE</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># square the standard error to get variance</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>fx0<span class="fl">.5</span><span class="sc">$</span>se.fit<span class="sc">^</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.01403683</code></pre>
</div>
</div>
<p>The variance estimated from that single sample is rather close to the true value we got from simulation. Variance is the one component that we can actually get a handle on even if we do not know the true <span class="math inline">\(f(x)\)</span>.</p>
</section>
<section id="bias" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="bias"><span class="header-section-number">2.3</span> Bias</h2>
<p>Lastly, we need to talk about the “bias”. In this example we know that the true model is a quadratic function, but our model is limited to learning linear functions. No matter how many observations we collect, our linear model will never match the best possible model. That difference is bias.</p>
<p>We characterize bias by how far our average prediction would be from the true model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y<span class="sc">~</span>x, <span class="at">data=</span>d)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>nSim <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>coefs <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">ncol=</span><span class="dv">2</span>, <span class="at">nrow=</span>nSim)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>predfx0<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, nSim)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nSim)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>  dSim <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">3</span>)) <span class="sc">|&gt;</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">fx =</span> (x<span class="dv">-1</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span>) <span class="sc">|&gt;</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">y =</span> fx <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.25</span>)))</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  lmSim <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data=</span>dSim)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># y-intercepts and slopes</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>  coefs[i,] <span class="ot">&lt;-</span> <span class="fu">coef</span>(lmSim)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the average prediction line, E_D fhat(x)</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>meanfx <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(coefs)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(meanfx, <span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the true model, f(x)</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="at">length=</span><span class="dv">100</span>)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, (x<span class="dv">-1</span>)<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">"orange"</span>, <span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>), </span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>      <span class="fu">c</span>(meanfx[<span class="dv">1</span>]<span class="sc">+</span>meanfx[<span class="dv">2</span>]<span class="sc">*</span><span class="fl">0.5</span>, (<span class="fl">0.5</span><span class="dv">-1</span>)<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span><span class="dv">2</span>),</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>      <span class="at">lwd=</span><span class="dv">3</span>, <span class="at">col=</span><span class="st">"#CC79A7"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="L3-prediction-bias-variance_files/figure-html/biasSimulation-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Bias of <span class="math inline">\(\hat f(x)\)</span>, vertical distance between <span class="math inline">\(f(x)\)</span> and average <span class="math inline">\(\mathbb{E}_D \hat f(x)\)</span></figcaption>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># bias squared</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>bias2 <span class="ot">&lt;-</span> ((<span class="fl">0.5</span><span class="dv">-1</span>)<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span><span class="dv">2</span> <span class="sc">-</span> (meanfx[<span class="dv">1</span>]<span class="sc">+</span>meanfx[<span class="dv">2</span>]<span class="sc">*</span><span class="fl">0.5</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>bias2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.06353409</code></pre>
</div>
</div>
<p>The vertical line at <span class="math inline">\(x_0=\frac{1}{2}\)</span> marks the difference between the true model and the prediction that we would get if we averaged over a lot of datasets. This is the bias caused by our insistence in using a linear model when a quadratic is the true model. This is another quantity that we will never know in reality. Here we see that limiting the flexibility of our model will create incurable bias in our predictions.</p>
<p>Formally, the squared bias is <span class="math display">\[
\mathbb{E}_{D|\mathbf{X}=\mathbf{x}_0} \left(f(\mathbf{x}_0)-\mathbb{E}_D \hat f(\mathbf{x}_0|D)\right)^2
\]</span></p>
</section>
<section id="prediction-error-is-noise-variance-squared-bias" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="prediction-error-is-noise-variance-squared-bias"><span class="header-section-number">2.4</span> Prediction error is noise + variance + squared bias</h2>
<p>We are going to put this all together now. In the end we are interested in the expected prediction error, the difference between the actual value, <span class="math inline">\(y\)</span>, and our predicted value, <span class="math inline">\(\hat f\left(\frac{1}{2}\right)\)</span>. Formally, <span class="math display">\[
\mathbb{E}_{D,y|\mathbf{X}=\mathbf{x}_0} \left(y-\hat f(\mathbf{x}_0|D)\right)^2
\]</span> This averages over all the distribution of datasets we might see and the values of <span class="math inline">\(y\)</span> that we might see. Let’s compute this quantity for our simulation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>nSim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>predfx0<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, nSim)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>ytrue <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, nSim)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nSim)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  dSim <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">3</span>)) <span class="sc">|&gt;</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">fx =</span> (x<span class="dv">-1</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span>) <span class="sc">|&gt;</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">y =</span> fx <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.25</span>)))</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  lmSim <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data=</span>dSim)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate a "true" value at x0=0.5</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>  ytrue[i] <span class="ot">&lt;-</span> (<span class="fl">0.5</span><span class="dv">-1</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.25</span>))</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># predict at 0.5</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>  predfx0<span class="fl">.5</span>[i] <span class="ot">&lt;-</span> <span class="fu">predict</span>(lmSim, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">x=</span><span class="fl">0.5</span>))</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co"># expected prediction error (EPE)</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>EPE0<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">mean</span>((ytrue <span class="sc">-</span> predfx0<span class="fl">.5</span>)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>EPE0<span class="fl">.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3389688</code></pre>
</div>
</div>
<p>On average when making predictions at <span class="math inline">\(x_0=\frac{1}{2}\)</span>, the squared difference between our prediction and the actual value will be about 0.34. There is a direct connection between the EPE and noise, variance, and squared bias.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># noise + variance + bias^2</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fl">0.25</span> <span class="sc">+</span> VarDfx0<span class="fl">.5</span> <span class="sc">+</span> bias2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3344761</code></pre>
</div>
</div>
<p>The expected prediction error we estimated is almost exactly the same as noise plus variance plus squared bias! The only reason they are not identical here is that our numbers are based on simulating datasets rather than computing the full integrals. In <a href="#sec-biasVarDecomp" class="quarto-xref">Section&nbsp;4</a> I will show that EPE is exactly noise plus variance plus squared bias.</p>
<p>First, we will explore the <span class="math inline">\(k\)</span>-nearest neighbor model to see how bias and variance influence should influence our modeling choices. You will see that when making a choice to reduce bias, we inevitably increase variance. If we try to reduce variance, we inevitably increase bias. This is known as the bias-variance tradeoff that will haunt us during the rest of our tour of machine learning methods.</p>
</section>
</section>
<section id="k-nearest-neighbor-regression" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> <span class="math inline">\(k\)</span>-nearest neighbor regression</h1>
<p>Refer to <span class="citation" data-cites="Hast:Tibs:2001">Hastie, Tibshirani, and Friedman (<a href="#ref-Hast:Tibs:2001" role="doc-biblioref">2001</a>)</span> Chapter 2.3.2.</p>
<p>Assume squared error loss so that we want to minimize <span class="math inline">\(J(f)=\mathbb{E}_{y,\mathbf{x}}(y-f(\mathbf{x}))^2\)</span>. The minimizer of <span class="math inline">\(J\)</span> is <span class="math inline">\(f(\mathbf{x})=\mathbb{E}(Y|\mathbf{x})\)</span>. Why?</p>
<p><span class="math display">\[
\begin{split}
J(f)&amp;=\mathbb{E}_{y,\mathbf{x}}(y-f(\mathbf{x}))^2 \\
&amp;=\mathbb{E}_{\mathbf{x}}\mathbb{E}_{y|\mathbf{x}}\left[(y-f(\mathbf{x}))^2|\mathbf{x}\right] \\
&amp;=\mathbb{E}_{\mathbf{x}}\mathbb{E}_{y|\mathbf{x}}\left[(y-\mathbb{E}(y|\mathbf{x})+\mathbb{E}(y|\mathbf{x})-f(\mathbf{x}))^2|\mathbf{x}\right] \\
&amp;=\mathbb{E}_{\mathbf{x}}\mathbb{E}_{y|\mathbf{x}}\left[(y-\mathbb{E}(y|\mathbf{x}))^2+2(y-\mathbb{E}(y|\mathbf{x}))(\mathbb{E}(y|\mathbf{x})-f(\mathbf{x}))+\right.\\
&amp; \left.\hspace{1in}(\mathbb{E}(y|\mathbf{x})-f(\mathbf{x}))^2|\mathbf{x}\right] \\
&amp;=\mathbb{E}_{\mathbf{x}}\left[\mathrm{Var}(y|\mathbf{x}) + 0 + (\mathbb{E}(y|\mathbf{x})-f(\mathbf{x}))^2|\mathbf{x}\right]
\end{split}
\]</span></p>
<p>We only have control over the choice of <span class="math inline">\(f(\mathbf{x})\)</span>, so to make <span class="math inline">\(J(f)\)</span> small we much set <span class="math inline">\(f(\mathbf{x})=\mathbb{E}(y|\mathbf{x})\)</span>.</p>
<p>If this is true, why would we fit a linear model to minimize squared error. It seems to imply that our prediction model should predict the average of <span class="math inline">\(y\)</span>’s for those observations with features <span class="math inline">\(\mathbf{x}\)</span>. The problem is that we might not have many (or any) observations in our dataset that have features exactly equal to <span class="math inline">\(\mathbf{x}\)</span>, especially if <span class="math inline">\(\mathbf{x}\)</span> is high dimensional. However, it is likely that there are some observations with features <em>near</em> <span class="math inline">\(\mathbf{x}\)</span>. If <span class="math inline">\(f(\mathbf{x})\)</span> is relatively smooth then averaging over the outcomes of observations in the neighborhood of <span class="math inline">\(\mathbf{x}\)</span> should yield a reasonable prediction model. This is the idea behind the nearest neighbor regression model.</p>
<ol type="1">
<li><p>Let <span class="math inline">\(D\)</span> represent our observed data and <span class="math inline">\(N_k(\mathbf{x})\)</span> be the set of <span class="math inline">\(k\)</span> observations in <span class="math inline">\(D\)</span> that are closest to <span class="math inline">\(\mathbf{x}\)</span>. <span class="math display">\[
\hat f(\mathbf{x})=\frac{1}{k}\sum_{\mathbf{x}_i\in N_k(\mathbf{x})} y_i
\]</span></p></li>
<li><p>Determining the “nearest” neighbors can be tricky. Euclidean distance is common. It is not clear how to compute distances for categorical variables. The model, of course, can be very sensitive to the choice of the distance measure.</p></li>
<li><p>In linear regression we know exactly how many parameters the model requires. How many parameters does the nearest neighbor model use? If we set <span class="math inline">\(k=n\)</span> then the resulting model fits a constant everywhere, one parameter. If <span class="math inline">\(k=1\)</span> then we have estimated <span class="math inline">\(n\)</span> values for prediction in each of the <span class="math inline">\(n\)</span> neighborhoods (also known as the Voronoi tessellation of the dataset). As a result we have effectively <span class="math inline">\(n\)</span> parameters. Generally, we estimate effectively <span class="math inline">\(\frac{n}{k}\)</span> parameters.</p></li>
</ol>
</section>
<section id="sec-biasVarDecomp" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Bias/variance decomposition</h1>
<p>Refer to <span class="citation" data-cites="Hast:Tibs:2001">Hastie, Tibshirani, and Friedman (<a href="#ref-Hast:Tibs:2001" role="doc-biblioref">2001</a>)</span> Chapter 2.9.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bias/variance/noise decomposition of expected prediction error
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\mathrm{EPE} = \mathrm{noise} + \mathrm{variance} + \mathrm{bias}^2
\]</span></p>
</div>
</div>
<p>This section uses the notation <span class="math inline">\(\hat f(\mathbf{x}_0|D)\)</span> to show that the prediction model we estimate depends on the dataset <span class="math inline">\(D\)</span>, which is random. Since <span class="math inline">\(D\)</span> is random we can think of <span class="math inline">\(\mathbb{E}_D \hat f(\mathbf{x}_0|D)\)</span> as the predicted value at <span class="math inline">\(\mathbf{x}_0\)</span> averaged over all the datasets we draw from <span class="math inline">\(P(y,\mathbf{x})\)</span>. For squared error we already know that <span class="math inline">\(f(\mathbf{x}_0)=\mathbb{E}(Y|\mathbf{x}_0)\)</span>. The expected prediction error (<span class="math inline">\(EPE\)</span>) can be decomposed as:</p>
<p><span id="eq-BiasVar2"><span class="math display">\[
\begin{split}
EPE(\mathbf{x}_0)
   =&amp; \mathbb{E}_{D,Y|\mathbf{X}=\mathbf{x}_0} \left[ (Y-\hat f(\mathbf{x}_0|D))^2 \right] \\
   =&amp; \mathbb{E}_{D,Y|\mathbf{X}=\mathbf{x}_0} \left[ (Y-f(\mathbf{x}_0)+f(\mathbf{x}_0)-\mathbb{E}_D \hat f(\mathbf{x}_0|D) +\mathbb{E}_D \hat f(\mathbf{x}_0|D) \right. \\
   &amp; \hspace{1in}\left. -\hat f(\mathbf{x}_0|D) )^2\right]\\
   =&amp; \mathbb{E}_{D,Y|\mathbf{X}=\mathbf{x}_0} (Y-f(\mathbf{x}_0))^2 +  \\
   &amp; \mathbb{E}_{D,Y|\mathbf{X}=\mathbf{x}_0} (f(\mathbf{x}_0)-\mathbb{E}_D \hat f(\mathbf{x}_0|D))^2 +  \\
   &amp; \mathbb{E}_{D,Y|\mathbf{X}=\mathbf{x}_0} (\hat f(\mathbf{x}_0|D)-\mathbb{E}_D \hat f(\mathbf{x}_0|D))^2 \\
   =&amp; \sigma^2 + \mathrm{Bias}(\hat f(\mathbf{x}_0|D))^2 + \mathrm{Var}(\hat f(\mathbf{x}_0|D))
\end{split}
\tag{3}\]</span></span></p>
<ol type="1">
<li><p>The decomposition decomposes the expected prediction error into</p>
<ol type="a">
<li><p><strong>Noise</strong>: The first term represents noise inherent in the system. There is nothing we can do about this given the data that we have. Only observing additional predictors (more covariates) that are predictive of <span class="math inline">\(Y\)</span> could possibly reduce this</p></li>
<li><p><strong>Bias</strong>: The bias term measures the difference between the true function and the average of the predicted values at <span class="math inline">\(\mathbf{x}_0\)</span>. If <span class="math inline">\(f\)</span> is not linear but <span class="math inline">\(\hat f(\mathbf{x}|D)\)</span> is restricted to be linear then this term can be large</p></li>
<li><p><strong>Variance</strong>: The variance term measures how sensitive the predicted value at <span class="math inline">\(\mathbf{x}_0\)</span> is to random fluctuations in the dataset. If the model predicts the same value at <span class="math inline">\(\mathbf{x}_0\)</span> no matter what dataset <span class="math inline">\(D\)</span> we use, then this term is 0. Of course, this is not a particularly useful model, but we can get the variance term to go away. The “model” <span class="math inline">\(\hat f(x)=4\)</span> has variance 0, but has large bias for almost all prediction questions of interest (except, say, “what is 2+2?”)</p></li>
</ol></li>
<li><p><strong>Bias/variance tradeoff</strong>: The bias/variance decomposition leads to one of the fundamental principles of prediction models: If an analyst wants to reduce variance, the price will be an increase in bias. If an analyst wants to reduce bias, the price will be an increase in variance.</p>
<ol type="a">
<li>Example: <span class="math inline">\(k\)</span>-nearest neighbors Assume that the <span class="math inline">\(\mathbf{x}_i\)</span>s are fixed so that the <span class="math inline">\(y_i\)</span>s are the only thing random in <span class="math inline">\(D\)</span>. <span class="math display">\[
\begin{split}
\mathbb{E}_D \hat f(\mathbf{x}_0|D) &amp;= \mathbb{E}_D \frac{1}{k}\hspace{-4pt}\sum_{\mathbf{x}_i\in N_k(\mathbf{x}_0)} \hspace{-8pt}y_i \\
   &amp;= \frac{1}{k}\hspace{-4pt}\sum_{\mathbf{x}_i\in N_k(\mathbf{x}_0)} \hspace{-4pt}\mathbb{E}_D y_i \\
   &amp;= \frac{1}{k}\hspace{-4pt}\sum_{\mathbf{x}_i\in N_k(\mathbf{x}_0)} \hspace{-4pt}f(\mathbf{x}_i) \\
EPE(\mathbf{x}_0)
   &amp;= \sigma^2 +
   \left(f(\mathbf{x}_0)-\frac{1}{k}\hspace{-4pt}\sum_{\mathbf{x}_i\in N_k(\mathbf{x}_0)} \hspace{-4pt}f(\mathbf{x}_i)\right)^2 + \frac{\sigma^2}{k}
\end{split}
\]</span> <span class="math inline">\(k\)</span> controls the bias/variance tradeoff for the <span class="math inline">\(k\)</span>-nearest neighbor model. Clearly large <span class="math inline">\(k\)</span> results in small variance. However, large <span class="math inline">\(k\)</span> also makes the neighborhoods larger so that the bias term gets larger as the model utilizes observations further away from <span class="math inline">\(\mathbf{x}_0\)</span>. The reverse is true when <span class="math inline">\(k\)</span> is small.</li>
</ol></li>
</ol>
</section>
<section id="predicting-high-school-dropout-using-the-national-education-longitudinal-study-of-1988-nels88" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Predicting high school dropout using the National Education Longitudinal Study of 1988 (NELS88)</h1>
<section id="background-on-nels88" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="background-on-nels88"><span class="header-section-number">5.1</span> Background on NELS88</h2>
<p>The <em>National Education Longitudinal Study of 1988</em> (NELS88) is a U.S.-based longitudinal dataset designed to track the educational experiences and outcomes of students, starting with those in the 8th grade in 1988. It provides a wealth of data about students, their academic performance, and their transitions through high school, post-secondary education, and into the workforce. The NELS88 dataset is widely used in education research, policy analysis, and studies on the social determinants of academic success.</p>
<p>The study began with a nationally representative sample of over 24,000 8th-grade students in 1988 and followed those students over multiple survey waves, in 1990 (10th grade), 1992 (12th grade), 1994 (two years post-high school), and 2000 (eight years post-high school). In each of these waves the survey captured data on students’ educational, occupational, and life trajectories. We will focus on the student-level data (test scores, socio-demographic characteristics, and attitudes and behaviors) and their relationship to dropping out of high school.</p>
<p>I have already acquired and cleaned up the dataset. If you want to do further work with the NELS data you may want to look at the <code>NELS data prep.R</code> script. We will start by loading some necessary R packages and the cleaned up dataset.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"data/nels.RData"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This will load <code>nels</code> (the complete dataset) and <code>nels0</code> a smaller cleaned up version of <code>nels</code>. Technically, the NELS data have sampling weights stored in <code>F4QWT</code>. We will not be using them correctly in this exercise to keep things simpler. Every analysis really should use the sampling weights. We will use them when you fit decision trees later in the course.</p>
<p>Let’s take a peek at the first few rows.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(nels0, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        F4QWT wave4dropout typeSchool urbanicity    region pctMinor
1 0.003988566            0   Catholic      urban northeast     none
2 0.003988566            0   Catholic      urban northeast     none
3 0.003988518            0   Catholic      urban northeast     none
  pctFreeLunch female  race    ses      parentEd famSize famStruct parMarital
1        11-20   male white -0.529 &gt;HS, &lt;College       4 Mom &amp; Dad    Married
2        11-20   male white -0.377 &gt;HS, &lt;College       6 Mom &amp; Dad    Married
3        11-20 female white -0.859 &gt;HS, &lt;College       6 Mom &amp; Dad    Married
  famIncome     langHome
1 $25k-$35k English only
2 $25k-$35k English only
3 $25k-$35k English only</code></pre>
</div>
</div>
<p>We will select every third student to be a part of the test set, leaving the remaing two-thirds of the students as training data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># select every third row for test data</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co">#   %% is the "mod" operator, gives remainder after dividing by 3</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>dTest  <span class="ot">&lt;-</span> nels0 <span class="sc">|&gt;</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">slice</span>(<span class="fu">which</span>(<span class="fu">row_number</span>() <span class="sc">%%</span> <span class="dv">3</span> <span class="sc">==</span> <span class="dv">0</span>))</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># select all other rows for training data</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>dTrain <span class="ot">&lt;-</span> nels0 <span class="sc">|&gt;</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(<span class="fu">which</span>(<span class="fu">row_number</span>() <span class="sc">%%</span> <span class="dv">3</span> <span class="sc">!=</span> <span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since most students graduate from high school, one reasonable prediction model is to simply predict that everyone will graduate. What kind of misclassification rate do we get if we set the predicted dropout probability to 0 for all students? Let’s start with setting the decision boundary at <span class="math inline">\(P(\mathrm{dropout}) &gt; 0.5\)</span>. Recall that this is equivalent to assuming equal cost of false positives and false negatives.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predict no dropouts, all graduate</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>dTest<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>dTest <span class="sc">|&gt;</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">misclass =</span> <span class="fu">mean</span>( (wave4dropout<span class="sc">==</span><span class="dv">1</span> <span class="sc">&amp;</span> p <span class="sc">&lt;=</span> <span class="fl">0.5</span>) <span class="sc">|</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>                             (wave4dropout<span class="sc">==</span><span class="dv">0</span> <span class="sc">&amp;</span> p <span class="sc">&gt;</span>  <span class="fl">0.5</span>) ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    misclass
1 0.04666491</code></pre>
</div>
</div>
<p>Since 4.7% of students dropout and we predicted everyone to graduate, our misclassification rate is 4.7%. We were right 95% of the time by predicting that everyone graduates! Clearly, this is unhelpful because it is really important to identify the students at risk of dropping out. Just because a classifier is correct 95% of the time, this example shows you that it is not always useful. Failing to identify a high school dropout is quite expensive, much more costly than incorrectly thinking a student will dropout but they end up graduating. Let’s assume that the false negative cost is 19 times more than the false positive cost. To be concrete, let’s say the false negative cost is 19 and the false positive cost is 1. Therefore, we should predict drop out of <span class="math inline">\(P(\mathrm{dropout}|\mathbf{x}) &gt; \frac{1}{1+19} = 0.05\)</span>. We can then compute the misclassification cost as</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>misclass0 <span class="ot">&lt;-</span> dTest <span class="sc">|&gt;</span> </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">misclassCost =</span> </span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>              <span class="fu">mean</span>(  <span class="dv">1</span><span class="sc">*</span>(wave4dropout<span class="sc">==</span><span class="dv">0</span> <span class="sc">&amp;</span> p<span class="sc">&gt;</span><span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">19</span><span class="sc">*</span>(wave4dropout<span class="sc">==</span><span class="dv">1</span> <span class="sc">&amp;</span> p<span class="sc">&lt;=</span><span class="fl">0.05</span>)))</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>misclass0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  misclassCost
1    0.8866333</code></pre>
</div>
</div>
<p>So the average misclassification <em>cost</em> is 0.89. Any machine learning method that we hope to use must have a smaller misclassification cost than this.</p>
</section>
<section id="a-1000-nearest-neighbor-classifier" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="a-1000-nearest-neighbor-classifier"><span class="header-section-number">5.2</span> A <span class="math inline">\(1000\)</span>-nearest neighbor classifier?</h2>
<p>We will start our investigation of the knn classifier using on the socio-economic status variable (<code>ses</code>), a standardized score that ranges from -2.88 to 2.56. We will determine nearest neighbors by selecting for each student in the test dataset (<code>dTest</code>) the <span class="math inline">\(k\)</span> students from the training dataset (<code>dTrain</code>) with the most similar values for <code>ses</code> as measured by absolute difference. Let’s start with <span class="math inline">\(k=1000\)</span>. So for each student in the test dataset, we select the 1000 training set students most similar in terms of <code>ses</code>.</p>
<p>In the following code, I run a for-loop through each student in the test set, find their 1000 nearest neighbors in the training set, and compute a predicted dropout probability as the percentage of dropouts in those 1000 nearest students. Note that in R, for-loops in R are very slow and should be avoided. I am only using a for-loop here so we can learn how knn works.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we will fill this in with predicted values</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>dTest<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="co"># set k to 1000 nearest </span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(dTest))</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>  dTest<span class="sc">$</span>p[i] <span class="ot">&lt;-</span> dTrain <span class="sc">|&gt;</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute the absolute distance between SES i and all training cases</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">distance =</span> <span class="fu">abs</span>(ses <span class="sc">-</span> dTest<span class="sc">$</span>ses[i])) <span class="sc">|&gt;</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get the k closest observations</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">slice_min</span>(distance, <span class="at">n=</span><span class="dv">1000</span>) <span class="sc">|&gt;</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use the average of these as the prediction</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">p =</span> <span class="fu">mean</span>(wave4dropout)) <span class="sc">|&gt;</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pull</span>(p)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s check the average misclassification cost for our test set.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test set misclassification cost if predicted p=0 for all students</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>dTest <span class="sc">|&gt;</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">misclassCost =</span> <span class="dv">19</span><span class="sc">*</span><span class="fu">mean</span>(wave4dropout<span class="sc">==</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  misclassCost
1    0.8866333</code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test set misclassification cost with 1000-nn classifier</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>misclassTest1000 <span class="ot">&lt;-</span> dTest <span class="sc">|&gt;</span> </span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">misclassCost =</span> </span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>              <span class="fu">mean</span>(  <span class="dv">1</span><span class="sc">*</span>(wave4dropout<span class="sc">==</span><span class="dv">0</span> <span class="sc">&amp;</span> p<span class="sc">&gt;</span><span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">19</span><span class="sc">*</span>(wave4dropout<span class="sc">==</span><span class="dv">1</span> <span class="sc">&amp;</span> p<span class="sc">&lt;=</span><span class="fl">0.05</span>)))</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>misclassTest1000</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  misclassCost
1    0.5929344</code></pre>
</div>
</div>
<p>Misclassification cost is 0.59, much lower than the misclassification cost if we classify everyone as a graduate. Let’s check out what the 1000-nn classifier looks like.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>dTest <span class="sc">|&gt;</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(ses) <span class="sc">|&gt;</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(p<span class="sc">~</span>ses, <span class="at">data=</span>_, <span class="at">type=</span><span class="st">"l"</span>, </span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(<span class="fl">0.1</span>,dTest<span class="sc">$</span>p)),</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab=</span><span class="st">"SES"</span>, <span class="at">ylab=</span><span class="st">"P(dropout)"</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co"># add the decision boundary</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fl">0.05</span>)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="fu">rug</span>(<span class="fu">quantile</span>(dTrain<span class="sc">$</span>ses, <span class="at">probs =</span> (<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>)<span class="sc">/</span><span class="dv">10</span>))</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="fu">rug</span>(<span class="fu">quantile</span>(dTrain<span class="sc">$</span>ses, <span class="at">probs =</span> (<span class="dv">0</span><span class="sc">:</span><span class="dv">2</span>)<span class="sc">/</span><span class="dv">2</span>), <span class="at">col=</span><span class="st">"red"</span>, <span class="at">lwd=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in rug(quantile(dTrain$ses, probs = (0:2)/2), col = "red", lwd = 3):
some values will be clipped</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-knn1000NELS" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-knn1000NELS-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L3-prediction-bias-variance_files/figure-html/fig-knn1000NELS-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knn1000NELS-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Predicted probability of dropout from 1000-nn model
</figcaption>
</figure>
</div>
</div>
</div>
<p>This figure shows strong threshold and saturation effects. A threshold effect is when the effect of a feature does not kick in until it reaches a certain threshold. Note that SES does not seem to matter (according to the 1000-nn model) until SES exceeds -1.5. Then increases in SES sharply decrease the risk of dropout. Saturation effects are when additional increases in a feature provide no additional changes to the outcome. Once SES exceeds 1.0, there is no further change in the dropout risk. Threshold and saturation effects are exceedingly common in reality. Yet many prediction models only consider linear combinations of features, which will always have bias that cannot be removed with additional data. One of the benefits of the knn classifier is that it can capture any relationship that might exist in the data.</p>
</section>
<section id="how-many-nearest-neighbors-for-k-nearest-neighbor-classifier" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="how-many-nearest-neighbors-for-k-nearest-neighbor-classifier"><span class="header-section-number">5.3</span> How many nearest neighbors for <span class="math inline">\(k\)</span>-nearest neighbor classifier?</h2>
<p>We were rather presumptuous in the previous section to set <span class="math inline">\(k=1000\)</span>. Perhaps there is another value of <span class="math inline">\(k\)</span> that could do even better. Let’s try a range of values for <span class="math inline">\(k\)</span> and measure their misclassification costs. I first make a <code>design</code> data frame that lists the values of <span class="math inline">\(k\)</span> that I want to try. It will take too long to try <em>every</em> choice for <span class="math inline">\(k\)</span> between 1 and 7588 (the number of students in the training dataset), so I created a selection of 36 values for <span class="math inline">\(k\)</span> in that range. This will allow us to narrow in on the best choice for <span class="math inline">\(k\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>design <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">k =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">10</span><span class="sc">*</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>), <span class="dv">100</span><span class="sc">*</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>), <span class="dv">500</span><span class="sc">*</span>(<span class="dv">3</span><span class="sc">:</span><span class="dv">10</span>), <span class="dv">6000</span>, <span class="dv">7000</span>),</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>                     <span class="at">misclassCost =</span> <span class="cn">NA</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(iK <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(design))</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>  k <span class="ot">&lt;-</span> design<span class="sc">$</span>k[iK]</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(dTest))</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dTest$p[i] &lt;- dTrain |&gt;</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   mutate(distance = abs(ses - dTest$ses[i])) |&gt;</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   slice_min(distance, n=k) |&gt;</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   summarize(p = mean(wave4dropout)) |&gt;</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   pull(p)</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># much faster than dplyr version</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    j <span class="ot">&lt;-</span> <span class="fu">abs</span>(dTrain<span class="sc">$</span>ses <span class="sc">-</span> dTest<span class="sc">$</span>ses[i]) <span class="sc">|&gt;</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>      <span class="fu">order</span>() <span class="sc">|&gt;</span> </span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>      <span class="fu">head</span>(k)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>    dTest<span class="sc">$</span>p[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(dTrain<span class="sc">$</span>wave4dropout[j])</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>  design<span class="sc">$</span>misclassCost[iK] <span class="ot">&lt;-</span> dTest <span class="sc">|&gt;</span></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">misclassCost =</span> <span class="fu">mean</span>( <span class="dv">1</span><span class="sc">*</span>(wave4dropout<span class="sc">==</span><span class="dv">0</span> <span class="sc">&amp;</span> p<span class="sc">&gt;</span> <span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>                                  <span class="dv">19</span><span class="sc">*</span>(wave4dropout<span class="sc">==</span><span class="dv">1</span> <span class="sc">&amp;</span> p<span class="sc">&lt;=</span><span class="fl">0.05</span>))) <span class="sc">|&gt;</span></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pull</span>(misclassCost)</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s plot the misclassification cost by <span class="math inline">\(k\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(misclassCost<span class="sc">~</span>k, <span class="at">data=</span>design, </span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">type=</span><span class="st">"l"</span>, <span class="at">lwd=</span><span class="dv">3</span>,</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"Misclassification cost"</span>)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># find the best value for k</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>kBest <span class="ot">&lt;-</span> design <span class="sc">|&gt;</span> </span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice_min</span>(misclassCost) <span class="sc">|&gt;</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(k)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>kBest)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>kBest</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7000</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-MisclassCostByK" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-MisclassCostByK-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L3-prediction-bias-variance_files/figure-html/fig-MisclassCostByK-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-MisclassCostByK-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Misclassification cost by choice of <span class="math inline">\(k\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
<p>It appears that the best choice for <span class="math inline">\(k\)</span> is around 500. Let’s look at a narrower range around <span class="math inline">\(k=500\)</span>. If you ran the previous block of code, you realized that it can take a while to try the 36 values of <span class="math inline">\(k\)</span>. It is a good habit to run such for loops in parallel. Each loop is independent of the other so we could have our computer work on multiple values of <span class="math inline">\(k\)</span> at the same time. The <code>doParallel</code> package makes this really easy. You can use <code>detectCores()</code> to learn how many cores you have available to you.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(doParallel)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># give R access to 12 cores</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>cl <span class="ot">&lt;-</span> <span class="fu">makeCluster</span>(<span class="dv">12</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="fu">registerDoParallel</span>(cl)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co"># look at a narrow range where performance is best</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>design <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">k =</span> <span class="fu">seq</span>(<span class="dv">250</span>,<span class="dv">600</span>,<span class="at">by=</span><span class="dv">3</span>), <span class="at">misclassCost =</span> <span class="cn">NA</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co"># run foreach() in parallel</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">foreach</span>(<span class="at">iK=</span><span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(design), <span class="at">.combine =</span> c) <span class="sc">%dopar%</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>  k <span class="ot">&lt;-</span> design<span class="sc">$</span>k[iK]</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(dTest))</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    j <span class="ot">&lt;-</span> <span class="fu">abs</span>(dTrain<span class="sc">$</span>ses <span class="sc">-</span> dTest<span class="sc">$</span>ses[i]) <span class="sc">|&gt;</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>      <span class="fu">order</span>() <span class="sc">|&gt;</span> </span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>      <span class="fu">head</span>(k)</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    dTest<span class="sc">$</span>p[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(dTrain<span class="sc">$</span>wave4dropout[j])</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>( <span class="fu">with</span>(dTest, <span class="fu">mean</span>( <span class="dv">1</span><span class="sc">*</span>(wave4dropout<span class="sc">==</span><span class="dv">0</span>)<span class="sc">*</span>(p<span class="sc">&gt;</span> <span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>                           <span class="dv">19</span><span class="sc">*</span>(wave4dropout<span class="sc">==</span><span class="dv">1</span>)<span class="sc">*</span>(p<span class="sc">&lt;=</span><span class="fl">0.05</span>))) )</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a><span class="co"># shut down the parallel processing cluster</span></span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a><span class="fu">stopCluster</span>(cl)</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>design<span class="sc">$</span>misclassCost <span class="ot">&lt;-</span> results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can plot the misclassification cost by choice of <span class="math inline">\(k\)</span> in this narrower range.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(misclassCost<span class="sc">~</span>k, <span class="at">data=</span>design, <span class="at">type=</span><span class="st">"l"</span>)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>kBest <span class="ot">&lt;-</span> design <span class="sc">|&gt;</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice_min</span>(misclassCost) <span class="sc">|&gt;</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(k)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>kBest)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>kBest</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 493</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-misclassCostNarrowRange" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-misclassCostNarrowRange-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L3-prediction-bias-variance_files/figure-html/fig-misclassCostNarrowRange-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-misclassCostNarrowRange-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Misclassification cost by choice of <span class="math inline">\(k\)</span> <span class="math inline">\((250\leq k\leq 600)\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
<p>Let’s have a look at what the optimal knn classifier look like.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(dTest))</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>  j <span class="ot">&lt;-</span> <span class="fu">abs</span>(dTrain<span class="sc">$</span>ses <span class="sc">-</span> dTest<span class="sc">$</span>ses[i]) <span class="sc">|&gt;</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">order</span>() <span class="sc">|&gt;</span> </span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">head</span>(kBest)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>  dTest<span class="sc">$</span>p[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(dTrain<span class="sc">$</span>wave4dropout[j])</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>dTest <span class="sc">|&gt;</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(ses) <span class="sc">|&gt;</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(p<span class="sc">~</span>ses, <span class="at">data=</span>_, </span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">type=</span><span class="st">"l"</span>, <span class="at">lwd=</span><span class="dv">3</span>,</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(<span class="fl">0.1</span>,dTest<span class="sc">$</span>p)),</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab=</span><span class="st">"SES"</span>, <span class="at">ylab=</span><span class="st">"P(dropout)"</span>)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a><span class="co"># add the decision boundary</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fl">0.05</span>)</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a><span class="fu">rug</span>(<span class="fu">quantile</span>(dTrain<span class="sc">$</span>ses, <span class="at">probs =</span> (<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">/</span><span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in rug(quantile(dTrain$ses, probs = (0:10)/10)): some values will be
clipped</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-fxDropoutOptimalK" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fxDropoutOptimalK-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L3-prediction-bias-variance_files/figure-html/fig-fxDropoutOptimalK-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fxDropoutOptimalK-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Predicted probability of dropout from optimal knn model, <span class="math inline">\(k=493\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="leave-one-out-cross-validation" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Leave-one-out cross-validation</h1>
<p>In this section, we will learn how to estimate tuning parameters (like <span class="math inline">\(k\)</span>) when we do not have an independent test set. We will use a fun, classic example of learning a spam email classifier. In this example, we have one dataset consisting of features of emails and a 0/1 outcome for whether the email was spam or not. From this single dataset we also need to figure out the best value for <span class="math inline">\(k\)</span>.</p>
<p>We will start by loading the data used in one of the early efforts to create an email spam filter.</p>
<p>L.F. Cranor and B.A. LaMacchia (1998) “Spam!,” <em>Communications of the ACM</em>, 41(8):74-83.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># download the spam dataset from the UCI machine learning archive</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>dSpam <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'</span>,</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>                    <span class="at">header=</span><span class="cn">FALSE</span>,</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">sep=</span><span class="st">","</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="co"># pull the column names from the data description file</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">scan</span>(<span class="st">'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names'</span>,</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">what=</span><span class="st">""</span>,<span class="at">sep=</span><span class="st">"</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>          <span class="at">skip =</span> <span class="dv">32</span>) <span class="sc">|&gt;</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>     <span class="fu">gsub</span>(<span class="st">":.*$"</span>, <span class="st">""</span>, <span class="at">x=</span>_)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fix the variable names</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(dSpam) <span class="ot">&lt;-</span> <span class="fu">c</span>(a, <span class="st">"spam"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As usual, we will take a peek at the first few rows.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(dSpam, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  word_freq_make word_freq_address word_freq_all word_freq_3d word_freq_our
1           0.00              0.64          0.64            0          0.32
2           0.21              0.28          0.50            0          0.14
3           0.06              0.00          0.71            0          1.23
  word_freq_over word_freq_remove word_freq_internet word_freq_order
1           0.00             0.00               0.00            0.00
2           0.28             0.21               0.07            0.00
3           0.19             0.19               0.12            0.64
  word_freq_mail word_freq_receive word_freq_will word_freq_people
1           0.00              0.00           0.64             0.00
2           0.94              0.21           0.79             0.65
3           0.25              0.38           0.45             0.12
  word_freq_report word_freq_addresses word_freq_free word_freq_business
1             0.00                0.00           0.32               0.00
2             0.21                0.14           0.14               0.07
3             0.00                1.75           0.06               0.06
  word_freq_email word_freq_you word_freq_credit word_freq_your word_freq_font
1            1.29          1.93             0.00           0.96              0
2            0.28          3.47             0.00           1.59              0
3            1.03          1.36             0.32           0.51              0
  word_freq_000 word_freq_money word_freq_hp word_freq_hpl word_freq_george
1          0.00            0.00            0             0                0
2          0.43            0.43            0             0                0
3          1.16            0.06            0             0                0
  word_freq_650 word_freq_lab word_freq_labs word_freq_telnet word_freq_857
1             0             0              0                0             0
2             0             0              0                0             0
3             0             0              0                0             0
  word_freq_data word_freq_415 word_freq_85 word_freq_technology word_freq_1999
1              0             0            0                    0           0.00
2              0             0            0                    0           0.07
3              0             0            0                    0           0.00
  word_freq_parts word_freq_pm word_freq_direct word_freq_cs word_freq_meeting
1               0            0             0.00            0                 0
2               0            0             0.00            0                 0
3               0            0             0.06            0                 0
  word_freq_original word_freq_project word_freq_re word_freq_edu
1               0.00                 0         0.00          0.00
2               0.00                 0         0.00          0.00
3               0.12                 0         0.06          0.06
  word_freq_table word_freq_conference char_freq_; char_freq_( char_freq_[
1               0                    0        0.00       0.000           0
2               0                    0        0.00       0.132           0
3               0                    0        0.01       0.143           0
  char_freq_! char_freq_$ char_freq_# capital_run_length_average
1       0.778       0.000       0.000                      3.756
2       0.372       0.180       0.048                      5.114
3       0.276       0.184       0.010                      9.821
  capital_run_length_longest capital_run_length_total spam
1                         61                      278    1
2                        101                     1028    1
3                        485                     2259    1</code></pre>
</div>
</div>
<p>The features in the dataset include:</p>
<ul>
<li><code>word_freq_xxx</code>: percentage of words in the email that are xxx</li>
<li><code>char_freq_x</code>: percentage of characters that are x</li>
<li><code>capital_run_length_average</code>: average length of uninterrupted sequences of capital letters</li>
<li><code>capital_run_length_longest</code>: length of longest uninterrupted sequence of capital letters</li>
<li><code>capital_run_length_total</code>: total number of capital letters in the e-mail</li>
<li><code>spam</code>: 0=no, 1=yes (unsolicited commercial e-mail)</li>
</ul>
<p>Load <code>ggplot2</code> so we can make some interesting figures.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dSpam, <span class="fu">aes</span>(<span class="st">`</span><span class="at">char_freq_$</span><span class="st">`</span>, <span class="st">`</span><span class="at">char_freq_!</span><span class="st">`</span>, <span class="at">color =</span> <span class="fu">factor</span>(spam))) <span class="sc">+</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">trans=</span><span class="st">'sqrt'</span>) <span class="sc">+</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">trans=</span><span class="st">'sqrt'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-plotDollarExclamation" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plotDollarExclamation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L3-prediction-bias-variance_files/figure-html/fig-plotDollarExclamation-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plotDollarExclamation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Plot of spam and non-spam emails based on $ and !
</figcaption>
</figure>
</div>
</div>
</div>
<p>Already from <a href="#fig-plotDollarExclamation" class="quarto-xref">Figure&nbsp;8</a> you can see some separation between spam and non-spam emails. Spam emails tend to have a strange number of dollar signs and exclamation marks and non-spam emails tend to have none.</p>
<p>While for the NELS88 dataset we built our knn classifier “by hand,” here we will use a more efficient implementation from the R package <code>FNN</code>, Fast Nearest Neighbor Search Algorithms. Inside the <code>FNN</code> package is a <code>knn()</code> function that works well. So you are aware, the <code>class</code> package also has a <code>knn()</code> function but it does not handle ties gracefully and it does not like <span class="math inline">\(k &gt; 1000\)</span>. If you get errors, then you are likely calling <code>class::knn()</code> instead of <code>FNN::knn()</code>. To be sure you are using the right <code>knn()</code>, prefix it with <code>FNN::</code> like <code>FNN::knn()</code>. This way you can be sure you are using the right version.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(FNN) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>First, we will practice using just the <code>char_freq_$</code> and <code>char_freq_!</code> as predictive features. In R, it is difficult to use features with $ and ! in their names. We’ll start by renaming them and then we will make a grid of values for <code>char_freq_$</code> and <code>char_freq_!</code> from 0 to their maximum values. We will extract predicted spam probabilities for all combinations in this range.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>dTrain <span class="ot">&lt;-</span> dSpam <span class="sc">|&gt;</span> </span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="st">`</span><span class="at">char_freq_$</span><span class="st">`</span>, <span class="st">`</span><span class="at">char_freq_!</span><span class="st">`</span>, spam) <span class="sc">|&gt;</span> </span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">dollar=</span><span class="st">`</span><span class="at">char_freq_$</span><span class="st">`</span>,</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">exclam=</span><span class="st">`</span><span class="at">char_freq_!</span><span class="st">`</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="co"># make a grid of values spanning $ and ! frequency </span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="co">#   (equally spaced on square root scale)</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>dGrid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">dollar=</span><span class="fu">seq</span>(<span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fu">max</span>(dTrain<span class="sc">$</span>dollar)), <span class="at">length=</span><span class="dv">100</span>),</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>                     <span class="at">exclam=</span><span class="fu">seq</span>(<span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fu">max</span>(dTrain<span class="sc">$</span>exclam)), <span class="at">length=</span><span class="dv">100</span>)) <span class="sc">|&gt;</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">dollar=</span>dollar<span class="sc">^</span><span class="dv">2</span>,</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>         <span class="at">exclam=</span>exclam<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s run the knn model to get predictions for all values in <code>dGrid</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get knn predictions</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>knnPred <span class="ot">&lt;-</span> FNN<span class="sc">::</span><span class="fu">knn</span>(</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># training data X</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">train   =</span> dTrain <span class="sc">|&gt;</span> <span class="fu">select</span>(<span class="sc">-</span>spam),</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># test data X</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">test    =</span> dGrid,</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># training data Y</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">cl      =</span> dTrain<span class="sc">$</span>spam,</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># number of neighbors for knn</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">k       =</span> <span class="dv">10</span>,</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># return probability</span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">prob    =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A few important things to note about predicted probabilities from <code>knn()</code>. Interpreting the predicted probability is tricky! Be careful!</p>
<ol type="1">
<li>the returned value in <code>knnPred</code> will be a vector of factor values “0” and “1”. It returns the majority class of the <span class="math inline">\(k\)</span> nearest neighbors, ignoring misclassification costs</li>
<li>the predicted probabilities are attached to <code>knnPred</code> as an attribute, so we use <code>attributes(knnPred)$prob</code> to extract them</li>
<li>the values stored in <code>attributes(knnPred)$prob</code> are <em>not</em> the probabilities of being spam, but rather the probability of being from the neighbors’ majority class. If the majority class is a 0, then the values in <code>attributes(knnPred)$prob</code> are actually <span class="math inline">\(1-P(\mathrm{spam})\)</span></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the probability of predicted class from knnPred</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>dGrid<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">attributes</span>(knnPred)<span class="sc">$</span>prob</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to P(spam)</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>dGrid<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(knnPred<span class="sc">==</span><span class="st">"1"</span>, dGrid<span class="sc">$</span>p, <span class="dv">1</span><span class="sc">-</span>dGrid<span class="sc">$</span>p)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can see how the 10-nn model divides the space. The lighter blue areas are those more likely to be spam and the dark areas tend to be non-spam.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data=</span>dGrid, <span class="fu">aes</span>(dollar, exclam, <span class="at">color =</span> p),</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">shape =</span> <span class="st">"square"</span>, <span class="at">size=</span><span class="dv">10</span>, <span class="at">alpha=</span><span class="fl">0.05</span>) <span class="sc">+</span> </span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data=</span>dTrain, <span class="fu">aes</span>(dollar, exclam, <span class="at">color =</span> spam)) <span class="sc">+</span> </span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">trans=</span><span class="st">'sqrt'</span>) <span class="sc">+</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">trans=</span><span class="st">'sqrt'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-10nnGrid" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-10nnGrid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L3-prediction-bias-variance_files/figure-html/fig-10nnGrid-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-10nnGrid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Predicted probability based on percentage of $ and ! using 10-nn model
</figcaption>
</figure>
</div>
</div>
</div>
<p>Note that there is a bit of a “scaling” issue. The percentage of characters that are dollar signs ranges from 0% to 6% while the percentage of characters that are exclamation marks ranges from 0% to 30%. Therefore, emails tend to be closer to each other in terms of dollar signs than they are in terms of exclamation marks just because of the difference in spread. Let’s regenerate the figure, but this time I am going to divide all the <code>char_freq_$</code> values by their maximum (6) and all the <code>char_freq_!</code> by their maximum (30) so that the new versions both range from 0 to 1.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>dTrain <span class="ot">&lt;-</span> dTrain <span class="sc">|&gt;</span> </span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">c</span>(dollar,exclam), <span class="cf">function</span>(x) x<span class="sc">/</span><span class="fu">max</span>(x)))</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>dGrid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">dollar=</span><span class="fu">seq</span>(<span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fu">max</span>(dTrain<span class="sc">$</span>dollar)), <span class="at">length=</span><span class="dv">100</span>),</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">exclam=</span><span class="fu">seq</span>(<span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fu">max</span>(dTrain<span class="sc">$</span>exclam)), <span class="at">length=</span><span class="dv">100</span>)) <span class="sc">|&gt;</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">dollar=</span>dollar<span class="sc">^</span><span class="dv">2</span>,</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">exclam=</span>exclam<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>knnPred <span class="ot">&lt;-</span> FNN<span class="sc">::</span><span class="fu">knn</span>(<span class="at">train   =</span> dTrain <span class="sc">|&gt;</span> <span class="fu">select</span>(<span class="sc">-</span>spam),</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">test    =</span> dGrid,</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>               <span class="at">cl      =</span> dTrain<span class="sc">$</span>spam,</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>               <span class="at">k       =</span> <span class="dv">10</span>,</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>               <span class="at">prob    =</span> <span class="cn">TRUE</span>)</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>dGrid<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">attributes</span>(knnPred)<span class="sc">$</span>prob</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>dGrid<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(knnPred<span class="sc">==</span><span class="st">"1"</span>, dGrid<span class="sc">$</span>p, <span class="dv">1</span><span class="sc">-</span>dGrid<span class="sc">$</span>p)</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a><span class="co"># show how knn divides the space</span></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data=</span>dGrid, <span class="fu">aes</span>(dollar, exclam, <span class="at">color =</span> p),</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>             <span class="at">shape =</span> <span class="st">"square"</span>, <span class="at">size=</span><span class="dv">10</span>, <span class="at">alpha=</span><span class="fl">0.05</span>) <span class="sc">+</span> </span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data=</span>dTrain, <span class="fu">aes</span>(dollar, exclam, <span class="at">color =</span> spam)) <span class="sc">+</span> </span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">trans=</span><span class="st">'sqrt'</span>) <span class="sc">+</span></span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">trans=</span><span class="st">'sqrt'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-10nnGridReScale" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-10nnGridReScale-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L3-prediction-bias-variance_files/figure-html/fig-10nnGridReScale-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-10nnGridReScale-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Predicted probability based on percentage of $ and !, rescaled to be between 0 and 1, using 10-nn model
</figcaption>
</figure>
</div>
</div>
</div>
<p>The classification boundaries have changed just by changing how we measure distance between emails. This is one of the fundamental challenges of using the knn classifier. The choice of distance measure will change the predicted probabilities, the classification boundaries, and ultimately the predictive performance of the model.</p>
<p>Now for a larger experiment. First, let’s go ahead and include all 57 recorded email features. Second, let’s experiment with five different ways of transforming the features. They include</p>
<ol type="1">
<li>Rescale by dividing by the maximum value (like we did before)</li>
<li>Standardize by subtracting the mean and dividing by the standard deviation (using <code>scale()</code>)</li>
<li>Take the square root of all the values and then standardize</li>
<li>Take the cube root of all the values and then standardize</li>
<li>Take the tenth root of all the values and then standardize</li>
</ol>
<p>I have no reason to believe that any of these methods would be better than another, but it is easy to try them out.</p>
<p>The last change we are going to make is to use <em>leave-one-out cross-validation</em> to test the performance. Just as it sounds, we will leave out the first email and find the <span class="math inline">\(k\)</span> nearest neighbors among the remaining emails. Those <span class="math inline">\(k\)</span> emails will “vote” on whether the first, held out email is spam or not. Then we repeat the same process for the second email, the third email, and so on, always leaving out one email and allowing its <span class="math inline">\(k\)</span> nearest emails to make predictions for it. When we do not have a separate test set, cross-validation is a convenient method for simulating what it would be like to make predictions on future emails. This is not hard to implement, but it takes some time and effort to do it well.</p>
<p>Fortunately for us, the <code>FNN</code> package has <code>knn.cv()</code> function built-in. It gives the leave-one-out cross-validated predictions for each email in the spam dataset. For this example, we will classify using majority vote (i.e.&nbsp;equal false positive and false negative costs, prediction threshold set to <span class="math inline">\(\frac{1}{2}\)</span>). As with <code>knn()</code>, there is a <code>prob=TRUE</code> option, but remember it will return the proportion of votes for the most common class (and <em>not</em> the probability of spam).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>design <span class="ot">&lt;-</span> </span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">expand.grid</span>(<span class="at">kval      =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>,</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">transform =</span> <span class="fu">c</span>(<span class="st">"divideMax"</span>,<span class="st">"scale"</span>,<span class="st">"sqrt"</span>,<span class="st">"cuberoot"</span>,<span class="st">"tenthroot"</span>),</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">misclass  =</span> <span class="cn">NA</span>)</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(design))</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>  dTrain <span class="ot">&lt;-</span> dSpam <span class="sc">|&gt;</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">spam=</span><span class="fu">factor</span>(spam, <span class="at">levels=</span><span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">labels=</span><span class="fu">c</span>(<span class="st">"not spam"</span>,<span class="st">"spam"</span>)))</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(design<span class="sc">$</span>transform[i]<span class="sc">==</span><span class="st">"divideMax"</span>)</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>    dTrain <span class="ot">&lt;-</span> dTrain <span class="sc">|&gt;</span></span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">57</span>, <span class="cf">function</span>(x) x<span class="sc">/</span><span class="fu">max</span>(x)))</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span> <span class="cf">if</span>(design<span class="sc">$</span>transform[i]<span class="sc">==</span><span class="st">"scale"</span>)</span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a>    dTrain <span class="ot">&lt;-</span> dTrain <span class="sc">|&gt;</span></span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">57</span>, scale))</span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span> <span class="cf">if</span>(design<span class="sc">$</span>transform[i]<span class="sc">==</span><span class="st">"sqrt"</span>)</span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a>    dTrain <span class="ot">&lt;-</span> dTrain <span class="sc">|&gt;</span></span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">57</span>, sqrt)) <span class="sc">|&gt;</span></span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">57</span>, scale))</span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb55-27"><a href="#cb55-27" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span> <span class="cf">if</span>(design<span class="sc">$</span>transform[i]<span class="sc">==</span><span class="st">"cuberoot"</span>)</span>
<span id="cb55-28"><a href="#cb55-28" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb55-29"><a href="#cb55-29" aria-hidden="true" tabindex="-1"></a>    dTrain <span class="ot">&lt;-</span> dTrain <span class="sc">|&gt;</span></span>
<span id="cb55-30"><a href="#cb55-30" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">57</span>, <span class="cf">function</span>(x) x<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>))) <span class="sc">|&gt;</span></span>
<span id="cb55-31"><a href="#cb55-31" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">57</span>, scale))</span>
<span id="cb55-32"><a href="#cb55-32" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb55-33"><a href="#cb55-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span> <span class="cf">if</span>(design<span class="sc">$</span>transform[i]<span class="sc">==</span><span class="st">"tenthroot"</span>)</span>
<span id="cb55-34"><a href="#cb55-34" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb55-35"><a href="#cb55-35" aria-hidden="true" tabindex="-1"></a>    dTrain <span class="ot">&lt;-</span> dTrain <span class="sc">|&gt;</span></span>
<span id="cb55-36"><a href="#cb55-36" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">57</span>, <span class="cf">function</span>(x) x<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">10</span>))) <span class="sc">|&gt;</span></span>
<span id="cb55-37"><a href="#cb55-37" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">57</span>, scale))</span>
<span id="cb55-38"><a href="#cb55-38" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb55-39"><a href="#cb55-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-40"><a href="#cb55-40" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get leave-one-out cross-validated predictions</span></span>
<span id="cb55-41"><a href="#cb55-41" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> FNN<span class="sc">::</span><span class="fu">knn.cv</span>(dTrain <span class="sc">|&gt;</span> <span class="fu">select</span>(<span class="sc">-</span>spam), </span>
<span id="cb55-42"><a href="#cb55-42" aria-hidden="true" tabindex="-1"></a>                   <span class="at">cl =</span> dTrain<span class="sc">$</span>spam,</span>
<span id="cb55-43"><a href="#cb55-43" aria-hidden="true" tabindex="-1"></a>                   <span class="at">k  =</span> design<span class="sc">$</span>kval[i])</span>
<span id="cb55-44"><a href="#cb55-44" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb55-45"><a href="#cb55-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute misclassification rate</span></span>
<span id="cb55-46"><a href="#cb55-46" aria-hidden="true" tabindex="-1"></a>  design<span class="sc">$</span>misclass[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(dTrain<span class="sc">$</span>spam<span class="sc">!=</span>p)</span>
<span id="cb55-47"><a href="#cb55-47" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>design <span class="sc">|&gt;</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> kval, <span class="at">y =</span> misclass, <span class="at">color =</span> transform, <span class="at">group =</span> transform)) <span class="sc">+</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="co"># Add points to highlight data</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">""</span>,</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"k"</span>,</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Misclassification Rate"</span>,</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"Transformation"</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="cn">NA</span>)) <span class="sc">+</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="cn">NA</span>)) <span class="sc">+</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>),</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">"bottom"</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-TransformExperiment" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-TransformExperiment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L3-prediction-bias-variance_files/figure-html/fig-TransformExperiment-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-TransformExperiment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Misclassification Rate by <span class="math inline">\(k\)</span> and Transformation
</figcaption>
</figure>
</div>
</div>
</div>
<p>First, it seems that the single nearest neighbor (<span class="math inline">\(k=1\)</span>) works best.</p>
<p>Also, you can see that how we transform the email features has an effect on the misclassification rate. Taking the tenth root of each of the features and standardizing seems to do the best. Why tenth root? I have no idea. Almost certainly there is another way to transform these email features to get even better predictive performance.</p>
<p>This process is “feature engineering”… trying to figure out what aspects and transforms of the inputs result in the best performance. knn’s performance depends heavily on the scale of the features. One desirable property of a machine learning method is that either the method is indifferent to the scale or that it can learn on its own what transformations result in the best predictive performance.</p>
<p>This dataset was already heavily feature engineered. The researchers already pulled 57 specific features from the emails presumably thinking that these were already good signs of spam. The best known performance on this dataset is 4%. To get that performance the researchers used boosting, a method that is indifferent to the scales of the features as well as any one-to-one transformations (e.g.&nbsp;log, exp, square root, square) of the features.</p>
</section>
<section id="summary" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Summary</h1>
<p>Predictive performance depends on</p>
<ul>
<li>Noise</li>
<li>Bias</li>
<li>Variance</li>
</ul>
<p><span class="math inline">\(k\)</span>-nearest neighbors is a rather simple approach for prediction problems. It is based on the idea that when trying to predict an outcome for an observation with features <span class="math inline">\(\mathbf{x}\)</span> finding the outcomes for known observations with features similar to <span class="math inline">\(\mathbf{x}\)</span> should work reasonably well.</p>
<p>There is a bias-variance tradeoff. If we choose more neighbors, we reduce variance but increase bias. If we choose few neighbors, then we increase variance but decrease bias.</p>
<p>Using test sets or cross-validation we can choose the models and model parameters that offer the best predictive performance, balancing the bias-variance tradeoff.</p>
<p>Ideally, a machine learning method could learn from the data what is the best way to transform the inputs to get the best predictive performance. knn requires a fair bit of <em>feature engineering</em> to improve predictive performance.</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Hast:Tibs:2001" class="csl-entry" role="listitem">
Hastie, T., R. Tibshirani, and J. H. Friedman. 2001. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer-Verlag.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>