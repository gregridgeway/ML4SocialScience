<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Greg Ridgeway">
<meta name="dcterms.date" content="2025-04-08">

<title>L10 Neural networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="L10-neural-nets_files/libs/clipboard/clipboard.min.js"></script>
<script src="L10-neural-nets_files/libs/quarto-html/quarto.js"></script>
<script src="L10-neural-nets_files/libs/quarto-html/popper.min.js"></script>
<script src="L10-neural-nets_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="L10-neural-nets_files/libs/quarto-html/anchor.min.js"></script>
<link href="L10-neural-nets_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L10-neural-nets_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="L10-neural-nets_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="L10-neural-nets_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L10-neural-nets_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L10-neural-nets_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="L10-neural-nets_files/libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#reading" id="toc-reading" class="nav-link active" data-scroll-target="#reading"><span class="header-section-number">1</span> Reading</a></li>
  <li><a href="#a-simple-neural-network" id="toc-a-simple-neural-network" class="nav-link" data-scroll-target="#a-simple-neural-network"><span class="header-section-number">2</span> A simple neural network</a></li>
  <li><a href="#neural-networks-with-hidden-layers" id="toc-neural-networks-with-hidden-layers" class="nav-link" data-scroll-target="#neural-networks-with-hidden-layers"><span class="header-section-number">3</span> Neural networks with hidden layers</a></li>
  <li><a href="#example-in-two-dimensions" id="toc-example-in-two-dimensions" class="nav-link" data-scroll-target="#example-in-two-dimensions"><span class="header-section-number">4</span> Example in two-dimensions</a></li>
  <li><a href="#a-more-complex-decision-boundary-in-two-dimensions" id="toc-a-more-complex-decision-boundary-in-two-dimensions" class="nav-link" data-scroll-target="#a-more-complex-decision-boundary-in-two-dimensions"><span class="header-section-number">5</span> A more complex decision boundary in two dimensions</a>
  <ul class="collapse">
  <li><a href="#searching-for-a-linear-boundary" id="toc-searching-for-a-linear-boundary" class="nav-link" data-scroll-target="#searching-for-a-linear-boundary"><span class="header-section-number">5.1</span> Searching for a linear boundary</a></li>
  <li><a href="#neural-network-with-hidden-layers" id="toc-neural-network-with-hidden-layers" class="nav-link" data-scroll-target="#neural-network-with-hidden-layers"><span class="header-section-number">5.2</span> Neural network with hidden layers</a></li>
  </ul></li>
  <li><a href="#rs-neuralnet-package" id="toc-rs-neuralnet-package" class="nav-link" data-scroll-target="#rs-neuralnet-package"><span class="header-section-number">6</span> R’s <code>neuralnet</code> package</a></li>
  <li><a href="#tensorflow-and-keras" id="toc-tensorflow-and-keras" class="nav-link" data-scroll-target="#tensorflow-and-keras"><span class="header-section-number">7</span> Tensorflow and Keras</a>
  <ul class="collapse">
  <li><a href="#tensorflow-playground" id="toc-tensorflow-playground" class="nav-link" data-scroll-target="#tensorflow-playground"><span class="header-section-number">7.1</span> Tensorflow Playground</a></li>
  <li><a href="#installing-tensorflow-and-keras" id="toc-installing-tensorflow-and-keras" class="nav-link" data-scroll-target="#installing-tensorflow-and-keras"><span class="header-section-number">7.2</span> Installing Tensorflow and Keras</a></li>
  <li><a href="#mnist-postal-digits-data" id="toc-mnist-postal-digits-data" class="nav-link" data-scroll-target="#mnist-postal-digits-data"><span class="header-section-number">7.3</span> MNIST postal digits data</a></li>
  <li><a href="#convolution-layers" id="toc-convolution-layers" class="nav-link" data-scroll-target="#convolution-layers"><span class="header-section-number">7.4</span> Convolution layers</a></li>
  <li><a href="#a-convolutional-neural-network-with-keras" id="toc-a-convolutional-neural-network-with-keras" class="nav-link" data-scroll-target="#a-convolutional-neural-network-with-keras"><span class="header-section-number">7.5</span> A convolutional neural network with Keras</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="L10-neural-nets.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">L10 Neural networks</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Greg Ridgeway <a href="mailto:gridge@upenn.edu" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            University of Pennsylvania
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 8, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<!-- In terminal -->
<!-- quarto render L10-neural-nets.qmd -->
<!-- quarto render L10-neural-nets.qmd --cache-refresh  -->
<!-- Include C:\cygwin64\bin in path to get to rsvg-convert -->
<!-- git commit L10-* -m "commit message" -->
<!-- git status -->
<!-- git push -->
<section id="reading" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Reading</h1>
<p>Read <span class="citation" data-cites="Hast:Tibs:2001">Hastie, Tibshirani, and Friedman (<a href="#ref-Hast:Tibs:2001" role="doc-biblioref">2001</a>)</span> Chapter 11.</p>
<p>Read <span class="citation" data-cites="ISLR2">James et al. (<a href="#ref-ISLR2" role="doc-biblioref">2021</a>)</span> Chapter 10.</p>
<p>Read Y. LeCun, Y. Bengio, and G. Hinton (2015). “Deep learning,” <em>Nature</em> 521, 436–444.</p>
</section>
<section id="a-simple-neural-network" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> A simple neural network</h1>
<p>Consider a simple neural network that has two inputs <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> that enter a linear transformation, which outputs a prediction. Let’s further assume that we wish to minimize squared error, <span class="math inline">\(J(\mathbf{y},\mathbf{\hat y})=\sum (y_i-\hat y_i)^2\)</span>. This is the same as ordinary least squares, but I introduce it with a neural network framework to move gradually into more complex neural networks. So, the structure of this model is</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-linearnn" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-linearnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-linearnn-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-linearnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A linear model as a network
</figcaption>
</figure>
</div>
</div>
</div>
<p>We start with some guess for <span class="math inline">\(\hat\beta\)</span>, perhaps <span class="math inline">\((0,0,0)\)</span>. Then, once we pass our data through the neural network, we get predicted values and learn the magnitude of our error by computing <span class="math inline">\(\sum (y_i-\hat y_i)^2\)</span>. Now we can improve our guess for <span class="math inline">\(\hat\beta\)</span> by computing the gradient of <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(\beta\)</span>. We will chain rule our way to figure out the gradient.</p>
<p><span class="math display">\[
\begin{split}
    \frac{\partial J}{\partial\hat y_i} &amp;= -2(y_i-\hat y_i) \\
    \frac{\partial\hat y_i}{\partial \beta_0} &amp;= 1 \\
    \frac{\partial\hat y_i}{\partial \beta_1} &amp;= x_{i1} \\
    \frac{\partial\hat y_i}{\partial \beta_2} &amp;= x_{i2} \\
    \frac{\partial J}{\partial \beta_0} &amp;=
    \frac{\partial J}{\partial\hat y_1}\frac{\partial\hat y_1}{\partial \beta_0} + \ldots + \frac{\partial J}{\partial\hat y_n}\frac{\partial\hat y_n}{\partial \beta_0} \\
    &amp;= -2(y_1-\hat y_1)(1) + \ldots + -2(y_n-\hat y_n)(1) \\
    &amp;= -2\sum(y_i-\hat y_i)
\end{split}
\]</span></p>
<p>So, to reduce the loss function <span class="math inline">\(J\)</span> we need to adjust <span class="math inline">\(\hat\beta_0\)</span> as <span class="math display">\[
    \hat\beta_0 \leftarrow \hat\beta_0 - \lambda\left(-2\sum(y_i-\hat y_i)\right)
\]</span> where <span class="math inline">\(\lambda\)</span> is the “learning rate.” Similarly for <span class="math inline">\(\hat\beta_1\)</span> and <span class="math inline">\(\hat\beta_2\)</span> <span class="math display">\[
\begin{split}
\frac{\partial J}{\partial \beta_1} &amp;=
    \frac{\partial J}{\partial\hat y_1}\frac{\partial\hat y_1}{\partial \beta_1} + \ldots + \frac{\partial J}{\partial\hat y_n}\frac{\partial\hat y_n}{\partial \beta_1} \\
    &amp;= -2(y_1-\hat y_1)x_{11} + \ldots + -2(y_n-\hat y_n)x_{n1} \\
    &amp;= -2\sum(y_i-\hat y_i)x_{i1} \\
\frac{\partial J}{\partial \beta_2} &amp;=
    -2\sum(y_i-\hat y_i)x_{i2}
\end{split}
\]</span> Putting this all together, an algorithm for optimizing this simple neural network would be <span class="math display">\[
\begin{split}
\begin{bmatrix} \hat\beta_0 \\ \hat\beta_1 \\ \hat\beta_2\end{bmatrix}
&amp;\leftarrow
\begin{bmatrix} \hat\beta_0 \\ \hat\beta_1 \\ \hat\beta_2\end{bmatrix}
- \lambda
\begin{bmatrix} -2\sum(y_i-\hat y_i) \\ -2\sum(y_i-\hat y_i)x_{i1} \\ -2\sum(y_i-\hat y_i)x_{i2}\end{bmatrix} \\
\hat\beta &amp;\leftarrow \hat\beta + \lambda \mathbf{X}'(\mathbf{y}-\mathbf{\hat y})
\end{split}
\]</span> where in the last line the <span class="math inline">\(\lambda\)</span> absorbed the <span class="math inline">\(2\)</span>.</p>
</section>
<section id="neural-networks-with-hidden-layers" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Neural networks with hidden layers</h1>
<p>The neural network in the previous section has no better features or capacity than ordinary least squares. To get more capacity to capture interesting shapes and boundaries we will make two changes: 1) add a hidden layer of nodes and 2) add non-linear transformations of the input features.</p>
<p>The network in <a href="#fig-nnhiddenlayer1" class="quarto-xref">Figure&nbsp;2</a> takes two continuous inputs, passes them through a hidden layer of four nodes, which passes their output to a final output node.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nnhiddenlayer1" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nnhiddenlayer1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-nnhiddenlayer1-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nnhiddenlayer1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: A neural network with a single hidden layer with 4 hidden nodes and a single output
</figcaption>
</figure>
</div>
</div>
</div>
<p>If all the nodes just did linear transformations, then the output node would also just be a linear transformation of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, offering no improvement over ordinary least squares. Instead, neural networks apply a non-linear “activation function” to the linear combination of features from the previous node. With a single hidden layer and a non-linear activation function, a neural network can capture any shape decision boundary. It is a “universal approximator.” The success of deep learning comes from how expressive neural networks can be with layers of these simple functions on top of each other.</p>
<p>The most common activation functions are the sigmoid (also known as the inverse logit transform or expit), the hyperbolic tangent, the rectified linear unit (ReLU… pronounced ray-loo), and the Heaviside function. <a href="#fig-activationFuncs" class="quarto-xref">Figure&nbsp;3</a> shows their shape.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>,<span class="at">length.out=</span><span class="dv">100</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># sigmoid, 1/(1+exp(-x))</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>x)), <span class="at">type=</span><span class="st">"l"</span>, <span class="at">lwd=</span><span class="dv">3</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="fu">expression</span>(<span class="fu">sigma</span>(x)))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># hyperbolic tangent</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, (<span class="dv">1</span><span class="sc">+</span><span class="fu">tanh</span>(x))<span class="sc">/</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">"red"</span>, <span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Heaviside function, perceptron</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, x<span class="sc">&gt;</span><span class="dv">0</span>, <span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="dv">4</span>)   </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ReLU - Rectified Linear Unit</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#   gradient is 0 or 1</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">pmax</span>(<span class="dv">0</span>,x), <span class="at">col=</span><span class="st">"orange"</span>, <span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="fl">0.8</span>,<span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"sigmoid"</span>,<span class="st">"tanh"</span>,<span class="st">"ReLU"</span>,<span class="st">"Heaviside"</span>),</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">col=</span><span class="fu">c</span>(<span class="st">"black"</span>,<span class="st">"red"</span>,<span class="st">"orange"</span>,<span class="st">"blue"</span>),</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-activationFuncs" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-activationFuncs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-activationFuncs-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-activationFuncs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Four common activation functions
</figcaption>
</figure>
</div>
</div>
</div>
<p>Each node applies one of these activation functions to the linear combination of inputs from the previous layer. Used in this manner, they are called “ridge” activation functions.</p>
<p>Returning to the network in <a href="#fig-nnhiddenlayer1" class="quarto-xref">Figure&nbsp;2</a>, the hidden nodes marked with a <span class="math inline">\([1]\)</span> superscript in them all have the form <span class="math display">\[
\mathbf{a}^{[1]} =
\begin{bmatrix}
    a_1^{[1]} \\ a_2^{[1]} \\ a_3^{[1]} \\a_4^{[1]}
\end{bmatrix}
=
\begin{bmatrix}
    \sigma\left(\mathbf{w}_1^{[1]}{'}\mathbf{x}\right) \\
    \sigma\left(\mathbf{w}_2^{[1]}{'}\mathbf{x}\right) \\
    \sigma\left(\mathbf{w}_3^{[1]}{'}\mathbf{x}\right) \\
    \sigma\left(\mathbf{w}_4^{[1]}{'}\mathbf{x}\right) \\
\end{bmatrix}
\]</span> In some neural network discussions you will see a constant term separate from the linear transform of the <span class="math inline">\(\mathbf{x}\)</span>, called the “bias”. Instead here we assume that <span class="math inline">\(\mathbf{x}\)</span> includes a constant 1 as its first element so that <span class="math inline">\(w_{11}^{[1]}\)</span>, for example, is that constant term.</p>
<p>The output layer will also apply an activation function to the inputs from the hidden layer. <span class="math display">\[
\begin{split}
\hat y=\mathbf{a}^{[2]} &amp;=
    \sigma\left(\mathbf{w}^{[2]}{'}\mathbf{a^{[1]}}\right) \\
    &amp;=
    \sigma\left(
    w_0^{[2]} + w_1^{[2]}a_1^{[1]} + w_2^{[2]}a_2^{[1]} +
    w_3^{[2]}a_3^{[1]} + w_4^{[2]}a_4^{[1]}\right)
\end{split}
\]</span> In its full, expanded form it can get rather messy. <span class="math display">\[
\hat y=\sigma\left(w_0^{[2]}+
w_1^{[2]}\sigma\left(\mathbf{w}_1^{[1]}{'}\mathbf{x}\right) +
w_2^{[2]}\sigma\left(\mathbf{w}_2^{[1]}{'}\mathbf{x}\right) +
w_3^{[2]}\sigma\left(\mathbf{w}_3^{[1]}{'}\mathbf{x}\right) +
w_4^{[2]}\sigma\left(\mathbf{w}_4^{[1]}{'}\mathbf{x}\right)
\right)
\]</span> It gets a lot more complicated when there are several layers and a lot more nodes in each layer.</p>
<p>Fitting a neural network means optimizing the values of <span class="math inline">\(\mathbf{w}_{j}^{[1]}\)</span> and <span class="math inline">\(\mathbf{w}_{j}^{[2]}\)</span> to minimize a loss function.</p>
<p>This time for a loss function we will minimize the negative Bernoulli log-likelihood (sometimes referred to as cross-entropy for binary outputs in the neural network literature). <span class="math display">\[
J(\mathbf{y},\mathbf{\hat y})=-\sum_{i=1}^n y_i\log(\hat y_i) + (1-y_i)\log(1-\hat y_i)
\]</span> We will set <span class="math inline">\(\sigma(\cdot)\)</span> to be the sigmoid function. Now we need to compute the gradient with respect to all the parameters. Let’s start at the output and work backwards. Recall that the derivative of the sigmoid function is <span class="math inline">\(\sigma'(x)=\sigma(x)(1-\sigma(x))\)</span>. <span class="math display">\[
\begin{split}
    \frac{\partial J}{\partial \hat y_i} &amp;=
    -\frac{y_i}{\hat y_i} + \frac{1-y_i}{1-\hat y_i} \\
    &amp;= -\frac{y_i-\hat y_i}{\hat y_i(1-\hat y_i)}
\end{split}
\]</span> Now we need to move to the next level of parameters, the <span class="math inline">\(\mathbf{w}^{[2]}\)</span>. Note that <span class="math display">\[
\begin{split}
\frac{\partial \hat y_i}{\partial w_j^{[2]}} &amp;=
  \frac{\partial}{\partial w_j^{[2]}} \sigma\left(\mathbf{w}^{[2]}{'}\mathbf{a}_i^{[1]}\right) \\
  &amp;= \sigma\left(\mathbf{w}^{[2]}{'}\mathbf{a}_i^{[1]}\right)
  \left(1-\sigma\left(\mathbf{w}^{[2]}{'}\mathbf{a}_i^{[1]}\right)\right)a_{ij}^{[1]}\\
  &amp;= \hat y_i (1-\hat y_i)a_{ij}^{[1]}
\end{split}
\]</span> We can chain rule these together to get the gradient for the output layer weights. <span class="math display">\[
\begin{split}
    \frac{\partial J}{\partial w_j^{[2]}} &amp;=
    \frac{\partial J}{\partial \hat y_1}\frac{\partial \hat y_1}{\partial w_j^{[2]}} + \ldots +
    \frac{\partial J}{\partial \hat y_n}\frac{\partial \hat y_n}{\partial w_j^{[2]}} \\
    &amp;= -\frac{y_1-\hat y_1}{\hat y_1(1-\hat y_1)}
        \hat y_1 (1-\hat y_1)a_{1j}^{[1]}
         - \ldots
        -\frac{y_n-\hat y_n}{\hat y_n(1-\hat y_n)}
        \hat y_n (1-\hat y_n)a_{nj}^{[1]} \\
    &amp;= -(y_1-\hat y_1)a_{ij}^{[1]} - \ldots -(y_n-\hat y_n)a_{nj}^{[1]} \\
    &amp;= -\sum (y_i-\hat y_i)a_{ij}^{[1]}
\end{split}
\]</span> A rather complicated idea becomes a rather simple expression. To adjust the output layer weights to make the model perform a little better we need to update as <span class="math display">\[
    \mathbf{w}^{[2]} \leftarrow \mathbf{w}^{[2]} + \lambda\mathbf{A}^{[1]}{'}(\mathbf{y}-\mathbf{\hat y})
\]</span> Now that we have the algorithm for updating the output layer parameters, we can move backwards through the network to update the next layer’s parameters.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reminders
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\hat y_i = a_i^{[2]} = \sigma\left(\mathbf{w}^{[2]}{'}\mathbf{a}_i^{[1]}\right) = \sigma\left(w_1^{[2]}a_{i1}^{[1]} + \ldots + w_4^{[2]}a_{i4}^{[1]}\right)
\]</span></p>
<p><span class="math display">\[
a_{ij}^{[1]} = \sigma\left(\mathbf{w}_j^{[1]}{'}\mathbf{x}_i\right) = \sigma\left(w_1^{[1]}x_{i1} + w_2^{[1]}x_{i2}\right)
\]</span></p>
</div>
</div>
<p>Here we look at layer <span class="math inline">\([1]\)</span> and adjust the weight on input <span class="math inline">\(k\)</span> associated with hidden node <span class="math inline">\(j\)</span>. <span class="math display">\[
\begin{split}
    \frac{\partial J}{\partial w_{jk}^{[1]}} &amp;= \sum_{i=1}^n  
     \color{DodgerBlue}{\frac{\partial J}{\partial \hat y_i}}
     \color{Orange}{\frac{\partial\hat y_i}{\partial a_{ij}^{[1]}} }
     \color{SeaGreen}{\frac{\partial a_{ij}^{[1]}}{\partial w_{jk}^{[1]}}} \\
     &amp;= \sum_{i=1}^n \color{DodgerBlue}{-\frac{y_i-\hat y_i}{\hat y_i(1-\hat y_i)}}
        \color{Orange}{\hat y_i (1-\hat y_i)w_{j}^{[2]}}
        \color{SeaGreen}{a_{ij}^{[1]}(1-a_{ij}^{[1]})x_{ik}} \\
     &amp;= -w_{j}^{[2]}\sum_{i=1}^n (y_i-\hat y_i)
        a_{ij}^{[1]}(1-a_{ij}^{[1]})x_{ik}
\end{split}
\]</span> Then we update as <span class="math display">\[
    w_{jk}^{[1]} \leftarrow w_{jk}^{[1]} + \lambda w_{j}^{[2]}\sum_{i=1}^n (y_i-\hat y_i)a_{ij}^{[1]}(1-a_{ij}^{[1]})x_{ik}
\]</span> The components of this expression were already computed when we passed forward through the network to make predictions. Now we work backward through the network applying the chain rule over and over to compute the gradients for each layer’s parameters. This process of working backwards computing the gradient is the “backpropagation” algorithm <span class="citation" data-cites="rosenblatt1962principles">(<a href="#ref-rosenblatt1962principles" role="doc-biblioref">Rosenblatt 1962</a>)</span>. The particular nice property of the algorithm is that the gradient is (relatively) easy to calculate knowing the derivatives from the next layer. So, working backwards to get the updates results is reasonably efficient optimization.</p>
</section>
<section id="example-in-two-dimensions" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Example in two-dimensions</h1>
<p>We are going to be using the sigmoid a lot, so I am going to go ahead and make a helper function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>sigmoid <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>x))}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s generate a dataset with a non-linear boundary, one for which a logistic regression model would not be ideal.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20240402</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x0=</span><span class="dv">1</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">x1=</span><span class="fu">rnorm</span>(<span class="dv">1000</span>),</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">x2=</span><span class="fu">rnorm</span>(<span class="dv">1000</span>))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>d<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(d<span class="sc">$</span>x2 <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span>(<span class="fu">sigmoid</span>(<span class="dv">4</span><span class="sc">*</span>d<span class="sc">$</span>x1)<span class="sc">-</span><span class="fl">0.5</span>))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># show class separation</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(d<span class="sc">$</span>x1, d<span class="sc">$</span>x2, <span class="at">col=</span>col1and2[d<span class="sc">$</span>y<span class="sc">+</span><span class="dv">1</span>],</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-sigmoidSeparationSimulation1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sigmoidSeparationSimulation1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-sigmoidSeparationSimulation1-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sigmoidSeparationSimulation1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Simulated data with a sigmoid separation boundary
</figcaption>
</figure>
</div>
</div>
</div>
<p>We will use the algorithm we developed earlier to fit a “single-layer perceptron” model, like the one shown in <a href="#fig-linearnn" class="quarto-xref">Figure&nbsp;1</a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set some starting values</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>learnRate <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(d[,<span class="fu">c</span>(<span class="st">"x0"</span>,<span class="st">"x1"</span>,<span class="st">"x2"</span>)])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  yPred <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(d[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]) <span class="sc">%*%</span> beta <span class="sc">|&gt;</span> <span class="fu">as.numeric</span>()</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> beta <span class="sc">+</span> learnRate<span class="sc">*</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> (d<span class="sc">$</span>y<span class="sc">-</span>yPred))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(i <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">50</span>,<span class="dv">100</span>))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(d<span class="sc">$</span>x1, d<span class="sc">$</span>x2, <span class="at">col=</span>col1and2[(yPred<span class="sc">&gt;</span><span class="fl">0.5</span>) <span class="sc">+</span> <span class="dv">1</span>],</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>,</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>         <span class="at">main=</span><span class="fu">paste</span>(<span class="st">"Iteration:"</span>,i))</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="at">length.out=</span><span class="dv">100</span>),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>          <span class="dv">2</span><span class="sc">*</span>(<span class="fu">sigmoid</span>(<span class="dv">4</span><span class="sc">*</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="at">length.out=</span><span class="dv">100</span>))<span class="sc">-</span><span class="fl">0.5</span>))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-singlelayerperceptron" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-singlelayerperceptron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-singlelayerperceptron-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-singlelayerperceptron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Predicted values from linear perceptron model
</figcaption>
</figure>
</div>
</div>
</div>
<p>In <a href="#fig-singlelayerperceptron" class="quarto-xref">Figure&nbsp;5</a>, the black curve shows the optimal decision boundary. The blue and orange points show the predicted values. The decision boundary is as best as you can get with a linear model. With a single-layer perceptron in which all relationships are linear, this is the best we can do.</p>
</section>
<section id="a-more-complex-decision-boundary-in-two-dimensions" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> A more complex decision boundary in two dimensions</h1>
<section id="searching-for-a-linear-boundary" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="searching-for-a-linear-boundary"><span class="header-section-number">5.1</span> Searching for a linear boundary</h2>
<p>Let’s consider a new decision boundary with which a linear neural network will really struggle.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>d<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">with</span>(d, <span class="fu">as.numeric</span>(x1<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span>x2<span class="sc">^</span><span class="dv">2</span> <span class="sc">&gt;</span> <span class="dv">1</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(d<span class="sc">$</span>x1, d<span class="sc">$</span>x2, <span class="at">col=</span>col1and2[d<span class="sc">$</span>y<span class="sc">+</span><span class="dv">1</span>],</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-simulationCircularBoundary" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-simulationCircularBoundary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-simulationCircularBoundary-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-simulationCircularBoundary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Simulated circular classification boundary
</figcaption>
</figure>
</div>
</div>
</div>
<p>And let’s again try the same algorithm</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># learning algorithm for a single-layer perceptron</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>learnRate <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>yPred <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(d[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]) <span class="sc">%*%</span> beta <span class="sc">|&gt;</span> <span class="fu">as.numeric</span>()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(d[,<span class="fu">c</span>(<span class="st">"x0"</span>,<span class="st">"x1"</span>,<span class="st">"x2"</span>)])</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  yPred <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(d[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]) <span class="sc">%*%</span> beta <span class="sc">|&gt;</span> <span class="fu">as.numeric</span>()</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> beta <span class="sc">+</span> learnRate<span class="sc">*</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> (d<span class="sc">$</span>y<span class="sc">-</span>yPred))</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(i <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">20</span>,<span class="dv">25</span>,<span class="dv">30</span>,<span class="dv">40</span>,<span class="dv">50</span>))</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(d<span class="sc">$</span>x1, d<span class="sc">$</span>x2, <span class="at">col=</span>col1and2[(yPred<span class="sc">&gt;</span><span class="fl">0.5</span>) <span class="sc">+</span> <span class="dv">1</span>],</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>,</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>         <span class="at">main=</span><span class="fu">paste</span>(<span class="st">"Iteration:"</span>,i))</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">cos</span>(<span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>pi,<span class="at">length.out=</span><span class="dv">100</span>)),</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>          <span class="fu">sin</span>(<span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>pi,<span class="at">length.out=</span><span class="dv">100</span>)))</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-singlelayerperceptronCircle" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-singlelayerperceptronCircle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-singlelayerperceptronCircle-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-singlelayerperceptronCircle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Predicted values from linear perceptron model for circular decision boundary
</figcaption>
</figure>
</div>
</div>
</div>
<p>The mean of <code>y</code> for this simulation is 0.583. The final estimate of <span class="math inline">\(\beta\)</span> is 0.5800297, 0.0052191, 9.2792344^{-4}. In the end, the linear restrictions we have put on the neural network makes it abandon trying to separate the blue and the orange and just predicts all points to be in the majority class, orange.</p>
</section>
<section id="neural-network-with-hidden-layers" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="neural-network-with-hidden-layers"><span class="header-section-number">5.2</span> Neural network with hidden layers</h2>
<p>To remedy the problem with the linear neural network, we are going to fit the model shown in <a href="#fig-nnhiddenlayer1" class="quarto-xref">Figure&nbsp;2</a>, with a hidden layer and using the sigmoid activation function. I borrowed this code structure from this blog post on <a href="https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/">Building A Neural Net from Scratch Using R</a>. The code on the website has several bugs that have been fixed here.</p>
<p>We start by setting up some functions to help us along the way. In this code, the “bias” terms (the “intercept” terms) <code>b1</code> and <code>b2</code> are stored separately from the weights.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a function to get the number of nodes at each level, input, hidden, and output</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>getLayerSize <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, hidden_neurons) {</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  n_x <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  n_h <span class="ot">&lt;-</span> hidden_neurons</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  n_y <span class="ot">&lt;-</span> <span class="fu">nrow</span>(y)   </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>( <span class="fu">list</span>(<span class="at">n_x=</span>n_x, <span class="at">n_h=</span>n_h, <span class="at">n_y=</span>n_y) )</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># set up some starting values by randomly selecting numbers between -1 and 1</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>initializeParameters <span class="ot">&lt;-</span> <span class="cf">function</span>(X, list_layer_size)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>  m <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>  n_x <span class="ot">&lt;-</span> list_layer_size<span class="sc">$</span>n_x</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>  n_h <span class="ot">&lt;-</span> list_layer_size<span class="sc">$</span>n_h</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>  n_y <span class="ot">&lt;-</span> list_layer_size<span class="sc">$</span>n_y</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>  W1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n_h <span class="sc">*</span> n_x,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>               <span class="at">nrow =</span> n_h, <span class="at">ncol =</span> n_x, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="co">#* 0.01</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>  b1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="dv">0</span>, n_h), <span class="at">nrow =</span> n_h)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>  W2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n_y <span class="sc">*</span> n_h,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>               <span class="at">nrow =</span> n_y, <span class="at">ncol =</span> n_h, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="co">#* 0.01</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>  b2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="dv">0</span>, n_y), <span class="at">nrow =</span> n_y)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>( <span class="fu">list</span>(<span class="at">W1 =</span> W1, <span class="at">b1 =</span> b1, <span class="at">W2 =</span> W2, <span class="at">b2 =</span> b2) )</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co"># run all the input data forward through the network to get predictions</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>forwardPropagation <span class="ot">&lt;-</span> <span class="cf">function</span>(X, params, list_layer_size)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>  m <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>  n_h <span class="ot">&lt;-</span> list_layer_size<span class="sc">$</span>n_h</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>  n_y <span class="ot">&lt;-</span> list_layer_size<span class="sc">$</span>n_y</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>  W1 <span class="ot">&lt;-</span> params<span class="sc">$</span>W1</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>  b1 <span class="ot">&lt;-</span> params<span class="sc">$</span>b1</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>  W2 <span class="ot">&lt;-</span> params<span class="sc">$</span>W2</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>  b2 <span class="ot">&lt;-</span> params<span class="sc">$</span>b2</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>  b1_new <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(b1, m), <span class="at">nrow =</span> n_h)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>  b2_new <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(b2, m), <span class="at">nrow =</span> n_y)</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>  Z1 <span class="ot">&lt;-</span> W1 <span class="sc">%*%</span> X <span class="sc">+</span> b1_new</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>  A1 <span class="ot">&lt;-</span> <span class="fu">sigmoid</span>(Z1)</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>  Z2 <span class="ot">&lt;-</span> W2 <span class="sc">%*%</span> A1 <span class="sc">+</span> b2_new</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>  A2 <span class="ot">&lt;-</span> <span class="fu">sigmoid</span>(Z2)</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>( <span class="fu">list</span>(<span class="at">Z1 =</span> Z1, <span class="at">A1 =</span> A1, <span class="at">Z2 =</span> Z2, <span class="at">A2 =</span> A2) )</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the average negative Bernoulli log likelihood</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>computeCost <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, cache) </span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>  m <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>  A2 <span class="ot">&lt;-</span> cache<span class="sc">$</span>A2</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>  logprobs <span class="ot">&lt;-</span> y<span class="sc">*</span><span class="fu">log</span>(A2) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">-</span>y)<span class="sc">*</span><span class="fu">log</span>(<span class="dv">1</span><span class="sc">-</span>A2)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>  cost <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">sum</span>(logprobs<span class="sc">/</span>m)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span> (cost)</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a><span class="co"># apply the chain rule working backwards through the network to update weights</span></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>backwardPropagation <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, cache, params, list_layer_size)</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>  m <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>  n_x <span class="ot">&lt;-</span> list_layer_size<span class="sc">$</span>n_x</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>  n_h <span class="ot">&lt;-</span> list_layer_size<span class="sc">$</span>n_h</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>  n_y <span class="ot">&lt;-</span> list_layer_size<span class="sc">$</span>n_y</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>  A2 <span class="ot">&lt;-</span> cache<span class="sc">$</span>A2</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>  A1 <span class="ot">&lt;-</span> cache<span class="sc">$</span>A1</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>  W2 <span class="ot">&lt;-</span> params<span class="sc">$</span>W2</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>  dZ2 <span class="ot">&lt;-</span> A2 <span class="sc">-</span> y</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>  dW2 <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>m <span class="sc">*</span> (dZ2 <span class="sc">%*%</span> <span class="fu">t</span>(A1)) </span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>  db2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">1</span><span class="sc">/</span>m <span class="sc">*</span> <span class="fu">sum</span>(dZ2), <span class="at">nrow =</span> n_y)</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>  db2_new <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(db2, m), <span class="at">nrow =</span> n_y)</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a><span class="co">#  dZ1 &lt;- (t(W2) %*% dZ2) * (1 - A1^2)</span></span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>  dZ1 <span class="ot">&lt;-</span> (<span class="fu">t</span>(W2) <span class="sc">%*%</span> dZ2) <span class="sc">*</span> A1<span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> A1)</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>  dW1 <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>m <span class="sc">*</span> (dZ1 <span class="sc">%*%</span> <span class="fu">t</span>(X))</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>  db1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">1</span><span class="sc">/</span>m <span class="sc">*</span> <span class="fu">rowSums</span>(dZ1), <span class="at">nrow =</span> n_h)</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>  db1_new <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(db1, m), <span class="at">nrow =</span> n_h)</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>( <span class="fu">list</span>(<span class="at">dW1 =</span> dW1, <span class="at">db1 =</span> db1, <span class="at">dW2 =</span> dW2, <span class="at">db2 =</span> db2) )</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="co"># take a gradient descent step</span></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>updateParameters <span class="ot">&lt;-</span> <span class="cf">function</span>(grads, params, learning_rate)</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>  W1 <span class="ot">&lt;-</span> params<span class="sc">$</span>W1</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>  b1 <span class="ot">&lt;-</span> params<span class="sc">$</span>b1</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>  W2 <span class="ot">&lt;-</span> params<span class="sc">$</span>W2</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>  b2 <span class="ot">&lt;-</span> params<span class="sc">$</span>b2</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>  dW1 <span class="ot">&lt;-</span> grads<span class="sc">$</span>dW1</span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>  db1 <span class="ot">&lt;-</span> grads<span class="sc">$</span>db1</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a>  dW2 <span class="ot">&lt;-</span> grads<span class="sc">$</span>dW2</span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>  db2 <span class="ot">&lt;-</span> grads<span class="sc">$</span>db2</span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a>  W1 <span class="ot">&lt;-</span> W1 <span class="sc">-</span> learning_rate <span class="sc">*</span> dW1</span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a>  b1 <span class="ot">&lt;-</span> b1 <span class="sc">-</span> learning_rate <span class="sc">*</span> db1</span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>  W2 <span class="ot">&lt;-</span> W2 <span class="sc">-</span> learning_rate <span class="sc">*</span> dW2</span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>  b2 <span class="ot">&lt;-</span> b2 <span class="sc">-</span> learning_rate <span class="sc">*</span> db2</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>( <span class="fu">list</span>(<span class="at">W1 =</span> W1, <span class="at">b1 =</span> b1, <span class="at">W2 =</span> W2, <span class="at">b2 =</span> b2) )</span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that all those functions are set up, let’s put all the steps in order and do one backpropagation step.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set up the data</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(d<span class="sc">$</span>x1, d<span class="sc">$</span>x2))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(d<span class="sc">$</span>y, <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">t</span>(X)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">t</span>(y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Step 0. Set up the network</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>layer_size <span class="ot">&lt;-</span> <span class="fu">getLayerSize</span>(X, y, <span class="at">hidden_neurons =</span> <span class="dv">4</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>layer_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$n_x
[1] 2

$n_h
[1] 4

$n_y
[1] 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>curr_params <span class="ot">&lt;-</span> <span class="fu">initializeParameters</span>(X, layer_size)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>curr_params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$W1
           [,1]       [,2]
[1,]  0.1086112  0.3794711
[2,] -0.9294748 -0.7302107
[3,]  0.3111992  0.6331077
[4,]  0.8354082  0.1209820

$b1
     [,1]
[1,]    0
[2,]    0
[3,]    0
[4,]    0

$W2
            [,1]       [,2]       [,3]        [,4]
[1,] -0.07868139 0.04723443 0.05089913 -0.02138735

$b2
     [,1]
[1,]    0</code></pre>
</div>
</div>
<p>Step 1. Make predictions moving forward, storing key intermediate values</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>fwd_prop <span class="ot">&lt;-</span> <span class="fu">forwardPropagation</span>(X, curr_params, layer_size)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the linear combinations of inputs</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>fwd_prop<span class="sc">$</span>Z1[,<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]         [,2]       [,3]        [,4]        [,5]
[1,] -0.4632792  0.001896368  0.3991996  0.05738323 -0.06177666
[2,]  0.1696516 -0.416297471 -2.0773103  0.11969689 -0.21375627
[3,] -0.6426956  0.077616547  0.9022255  0.05421843 -0.04305224
[4,]  0.6545844  0.459247219  1.5823260 -0.23747315  0.35001233</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sigmoid transform of the linear combinations of inputs</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>fwd_prop<span class="sc">$</span>A1[,<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]      [,2]      [,3]      [,4]      [,5]
[1,] 0.3862082 0.5004741 0.5984954 0.5143419 0.4845607
[2,] 0.5423115 0.3974031 0.1113218 0.5298885 0.4467635
[3,] 0.3446375 0.5193944 0.7114066 0.5135513 0.4892386
[4,] 0.6580428 0.6128356 0.8295337 0.4409091 0.5866206</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># linear transform of hidden layer</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>fwd_prop<span class="sc">$</span>Z2[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.001303673 -0.007277098 -0.023363776  0.001269283 -0.004667737</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sigmoid transform of linear combo from hidden layer</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>fwd_prop<span class="sc">$</span>A2[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4996741 0.4981807 0.4941593 0.5003173 0.4988331</code></pre>
</div>
</div>
<p>Step 2. Evaluate loss function</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>cost <span class="ot">&lt;-</span> <span class="fu">computeCost</span>(X, y, fwd_prop)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>cost</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6933894</code></pre>
</div>
</div>
<p>Step 3. Compute gradient moving back through the network</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>back_prop <span class="ot">&lt;-</span> <span class="fu">backwardPropagation</span>(X, y, fwd_prop, curr_params, layer_size)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient for the w1</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>back_prop<span class="sc">$</span>dW1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              [,1]          [,2]
[1,]  2.706173e-04  3.478153e-05
[2,] -1.091090e-04  1.241427e-05
[3,] -1.524197e-04 -1.168966e-05
[4,]  7.629919e-05 -1.840025e-05</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient for the b1</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>back_prop<span class="sc">$</span>db1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              [,1]
[1,]  1.366440e-03
[2,] -5.919621e-05
[3,] -5.751186e-04
[4,]  1.765536e-04</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient for the w2</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>back_prop<span class="sc">$</span>dW2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]        [,2]        [,3]        [,4]
[1,] -0.04223389 -0.03869427 -0.04296879 -0.04469599</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient for the b2</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>back_prop<span class="sc">$</span>db2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]
[1,] -0.08326909</code></pre>
</div>
</div>
<p>Step 4. Gradient descent step</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>curr_params <span class="ot">&lt;-</span> <span class="fu">updateParameters</span>(back_prop, curr_params, <span class="at">learning_rate =</span> <span class="fl">0.01</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>curr_params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$W1
           [,1]       [,2]
[1,]  0.1086085  0.3794707
[2,] -0.9294737 -0.7302108
[3,]  0.3112008  0.6331078
[4,]  0.8354074  0.1209822

$b1
              [,1]
[1,] -1.366440e-05
[2,]  5.919621e-07
[3,]  5.751186e-06
[4,] -1.765536e-06

$W2
            [,1]       [,2]       [,3]        [,4]
[1,] -0.07825906 0.04762137 0.05132882 -0.02094039

$b2
             [,1]
[1,] 0.0008326909</code></pre>
</div>
</div>
<p>And that’s one step of the backpropagation algorithm. Repeating Steps 1, 2, 3, and 4 will push the parameters towards values that improve the fit of the neural network to the data.</p>
<p>It is possible to create marvelously complex neural networks and apply the chain rule like crazy to tune the parameters of the neural network to fit the data. We use gradient descent, just like we did for optimizing logistic regression models, to learn the neural network. Most implementations of backpropagation today use some variation of <em>stochastic gradient descent</em>. Stochastic gradient descent looks largely the same as described here, except that the gradient is computed using a subsample of the data rather than the entire dataset. This can greatly speed up the computation and avoid locally minima, but can require more iterations to converge.</p>
<p>Let’s wrap these gradient steps into a <code>trainModel()</code> function that will iterate for <code>num_iterations</code> with a learning rate of <code>lr</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>trainModel <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, num_iteration, hidden_neurons, lr)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>  layer_size <span class="ot">&lt;-</span> <span class="fu">getLayerSize</span>(X, y, hidden_neurons)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  init_params <span class="ot">&lt;-</span> <span class="fu">initializeParameters</span>(X, layer_size)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>  cost_history <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, num_iteration)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iteration) {</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    fwd_prop <span class="ot">&lt;-</span> <span class="fu">forwardPropagation</span>(X, init_params, layer_size)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    cost <span class="ot">&lt;-</span> <span class="fu">computeCost</span>(X, y, fwd_prop)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    back_prop <span class="ot">&lt;-</span> <span class="fu">backwardPropagation</span>(X, y, fwd_prop, init_params, layer_size)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    update_params <span class="ot">&lt;-</span> <span class="fu">updateParameters</span>(back_prop, init_params, <span class="at">learning_rate =</span> lr)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    init_params <span class="ot">&lt;-</span> update_params</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    cost_history[i] <span class="ot">&lt;-</span> cost</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="sc">%%</span> <span class="dv">10000</span> <span class="sc">==</span> <span class="dv">0</span>) <span class="fu">cat</span>(<span class="st">"Iteration"</span>, i, <span class="st">" | Cost: "</span>, cost, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>  model_out <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="st">"updated_params"</span> <span class="ot">=</span> update_params,</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"cost_hist"</span> <span class="ot">=</span> cost_history)</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span> (model_out)</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>makePrediction <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, modNN, hidden_neurons)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>  layer_size <span class="ot">&lt;-</span> <span class="fu">getLayerSize</span>(X, y, </span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">nrow</span>(modNN<span class="sc">$</span>updated_params<span class="sc">$</span>W1))</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>  params <span class="ot">&lt;-</span> modNN<span class="sc">$</span>updated_params</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>  fwd_prop <span class="ot">&lt;-</span> <span class="fu">forwardPropagation</span>(X, params, layer_size)</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(fwd_prop<span class="sc">$</span>A2)</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s first try fitting the neural net to the “circle” data, first using a network with two hidden nodes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>train_model <span class="ot">&lt;-</span> <span class="fu">trainModel</span>(X, y,</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">hidden_neurons =</span> <span class="dv">2</span>, <span class="co"># are two hidden nodes enough?</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">num_iteration =</span> <span class="dv">50000</span>, <span class="at">lr =</span> <span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration 10000  | Cost:  0.4590469 
Iteration 20000  | Cost:  0.449592 
Iteration 30000  | Cost:  0.4373529 
Iteration 40000  | Cost:  0.4355029 
Iteration 50000  | Cost:  0.4343475 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(train_model<span class="sc">$</span>cost_hist, <span class="at">type=</span><span class="st">"l"</span>,</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"Iteration"</span>, <span class="at">ylab=</span><span class="st">"Negative Bernoulli log likelihood"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-neuralnetIterationxLL" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-neuralnetIterationxLL-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-neuralnetIterationxLL-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neuralnetIterationxLL-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Negative Bernoulli log likelihood by gradient descent iteration, neural network with 2 hidden nodes
</figcaption>
</figure>
</div>
</div>
</div>
<p>The plot shows the reduction in the negative Bernoulli log likelihood with each iteration. But did it fit the data well?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="ot">&lt;-</span> <span class="fu">makePrediction</span>(X, y, train_model)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X[<span class="dv">1</span>,], X[<span class="dv">2</span>,], <span class="at">col=</span>col1and2[(y_pred[<span class="dv">1</span>,]<span class="sc">&gt;</span><span class="fl">0.5</span>) <span class="sc">+</span> <span class="dv">1</span>],</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">cos</span>(<span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>pi,<span class="at">length.out=</span><span class="dv">100</span>)),</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">sin</span>(<span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>pi,<span class="at">length.out=</span><span class="dv">100</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-nnFitCircular2HiddenNodes" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nnFitCircular2HiddenNodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-nnFitCircular2HiddenNodes-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nnFitCircular2HiddenNodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Predictions from a neural network with two hidden nodes
</figcaption>
</figure>
</div>
</div>
</div>
<p>Clearly with two hidden nodes, the neural network does not have sufficient complexity or capacity to capture the elliptical boundary. So, let’s try again with a network with four hidden nodes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>train_model <span class="ot">&lt;-</span> <span class="fu">trainModel</span>(X, y,</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">hidden_neurons =</span> <span class="dv">4</span>,</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">num_iteration =</span> <span class="dv">50000</span>, <span class="at">lr =</span> <span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration 10000  | Cost:  0.138251 
Iteration 20000  | Cost:  0.08347349 
Iteration 30000  | Cost:  0.06709865 
Iteration 40000  | Cost:  0.05971714 
Iteration 50000  | Cost:  0.05542842 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(train_model<span class="sc">$</span>cost_hist, <span class="at">type=</span><span class="st">"l"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-neuralnetIterationxLL4HiddenNodes" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-neuralnetIterationxLL4HiddenNodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-neuralnetIterationxLL4HiddenNodes-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neuralnetIterationxLL4HiddenNodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Negative Bernoulli log likelihood by gradient descent iteration, neural network with 4 hidden nodes
</figcaption>
</figure>
</div>
</div>
</div>
<p>We seem to get better predictive performance here since we got the cost much lower. Did we successfully capture the elliptical boundary?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="ot">&lt;-</span> <span class="fu">makePrediction</span>(X, y, train_model)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>dAll <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="fl">4.1</span>,<span class="fl">4.1</span>,<span class="at">length.out=</span><span class="dv">100</span>),</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">x2=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="fl">4.1</span>,<span class="fl">4.1</span>,<span class="at">length.out=</span><span class="dv">100</span>))</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>dAll<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>dAll<span class="sc">$</span>y_pred <span class="ot">&lt;-</span> <span class="fu">makePrediction</span>(<span class="fu">t</span>(<span class="fu">as.matrix</span>(dAll[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])), </span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">matrix</span>(dAll<span class="sc">$</span>y,<span class="at">nrow=</span><span class="dv">1</span>), </span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>                              train_model) <span class="sc">|&gt;</span> <span class="fu">as.numeric</span>()</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dAll<span class="sc">$</span>x1, dAll<span class="sc">$</span>x2, <span class="at">col=</span>col1and2transparent[(dAll<span class="sc">$</span>y_pred<span class="sc">&gt;</span><span class="fl">0.5</span>) <span class="sc">+</span> <span class="dv">1</span>],</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>, <span class="at">pch=</span><span class="dv">15</span>)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(X[<span class="dv">1</span>,], X[<span class="dv">2</span>,], <span class="at">col=</span>col1and2[(y[<span class="dv">1</span>,]<span class="sc">&gt;</span><span class="fl">0.5</span>) <span class="sc">+</span> <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-nnFitCircular4HiddenNodes" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nnFitCircular4HiddenNodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-nnFitCircular4HiddenNodes-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nnFitCircular4HiddenNodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Predictions from a neural network with four hidden nodes
</figcaption>
</figure>
</div>
</div>
</div>
<p>Yes! With four hidden nodes, our neural network has enough “brain power” to capture the concept of an elliptical boundary separating the blue and orange points.</p>
<p>Let’s try this out on a more complex shape.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>planar_dataset <span class="ot">&lt;-</span> <span class="cf">function</span>(){</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="fu">set.seed</span>(<span class="dv">20240402</span>))</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>  m <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>  N <span class="ot">&lt;-</span> m<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>  D <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> m, <span class="at">ncol =</span> D)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> m, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>  a <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>){</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    ix <span class="ot">&lt;-</span> <span class="fu">seq</span>((N<span class="sc">*</span>j)<span class="sc">+</span><span class="dv">1</span>, N<span class="sc">*</span>(j<span class="sc">+</span><span class="dv">1</span>))</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    t <span class="ot">&lt;-</span> <span class="fu">seq</span>(j<span class="sc">*</span><span class="fl">3.12</span>,(j<span class="sc">+</span><span class="dv">1</span>)<span class="sc">*</span><span class="fl">3.12</span>,<span class="at">length.out =</span> N) <span class="sc">+</span> <span class="fu">rnorm</span>(N, <span class="at">sd =</span> <span class="fl">0.2</span>)</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    r <span class="ot">&lt;-</span> a<span class="sc">*</span><span class="fu">sin</span>(<span class="dv">4</span><span class="sc">*</span>t) <span class="sc">+</span> <span class="fu">rnorm</span>(N, <span class="at">sd =</span> <span class="fl">0.2</span>)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>    X[ix,<span class="dv">1</span>] <span class="ot">&lt;-</span> r<span class="sc">*</span><span class="fu">sin</span>(t)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    X[ix,<span class="dv">2</span>] <span class="ot">&lt;-</span> r<span class="sc">*</span><span class="fu">cos</span>(t)</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>    Y[ix,] <span class="ot">&lt;-</span> j</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(X, Y))</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">names</span>(d) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">'X1'</span>,<span class="st">'X2'</span>,<span class="st">'Y'</span>)</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>  d</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">planar_dataset</span>()</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">as.matrix</span>(df[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]))</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">t</span>(df[,<span class="dv">3</span>])</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X[<span class="dv">1</span>,], X[<span class="dv">2</span>,], <span class="at">col=</span>col1and2[y[<span class="dv">1</span>,] <span class="sc">+</span> <span class="dv">1</span>],</span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-simulatedFlower" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-simulatedFlower-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-simulatedFlower-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-simulatedFlower-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Simulated data with a complex classification boundary
</figcaption>
</figure>
</div>
</div>
</div>
<p>This one has a more complex decision boundary, but let’s see if four hidden nodes are sufficient.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>train_model <span class="ot">&lt;-</span> <span class="fu">trainModel</span>(X, y,</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">hidden_neurons =</span> <span class="dv">4</span>,</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">num_iteration =</span> <span class="dv">50000</span>, <span class="at">lr =</span> <span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration 10000  | Cost:  0.2863198 
Iteration 20000  | Cost:  0.2676711 
Iteration 30000  | Cost:  0.2595525 
Iteration 40000  | Cost:  0.25454 
Iteration 50000  | Cost:  0.2509547 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(train_model<span class="sc">$</span>cost_hist, <span class="at">type=</span><span class="st">"l"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-neuralnetIterationxLLFlower4HiddenNodes" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-neuralnetIterationxLLFlower4HiddenNodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-neuralnetIterationxLLFlower4HiddenNodes-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neuralnetIterationxLLFlower4HiddenNodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Negative Bernoulli log likelihood by gradient descent iteration, neural network with 4 hidden nodes, “flower shaped” data
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="ot">&lt;-</span> <span class="fu">makePrediction</span>(X, y, train_model)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>dAll <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="fl">4.1</span>,<span class="fl">4.1</span>,<span class="at">length.out=</span><span class="dv">100</span>),</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">x2=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="fl">4.1</span>,<span class="fl">4.1</span>,<span class="at">length.out=</span><span class="dv">100</span>))</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>dAll<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>dAll<span class="sc">$</span>y_pred <span class="ot">&lt;-</span> <span class="fu">makePrediction</span>(<span class="fu">t</span>(<span class="fu">as.matrix</span>(dAll[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])), </span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">matrix</span>(dAll<span class="sc">$</span>y,<span class="at">nrow=</span><span class="dv">1</span>), </span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>                              train_model) <span class="sc">|&gt;</span> <span class="fu">as.numeric</span>()</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dAll<span class="sc">$</span>x1, dAll<span class="sc">$</span>x2, <span class="at">col=</span>col1and2transparent[(dAll<span class="sc">$</span>y_pred<span class="sc">&gt;</span><span class="fl">0.5</span>) <span class="sc">+</span> <span class="dv">1</span>],</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>, <span class="at">pch=</span><span class="dv">15</span>)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(X[<span class="dv">1</span>,], X[<span class="dv">2</span>,], <span class="at">col=</span>col1and2[(y[<span class="dv">1</span>,]<span class="sc">&gt;</span><span class="fl">0.5</span>) <span class="sc">+</span> <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-nnFitFlower4HiddenNodes" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nnFitFlower4HiddenNodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-nnFitFlower4HiddenNodes-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nnFitFlower4HiddenNodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Predictions from a neural network with four hidden nodes
</figcaption>
</figure>
</div>
</div>
</div>
<p>Yes! Four hidden nodes are sufficient to capture this pattern too.</p>
</section>
</section>
<section id="rs-neuralnet-package" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> R’s <code>neuralnet</code> package</h1>
<p>Now that we have explored fitting a neural network “by hand,” in this section we will cover using existing software to fit neural networks.</p>
<p>First, we will walk through using the <code>neuralnet</code> package. There is also a <code>nnet</code> package, but it allows only one hidden layer. The <code>neuralnet</code> package simply allows for more flexibility. After that, we will experiment with Keras, a professional grade neural network system regularly used in scientific analyses.</p>
<p>Start by loading the neuralnet package and revisiting our circular decision boundary dataset.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(neuralnet)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="co"># our circular decision boundary</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(d<span class="sc">$</span>x1, d<span class="sc">$</span>x2, <span class="at">col=</span>col1and2[d<span class="sc">$</span>y<span class="sc">+</span><span class="dv">1</span>],</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-simulationCircularBoundary2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-simulationCircularBoundary2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-simulationCircularBoundary2-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-simulationCircularBoundary2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Simulated circular classification boundary
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now we will fit a neural net with a single hidden layer with four nodes using squared error loss and the sigmoid activation function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>nn1 <span class="ot">&lt;-</span> <span class="fu">neuralnet</span>(y <span class="sc">~</span> x1<span class="sc">+</span>x2,</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data=</span>d,</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">hidden=</span><span class="dv">4</span>,    </span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">linear.output =</span> <span class="cn">FALSE</span>, <span class="co"># apply sigmoid to output</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">stepmax =</span> <span class="dv">1000000</span>,</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">err.fct=</span><span class="st">"sse"</span>,         <span class="co"># squared error</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">act.fct=</span><span class="st">"logistic"</span>,    <span class="co"># sigmoid</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>                 <span class="at">lifesign=</span><span class="st">"minimal"</span>)    <span class="co"># how much detail to print</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>hidden: 4    thresh: 0.01    rep: 1/1    steps:  176764 error: 0.04531  time: 1.85 mins</code></pre>
</div>
</div>
<p>The plot function will draw the network graph for us with the coefficients on all of the edges.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nn1,</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">show.weights =</span> <span class="cn">TRUE</span>,</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">information =</span> <span class="cn">FALSE</span>,</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">col.entry.synapse =</span> col2,</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">col.out.synapse =</span> col2,</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">col.hidden =</span> col1,</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">col.hidden.synapse =</span> <span class="st">"black"</span>,</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">fill =</span> col1,</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">rep=</span><span class="st">"best"</span>) <span class="co"># "best" request plot of single best, rather than all</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-drawNeuralNetwork" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-drawNeuralNetwork-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-drawNeuralNetwork-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-drawNeuralNetwork-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Neural network estimated with <code>neuralnet()</code>
</figcaption>
</figure>
</div>
</div>
</div>
<p>Create a plot to show the decision boundary.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>dAll <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="fl">4.1</span>,<span class="fl">4.1</span>,<span class="at">length.out=</span><span class="dv">100</span>),</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">x2=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="fl">4.1</span>,<span class="fl">4.1</span>,<span class="at">length.out=</span><span class="dv">100</span>))</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>dAll<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>dAll<span class="sc">$</span>y_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(nn1, <span class="at">newdata =</span> dAll)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dAll<span class="sc">$</span>x1, dAll<span class="sc">$</span>x2, <span class="at">col=</span>col1and2transparent[dAll<span class="sc">$</span>y_pred <span class="sc">+</span> <span class="dv">1</span>],</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>, <span class="at">pch=</span><span class="dv">15</span>)</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(d<span class="sc">$</span>x1, d<span class="sc">$</span>x2, <span class="at">col=</span>col1and2[(d<span class="sc">$</span>y<span class="sc">&gt;</span><span class="fl">0.5</span>) <span class="sc">+</span> <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-nnFitCircularNNpackage" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nnFitCircularNNpackage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-nnFitCircularNNpackage-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nnFitCircularNNpackage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Predictions from a neural network from the <code>neuralnet</code> package with four hidden nodes
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="tensorflow-and-keras" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Tensorflow and Keras</h1>
<p><a href="https://www.tensorflow.org/">Tensorflow</a> is a Google product for efficient machine learning. It allows for computation with GPUs, but current GPU functionality on Windows is broken and removed. <a href="https://keras.io/">Keras</a> is a set of Python tools for communicating what neural network structure you want. It then translates that structure into a neural network model that can be fit with Tensorflow (or with <a href="https://pytorch.org/">PyTorch</a>, Facebook AI group’s Python tools for machine learning). I encourage you to read the documentation and explore demos available for all of these tools.</p>
<p>A lot of the scientific community using deep learning works in Python. For that reason, you will find a lot of Python resources for developing neural networks. Since most social science research is conducted in R, I focused on R for this course. We will still use the R Keras library, which hooks into Python, which hooks into Keras, which hooks into Tensorflow. With all of your variations in computers, operating systems, and settings there is a lot of opportunity for some settings, versions, and options to not be compatible. I will have limited ability to troubleshoot why Keras, Tensorflow, or Python is giving you errors. In this case you will have to learn how to learn to figure these things out.</p>
<p>For the most serious deep learning analysis, that work is done directly in Tensorflow. So, if you really want to learn this area well, start with Keras and then start digging into working with Tensorflow (or PyTorch).</p>
<p>Tensorflow and Keras are usually behind a version or two of Python. Python 3.13 is the current version, but Keras/Tensorflow support up to Python 3.12. This constantly changes. I will be using Python 3.11, because I know it works.</p>
<section id="tensorflow-playground" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="tensorflow-playground"><span class="header-section-number">7.1</span> Tensorflow Playground</h2>
<p>Visit the (Tensorflow Playground)[https://playground.tensorflow.org]. This let’s you freely experiment with datasets with different shapes for their classification boundaries, change the number of hidden layers and the number of nodes in each layer, and see the effect on the network’s ability to learn the decision boundary.</p>
<p>Challenge: Using only <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> as inputs, can you alter the number of the hidden layers and nodes that will successful learn the spiral pattern?</p>
</section>
<section id="installing-tensorflow-and-keras" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="installing-tensorflow-and-keras"><span class="header-section-number">7.2</span> Installing Tensorflow and Keras</h2>
<p>First, we will do a one-time installation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">"keras"</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co"># March 2024, Keras works with 3.12... sticking with 3.11 for now</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="fu">install_keras</span>(<span class="at">python_version=</span><span class="st">"3.11"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You will need to restart R one last time after this installation.</p>
<p>Now we can get busy with Keras by loading the library. If all is installed correctly, then this line will run with no errors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mnist-postal-digits-data" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="mnist-postal-digits-data"><span class="header-section-number">7.3</span> MNIST postal digits data</h2>
<p>We are going to experiment with the NIST postal digits data (<a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST database</a>). Why? Because it seems to be a rite of passage for anyone working on neural networks. Everyone has to run the MNIST problem at some point. It is a set of 60,000 28x28 grayscale images handwritten digits. There is also a test set of 10,000 images. The Keras library comes with the MNIST database. We can load the dataset and print out one of the images.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>numData <span class="ot">&lt;-</span> <span class="fu">dataset_mnist</span>()</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>xTrain <span class="ot">&lt;-</span> numData<span class="sc">$</span>train<span class="sc">$</span>x</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>yTrain <span class="ot">&lt;-</span> numData<span class="sc">$</span>train<span class="sc">$</span>y</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>xTest <span class="ot">&lt;-</span> numData<span class="sc">$</span>test<span class="sc">$</span>x</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>yTest <span class="ot">&lt;-</span> numData<span class="sc">$</span>test<span class="sc">$</span>y</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>img <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(xTrain[i,,], <span class="dv">2</span>, rev))</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, img, </span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="fu">gray</span>((<span class="dv">255</span><span class="sc">:</span><span class="dv">0</span>) <span class="sc">/</span> <span class="dv">255</span>), </span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">axes =</span> <span class="cn">FALSE</span>,</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab=</span><span class="st">""</span>,<span class="at">ylab=</span><span class="st">""</span>,</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>      <span class="at">main=</span>yTrain[i])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-MNIST4" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-MNIST4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-MNIST4-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-MNIST4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: An example number 4 from the MNIST data
</figcaption>
</figure>
</div>
</div>
</div>
<p>That gives us a rough image of a handwritten number 4. Let’s take a look at a number of other handwritten digits to get an idea of what these images look like.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">5</span>), <span class="at">mai=</span><span class="fl">0.02</span><span class="sc">+</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">0</span>))</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">25</span><span class="sc">+</span><span class="dv">25</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>  img <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(xTrain[i,,], <span class="dv">2</span>, rev))</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">image</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, img, </span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">gray</span>((<span class="dv">255</span><span class="sc">:</span><span class="dv">0</span>) <span class="sc">/</span> <span class="dv">255</span>), </span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">xaxt =</span> <span class="st">'n'</span>, <span class="at">yaxt =</span> <span class="st">'n'</span>,</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">""</span>,<span class="at">ylab=</span><span class="st">""</span>,</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">4</span>,<span class="dv">0</span>)<span class="sc">+</span><span class="fl">0.01</span>,</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">main=</span>yTrain[i])</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-MNIST25" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-MNIST25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-MNIST25-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-MNIST25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: 25 examples from the MNIST data
</figcaption>
</figure>
</div>
</div>
</div>
<p>We need to do a little restructuring of the dataset. Much like we did with the emoji data, we are going to stretch these data out wide, but we need to pay attention to how R stores array data.</p>
<p>I am going to make a little array with three “images.” The first image will have the numbers 1 to 4, the second 5 to 8, and the third 9 to 12.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">1</span>,,] <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">ncol=</span><span class="dv">2</span>,<span class="at">byrow=</span><span class="cn">TRUE</span>)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">2</span>,,] <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">5</span><span class="sc">:</span><span class="dv">8</span>, <span class="at">ncol=</span><span class="dv">2</span>,<span class="at">byrow=</span><span class="cn">TRUE</span>)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">3</span>,,] <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">9</span><span class="sc">:</span><span class="dv">12</span>,<span class="at">ncol=</span><span class="dv">2</span>,<span class="at">byrow=</span><span class="cn">TRUE</span>)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>, , 1

     [,1] [,2]
[1,]    1    3
[2,]    5    7
[3,]    9   11

, , 2

     [,1] [,2]
[1,]    2    4
[2,]    6    8
[3,]   10   12</code></pre>
</div>
</div>
<p>When we print out a three dimensional array in R, it prints it out so that the last index changes the “fastest.” It first shows <code>a[,,1]</code> and then <code>a[,,2]</code>. To work with Keras, when we stretch the array out wide we need to put the array in “C” format in which the last index changes the fastest.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">array_reshape</span>(a, <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">2</span><span class="sc">*</span><span class="dv">2</span>), <span class="at">order=</span><span class="st">"C"</span>) <span class="co"># C = last index changes fastest</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    5    6    7    8
[3,]    9   10   11   12</code></pre>
</div>
</div>
<p>Now we can apply this reformatting to our MNISt data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(xTrain)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 60000    28    28</code></pre>
</div>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>xTrain <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(xTrain, <span class="fu">c</span>(<span class="dv">60000</span>, <span class="dv">28</span><span class="sc">*</span><span class="dv">28</span>)) <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>xTest  <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(xTest,  <span class="fu">c</span>(<span class="dv">10000</span>, <span class="dv">28</span><span class="sc">*</span><span class="dv">28</span>)) <span class="sc">/</span> <span class="dv">255</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We also need to convert our outcome values to be 0/1 indicators. So, rather than the outcome being “4,” we are going to create a vector with 10 numbers, all which are 0 except for the fourth one, which we will set to 1.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># converts outcome to 0/1 coding</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>yTrain <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(yTrain)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>yTest <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(yTest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we will set up the neural network, describing our input data, the number of nodes in the hidden layer (512), setting the activation function (ReLU), and insist that the output predictions sum to one (softmax) so that we have a probability for each number of each image.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>keras1 <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>(<span class="at">input_shape =</span> <span class="dv">28</span><span class="sc">*</span><span class="dv">28</span>) <span class="sc">|&gt;</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 512 hidden nodes</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">512</span>, <span class="at">activation =</span> <span class="st">"relu"</span>) <span class="sc">|&gt;</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># randomly set 20% of nodes to 0, supposedly reduces overfitting</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="fl">0.2</span>) <span class="sc">|&gt;</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># softmax makes sure outputs sum to 1</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">10</span>, <span class="at">activation =</span> <span class="st">"softmax"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can ask Keras to describe the model we are about to fit.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(keras1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential"
________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     
================================================================================
 dense_1 (Dense)                    (None, 512)                     401920      
 dropout (Dropout)                  (None, 512)                     0           
 dense (Dense)                      (None, 10)                      5130        
================================================================================
Total params: 407050 (1.55 MB)
Trainable params: 407050 (1.55 MB)
Non-trainable params: 0 (0.00 Byte)
________________________________________________________________________________</code></pre>
</div>
</div>
<p>Then we tell Keras about how we want it to optimize the model and evaluate its performance. rmsprop is Root Mean Square Propagation. It is a popular variant of backpropagation that regularly adjusts the learning rate parameter. Categorical cross-entropy is the multinomial generalization of the negative Bernoulli log likelihood. We can also have Keras track other metrics, like here I ask it to track accuracy.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">compile</span>(keras1,</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">optimizer =</span> <span class="st">"rmsprop"</span>,</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">loss =</span> <span class="st">"categorical_crossentropy"</span>,</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">metrics =</span> <span class="st">"accuracy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ready to go! It’s now time to tell Keras to actually fit the model. The <code>batch_size</code> is the number of training observations used in one forward and backward pass through the neural network. This is called “mini-batch gradient descent.” <code>epochs</code> is the number of full passes through the dataset. Since there are 60,000 images, and we are using 80% of them for training and 20% for validation, and we have a batch size of 128, you will see each epoch require 375 (= 60000*0.8/128) updates within each epoch.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>fitHx <span class="ot">&lt;-</span> <span class="fu">fit</span>(keras1,</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>  xTrain,</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>  yTrain,</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">20</span>,</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> <span class="fl">0.2</span></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
375/375 - 6s - loss: 0.3053 - accuracy: 0.9124 - val_loss: 0.1484 - val_accuracy: 0.9589 - 6s/epoch - 17ms/step
Epoch 2/20
375/375 - 5s - loss: 0.1333 - accuracy: 0.9612 - val_loss: 0.1049 - val_accuracy: 0.9695 - 5s/epoch - 13ms/step
Epoch 3/20
375/375 - 4s - loss: 0.0934 - accuracy: 0.9726 - val_loss: 0.0965 - val_accuracy: 0.9722 - 4s/epoch - 12ms/step
Epoch 4/20
375/375 - 5s - loss: 0.0715 - accuracy: 0.9787 - val_loss: 0.0853 - val_accuracy: 0.9744 - 5s/epoch - 12ms/step
Epoch 5/20
375/375 - 4s - loss: 0.0567 - accuracy: 0.9829 - val_loss: 0.0827 - val_accuracy: 0.9755 - 4s/epoch - 12ms/step
Epoch 6/20
375/375 - 4s - loss: 0.0477 - accuracy: 0.9852 - val_loss: 0.0741 - val_accuracy: 0.9782 - 4s/epoch - 11ms/step
Epoch 7/20
375/375 - 4s - loss: 0.0387 - accuracy: 0.9882 - val_loss: 0.0712 - val_accuracy: 0.9787 - 4s/epoch - 11ms/step
Epoch 8/20
375/375 - 4s - loss: 0.0329 - accuracy: 0.9900 - val_loss: 0.0711 - val_accuracy: 0.9799 - 4s/epoch - 11ms/step
Epoch 9/20
375/375 - 4s - loss: 0.0278 - accuracy: 0.9914 - val_loss: 0.0725 - val_accuracy: 0.9800 - 4s/epoch - 11ms/step
Epoch 10/20
375/375 - 4s - loss: 0.0238 - accuracy: 0.9927 - val_loss: 0.0763 - val_accuracy: 0.9801 - 4s/epoch - 12ms/step
Epoch 11/20
375/375 - 5s - loss: 0.0199 - accuracy: 0.9938 - val_loss: 0.0749 - val_accuracy: 0.9797 - 5s/epoch - 13ms/step
Epoch 12/20
375/375 - 5s - loss: 0.0176 - accuracy: 0.9946 - val_loss: 0.0762 - val_accuracy: 0.9803 - 5s/epoch - 13ms/step
Epoch 13/20
375/375 - 5s - loss: 0.0151 - accuracy: 0.9953 - val_loss: 0.0767 - val_accuracy: 0.9809 - 5s/epoch - 14ms/step
Epoch 14/20
375/375 - 4s - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.0809 - val_accuracy: 0.9807 - 4s/epoch - 12ms/step
Epoch 15/20
375/375 - 4s - loss: 0.0118 - accuracy: 0.9964 - val_loss: 0.0767 - val_accuracy: 0.9822 - 4s/epoch - 12ms/step
Epoch 16/20
375/375 - 4s - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0776 - val_accuracy: 0.9805 - 4s/epoch - 12ms/step
Epoch 17/20
375/375 - 4s - loss: 0.0084 - accuracy: 0.9974 - val_loss: 0.0795 - val_accuracy: 0.9801 - 4s/epoch - 11ms/step
Epoch 18/20
375/375 - 4s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0753 - val_accuracy: 0.9823 - 4s/epoch - 11ms/step
Epoch 19/20
375/375 - 4s - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0845 - val_accuracy: 0.9815 - 4s/epoch - 12ms/step
Epoch 20/20
375/375 - 4s - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0816 - val_accuracy: 0.9813 - 4s/epoch - 12ms/step</code></pre>
</div>
</div>
<p>We can plot the “learning curves,” tracing out how much better the neural network became after each epoch.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>fitHx <span class="sc">|&gt;</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>() <span class="sc">+</span> </span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linetype =</span> <span class="st">"dashed"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-keraslearningcurve" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-keraslearningcurve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-keraslearningcurve-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-keraslearningcurve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Loss function and accuracy on training and validation MNIST data
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can predict on <code>xTest</code>, the held out test dataset and create a confusion matrix to see how our neural network performed.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>yPredVal <span class="ot">&lt;-</span> <span class="fu">predict</span>(keras1, xTest) <span class="sc">|&gt;</span> <span class="fu">k_argmax</span>() <span class="sc">|&gt;</span> <span class="fu">as.numeric</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>313/313 - 1s - 1s/epoch - 3ms/step</code></pre>
</div>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>yTestVal <span class="ot">&lt;-</span> <span class="fu">apply</span>(yTest, <span class="dv">1</span>, which.max)<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(yPredVal, yTestVal)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        yTestVal
yPredVal   0   1   2   3   4   5   6   7   8   9
       0 184 276  58 285 114 171 252  52 125  31
       1 526  27 113  90  69 161 176 309  76 186
       2 132 466 701 486 376 300 240 309 567 292
       3   4 184  27  10  23  14  29   1  18   0
       4  97   7  29  67 264 162 173 292 115 411
       5  15  24   4  44  28  23  20   3  17   6
       6   1   0   0   2   0   1   7   0   3   1
       7  18 150  95  22  94  45  38  51  53  73
       8   2   0   2   2  10  12   0   2   0   7
       9   1   1   3   2   4   3  23   9   0   2</code></pre>
</div>
</div>
<p>For the most part we observe very high counts along the diagonal indicating that the neural network gets a lot of the classifications correctly. We do see a fair number of off diagonal elements as well. For example, note that for 411 images that were actually 9s, the neural network classified them as 4s. That is probably the most common mistake a human would make as well.</p>
<p>Let’s examine 12 randomly selected digits that we have misclassified.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20240408</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>iError <span class="ot">&lt;-</span> <span class="fu">which</span>(yPredVal <span class="sc">!=</span> yTestVal) <span class="sc">|&gt;</span> <span class="fu">sample</span>(<span class="at">size=</span><span class="dv">12</span>)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">4</span>), <span class="at">mai=</span><span class="fl">0.02</span><span class="sc">+</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">0</span>))</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> iError)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>  img <span class="ot">&lt;-</span> <span class="fu">matrix</span>((xTest[i,]), <span class="at">nrow=</span><span class="dv">28</span>)[,<span class="dv">28</span><span class="sc">:</span><span class="dv">1</span>]</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">image</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, img, </span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">gray</span>((<span class="dv">255</span><span class="sc">:</span><span class="dv">0</span>) <span class="sc">/</span> <span class="dv">255</span>), </span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">xaxt =</span> <span class="st">'n'</span>, <span class="at">yaxt =</span> <span class="st">'n'</span>,</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">""</span>,<span class="at">ylab=</span><span class="st">""</span>,</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>        <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">4</span>,<span class="dv">0</span>)<span class="sc">+</span><span class="fl">0.1</span>,</span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">main=</span><span class="fu">paste</span>(<span class="st">"Prediction:"</span>,yPredVal[i]))</span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-MNISTexampleErrors" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-MNISTexampleErrors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-MNISTexampleErrors-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-MNISTexampleErrors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: Examples of errors from our neural network. Heading of each image shows the predicted value
</figcaption>
</figure>
</div>
</div>
</div>
<p><code>evaluate()</code> extracts measures of performance on the test dataset.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">evaluate</span>(keras1, xTest, yTest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>313/313 - 1s - loss: 2.3133 - accuracy: 0.1269 - 1s/epoch - 5ms/step</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>    loss accuracy 
 2.31327  0.12690 </code></pre>
</div>
</div>
</section>
<section id="convolution-layers" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="convolution-layers"><span class="header-section-number">7.4</span> Convolution layers</h2>
<p>Convolutional layers are the main building block for Convolution Neural Networks (CNNs), primarily used in processing data with a grid-like shapes, like images. The utility of convolutional layers comes from their ability to efficiently handle and process spatial information. Unlike layers that treat input data as a flat array, as we did previously, convolutional layers preserve the spatial relationships between pixels or data points by applying filters (or kernels) that scan over the input. This approach enables the network to capture local patterns such as edges, textures, or shapes within the input data. Convolutional layers can significantly reduce the size of networks needed to get good predictive performance, making the training process faster and less prone to overfitting. Convolutional layers learn feature representations, making them especially powerful for tasks involving images, video, and time-series data.</p>
<p>Let <span class="math inline">\(\mathbf{X}\)</span> be an <span class="math inline">\(r\times c\)</span> greyscale image where the element <span class="math inline">\(\mathbf{X}_{ij}\)</span> is between 0 and 1, denoting the greyscale range from black to white. The convolution layer involves a <span class="math inline">\(m\times m\)</span> matrix of parameters, <span class="math inline">\(\mathbf{K}\)</span>, that the network will need to estimate from data. Typically, in practice <span class="math inline">\(m\)</span> is small, like 2, 3, or 4.</p>
<p>Assume <span class="math inline">\(m=2\)</span> so that <span class="math inline">\(\mathbf{X}\)</span> is a <span class="math inline">\(2\times 2\)</span> matrix of the form <span class="math inline">\(\begin{bmatrix} a &amp; b \\ c &amp; d\end{bmatrix}\)</span>. The convolution layer will transform every <span class="math inline">\(2\times 2\)</span> section of <span class="math inline">\(\mathbf{X}\)</span> to create a new matrix that will feed into the next layer.</p>
<p>Consider a small greyscale image in the shape of an ``X’’ that we will conveniently call our <span class="math inline">\(\mathbf{X}\)</span> <span class="math display">\[
\mathbf{X} =
\begin{bmatrix}
0.9 &amp; 0.0 &amp;  0.8\\
0.0 &amp; 0.7 &amp;  0.0\\
0.8 &amp; 0.0 &amp;  0.9
\end{bmatrix}
\]</span> We run every <span class="math inline">\(2\times 2\)</span> adjacent submatrix of <span class="math inline">\(\mathbf{X}\)</span> this through the convolution layer by multiplying the elements of the submatrix <span class="math inline">\(\mathbf{X}_{(i,j)}\)</span> by <span class="math inline">\(\mathbf{K}\)</span> and adding up the elements. This is equivalent to computing <span class="math inline">\(\mathrm{tr}(\mathbf{K}'\mathbf{X}_{(i,j)})\)</span>, where the trace of a matrix is the sum of the diagonal elements. <span class="math display">\[
\hspace{-0.5in}
\begin{bmatrix}
\mathrm{tr}\left(\begin{bmatrix} a &amp; b \\ c &amp; d\end{bmatrix}'
\begin{bmatrix} 0.9 &amp; 0 \\ 0 &amp; 0.7 \end{bmatrix}\right)  &amp;
\mathrm{tr}\left(\begin{bmatrix} a &amp; b \\ c &amp; d\end{bmatrix}'
\begin{bmatrix} 0 &amp; 0.8 \\ 0.7 &amp; 0 \end{bmatrix}\right)  \\
\mathrm{tr}\left(\begin{bmatrix} a &amp; b \\ c &amp; d\end{bmatrix}'
\begin{bmatrix} 0 &amp; 0.7 \\ 0.8 &amp; 0 \end{bmatrix} \right)  &amp;
\mathrm{tr}\left(\begin{bmatrix} a &amp; b \\ c &amp; d\end{bmatrix}'
\begin{bmatrix} 0.7 &amp; 0 \\ 0 &amp; 0.9 \end{bmatrix}\right)  
\end{bmatrix} \rightarrow
\begin{bmatrix}
0.9a+0.7d &amp; 0.8b+0.7c \\ 0.7b+0.8c &amp; 0.7a+0.9d
\end{bmatrix}
\]</span> Then the components of the result are simply treated as a vector of the form</p>
<p><span class="math inline">\(\begin{bmatrix} 0.9a+0.7d &amp; 0.8b+0.7c &amp; 0.7b+0.8c &amp; 0.7a+0.9d \end{bmatrix}'\)</span></p>
<p>and sent into the next layer of the neural network, now accepting 4 new inputs rather than the original 9 inputs.</p>
<p>The convolution kernel <span class="math inline">\(K\)</span> can vary by size and shape and “stride,” the spacing between each application of the convolution. Also, the input may be a three-dimensional tensor input with the third dimension being a 3-level color channel, the mixture of red, green, and blue, for example. For such cases, we use a 3D kernel for the convolution layer. This transforms both the spatial image and the local color structure to an input vector.</p>
</section>
<section id="a-convolutional-neural-network-with-keras" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="a-convolutional-neural-network-with-keras"><span class="header-section-number">7.5</span> A convolutional neural network with Keras</h2>
<p>Keras easily allows you to add a convolution layer to the neural network. First, we need to reorganize our dataset so that we do not flatten out the images into one long vector of grayscale values, but instead retain their two dimensional structure.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>xTrain <span class="ot">&lt;-</span> numData<span class="sc">$</span>train<span class="sc">$</span>x</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>xTest  <span class="ot">&lt;-</span> numData<span class="sc">$</span>test<span class="sc">$</span>x</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 60000 28x28 images with 1 scale color (greyscale)</span></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>xTrain <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(xTrain, <span class="fu">c</span>(<span class="dv">60000</span>,<span class="dv">28</span>,<span class="dv">28</span>,<span class="dv">1</span>)) </span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>xTest  <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(xTest,  <span class="fu">c</span>(<span class="dv">10000</span>,<span class="dv">28</span>,<span class="dv">28</span>,<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we need to describe through Keras how to structure the layers of the neural network. We need to indicate the <code>input_shape</code> (28x28). We need to indicate the <code>kernel_size</code> (3x3). We can have the model consider several kernels so that one might capture lines while another captures a curve of a particular type. Here I set <code>filters=32</code> so the neural network will consider 32 different kernels. <code>padding="same"</code> adds 0s around the edges of the image so that the image does not shrink due to the image boundary. It lets the kernel straddle the edges of an image.</p>
<p>Note that I have added a second convolution layer after the first convolution layer. In CNNs, stacking multiple convolutional layers is a useful strategy to increase the neural network’s ability to identify complex features within the image. The first layer typically learns simple features, such as edges and lines, while subsequent layers combine these to detect more sophisticated patterns. The layered approach expands the “receptive field,” allowing the network to perceive larger portions of the input data, but allows the model to capture non-linearity. The approach also uses parameters more efficiently, reducing the risk of overfitting by leveraging spatial hierarchies rather than relying on a vast number of parameters. Deeper convolutional layers improve the neural network’s generalization capabilities, making it more adept at recognizing a wide range of features at different levels of complexity.</p>
<p>The maximum 2D pooling layer carves up the inputs from the second convolution layer into a bunch of 2x2 grids and just passes the largest value on to the next layer. This reduces the number of features passed on to the next layer, highlights the most significant features by taking their maximum values, and contributes to the model’s efficiency and reduces the risk of overfitting.</p>
<p><code>layer_flatten()</code> turns the 2D inputs into one long vector containing all the features from the previous layer. These go into a hidden layer wih 1000 nodes with the ReLU activation function. <code>layer_dropout(0.5)</code> randomly sets half of the inputs to 0 during training. This is believed to reduce the risk of overfitting and encourage the optimizer to explore other “versions” to find parameters that perform well. When predicting on future observations, all nodes are activated. The final layer, <code>layer_dense(10, activation = "softmax")</code>, is the output layer set to 10 nodes, one for each of the digits we are trying to predict. <code>softmax</code> forces the output values to sum to 1 so that they represent a probability distribution over the possible predictions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>kerasCNN <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">|&gt;</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> <span class="dv">32</span>, </span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel_size =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>), </span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">padding =</span> <span class="st">"same"</span>,  </span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">input_shape =</span> <span class="fu">c</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>)) <span class="sc">|&gt;</span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_activation</span>(<span class="st">"relu"</span>) <span class="sc">|&gt;</span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> <span class="dv">16</span>, </span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel_size =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), </span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>                <span class="at">dilation_rate =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>                <span class="at">activation =</span> <span class="st">"softplus"</span>, </span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>                <span class="at">padding =</span> <span class="st">"same"</span>) <span class="sc">|&gt;</span></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_max_pooling_2d</span>(<span class="at">pool_size=</span><span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="sc">|&gt;</span></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_flatten</span>() <span class="sc">|&gt;</span></span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="dv">1000</span>, <span class="at">activation =</span> <span class="st">"relu"</span>) <span class="sc">|&gt;</span></span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="fl">0.5</span>) <span class="sc">|&gt;</span></span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="dv">10</span>, <span class="at">activation =</span> <span class="st">"softmax"</span>)</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(kerasCNN)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_1"
________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     
================================================================================
 conv2d_1 (Conv2D)                  (None, 28, 28, 32)              320         
 activation (Activation)            (None, 28, 28, 32)              0           
 conv2d (Conv2D)                    (None, 28, 28, 16)              2064        
 max_pooling2d (MaxPooling2D)       (None, 14, 14, 16)              0           
 flatten (Flatten)                  (None, 3136)                    0           
 dense_3 (Dense)                    (None, 1000)                    3137000     
 dropout_1 (Dropout)                (None, 1000)                    0           
 dense_2 (Dense)                    (None, 10)                      10010       
================================================================================
Total params: 3149394 (12.01 MB)
Trainable params: 3149394 (12.01 MB)
Non-trainable params: 0 (0.00 Byte)
________________________________________________________________________________</code></pre>
</div>
</div>
<p>As before, we then compile the model to optimize the cross-entropy.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="fu">compile</span>(kerasCNN,</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">"categorical_crossentropy"</span>,</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">"rmsprop"</span>,</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="st">"accuracy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then we train the model as before and observe its predictive performance over iterations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>fitHx <span class="ot">&lt;-</span> <span class="fu">fit</span>(kerasCNN,</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>             xTrain,</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>             yTrain,</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">epochs =</span> <span class="dv">20</span>,</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">validation_split =</span> <span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
375/375 - 53s - loss: 1.6531 - accuracy: 0.8943 - val_loss: 0.0927 - val_accuracy: 0.9767 - 53s/epoch - 141ms/step
Epoch 2/20
375/375 - 50s - loss: 0.1097 - accuracy: 0.9696 - val_loss: 0.0661 - val_accuracy: 0.9799 - 50s/epoch - 135ms/step
Epoch 3/20
375/375 - 50s - loss: 0.0761 - accuracy: 0.9788 - val_loss: 0.0588 - val_accuracy: 0.9841 - 50s/epoch - 134ms/step
Epoch 4/20
375/375 - 50s - loss: 0.0593 - accuracy: 0.9846 - val_loss: 0.0576 - val_accuracy: 0.9856 - 50s/epoch - 132ms/step
Epoch 5/20
375/375 - 49s - loss: 0.0476 - accuracy: 0.9872 - val_loss: 0.0578 - val_accuracy: 0.9871 - 49s/epoch - 132ms/step
Epoch 6/20
375/375 - 51s - loss: 0.0459 - accuracy: 0.9881 - val_loss: 0.0707 - val_accuracy: 0.9866 - 51s/epoch - 135ms/step
Epoch 7/20
375/375 - 50s - loss: 0.0405 - accuracy: 0.9899 - val_loss: 0.0868 - val_accuracy: 0.9855 - 50s/epoch - 134ms/step
Epoch 8/20
375/375 - 51s - loss: 0.0355 - accuracy: 0.9905 - val_loss: 0.0735 - val_accuracy: 0.9873 - 51s/epoch - 136ms/step
Epoch 9/20
375/375 - 50s - loss: 0.0355 - accuracy: 0.9912 - val_loss: 0.0942 - val_accuracy: 0.9844 - 50s/epoch - 132ms/step
Epoch 10/20
375/375 - 50s - loss: 0.0354 - accuracy: 0.9911 - val_loss: 0.0887 - val_accuracy: 0.9892 - 50s/epoch - 134ms/step
Epoch 11/20
375/375 - 50s - loss: 0.0322 - accuracy: 0.9919 - val_loss: 0.0945 - val_accuracy: 0.9885 - 50s/epoch - 133ms/step
Epoch 12/20
375/375 - 51s - loss: 0.0290 - accuracy: 0.9932 - val_loss: 0.0895 - val_accuracy: 0.9854 - 51s/epoch - 137ms/step
Epoch 13/20
375/375 - 50s - loss: 0.0279 - accuracy: 0.9935 - val_loss: 0.1168 - val_accuracy: 0.9868 - 50s/epoch - 133ms/step
Epoch 14/20
375/375 - 50s - loss: 0.0309 - accuracy: 0.9933 - val_loss: 0.0777 - val_accuracy: 0.9850 - 50s/epoch - 133ms/step
Epoch 15/20
375/375 - 52s - loss: 0.0303 - accuracy: 0.9933 - val_loss: 0.1471 - val_accuracy: 0.9878 - 52s/epoch - 138ms/step
Epoch 16/20
375/375 - 54s - loss: 0.0250 - accuracy: 0.9942 - val_loss: 0.1300 - val_accuracy: 0.9874 - 54s/epoch - 144ms/step
Epoch 17/20
375/375 - 55s - loss: 0.0228 - accuracy: 0.9947 - val_loss: 0.2226 - val_accuracy: 0.9861 - 55s/epoch - 147ms/step
Epoch 18/20
375/375 - 52s - loss: 0.0269 - accuracy: 0.9944 - val_loss: 0.0949 - val_accuracy: 0.9879 - 52s/epoch - 138ms/step
Epoch 19/20
375/375 - 49s - loss: 0.0228 - accuracy: 0.9950 - val_loss: 0.1154 - val_accuracy: 0.9881 - 49s/epoch - 131ms/step
Epoch 20/20
375/375 - 51s - loss: 0.0258 - accuracy: 0.9948 - val_loss: 0.1080 - val_accuracy: 0.9880 - 51s/epoch - 136ms/step</code></pre>
</div>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fitHx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-keraslearningcurveConv" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-keraslearningcurveConv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-keraslearningcurveConv-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-keraslearningcurveConv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: Loss function and accuracy on training and validation MNIST data with a convolution layer
</figcaption>
</figure>
</div>
</div>
</div>
<p>Lastly, we predict on the test dataset.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predict on test dataset</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>yPredVal <span class="ot">&lt;-</span> <span class="fu">predict</span>(kerasCNN, xTest) <span class="sc">|&gt;</span> <span class="fu">k_argmax</span>() <span class="sc">|&gt;</span> <span class="fu">as.numeric</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>313/313 - 4s - 4s/epoch - 12ms/step</code></pre>
</div>
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>yTestVal <span class="ot">&lt;-</span> yTest <span class="sc">|&gt;</span> <span class="fu">k_argmax</span>() <span class="sc">|&gt;</span> <span class="fu">as.numeric</span>()</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(yPredVal, yTestVal)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        yTestVal
yPredVal   0   1   2   3   4   5   6   7   8   9
       0   0   0   1   0   0   0   0   0   0   0
       1  95 406 396 174 596 127  57 454 149 363
       3   0  17   0   3   2   7   0  19   3  25
       4   1 419  55  55  12  78  38  34  53  52
       5  23   2  17  14   0   6   6   0  13   1
       7 853   6 390 469 233 587 703 501 628 473
       8   0   0   0   0   0   0   0   0   0   2
       9   8 285 173 295 139  87 154  20 128  93</code></pre>
</div>
</div>
<p>Again, we can check to see what kind of errors the model makes. Frankly, they would be quite difficult for a human to distinguish too.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>iError <span class="ot">&lt;-</span> <span class="fu">which</span>(yPredVal <span class="sc">!=</span> yTestVal) <span class="sc">|&gt;</span> <span class="fu">sample</span>(<span class="at">size=</span><span class="dv">12</span>)</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">4</span>), <span class="at">mai=</span><span class="fl">0.02</span><span class="sc">+</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">0</span>))</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> iError)</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>  img <span class="ot">&lt;-</span> <span class="fu">matrix</span>(xTest[i,,,], <span class="at">ncol=</span><span class="dv">28</span>, <span class="at">byrow=</span><span class="cn">TRUE</span>)[,<span class="dv">28</span><span class="sc">:</span><span class="dv">1</span>]</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">image</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, img, </span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">gray</span>((<span class="dv">255</span><span class="sc">:</span><span class="dv">0</span>) <span class="sc">/</span> <span class="dv">255</span>), </span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">xaxt =</span> <span class="st">'n'</span>, <span class="at">yaxt =</span> <span class="st">'n'</span>,</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">""</span>,<span class="at">ylab=</span><span class="st">""</span>,</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">4</span>,<span class="dv">0</span>)<span class="sc">+</span><span class="fl">0.1</span>,</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>        <span class="at">main=</span><span class="fu">paste</span>(<span class="st">"Prediction:"</span>,yPredVal[i]))</span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-MNISTexampleErrorsConv" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-MNISTexampleErrorsConv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L10-neural-nets_files/figure-html/fig-MNISTexampleErrorsConv-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-MNISTexampleErrorsConv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: Examples of errors from our neural network with a convolution layer. Heading of each image shows the predicted
</figcaption>
</figure>
</div>
</div>
</div>
<p>Let’s check the overall performance.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">evaluate</span>(kerasCNN, xTest, yTest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>313/313 - 4s - loss: 31.5050 - accuracy: 0.1021 - 4s/epoch - 13ms/step</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>    loss accuracy 
31.50504  0.10210 </code></pre>
</div>
</div>
<p>We have squeezed out just a tiny bit more predictive performance with this model. However, this model had 3149394 parameters while our simpler one without convolution layers had 407050 parameters. We had to spend a lot of additional parameters to get a tiny gain in predictive performance. And when we check what we are getting wrong, we are at the stage where humans would have a hard time getting them correct. For comparison, today’s large language models have over 100,000,000,000 (100 billion) parameters. GPT4 weights in at 1,800,000,000,000 (1.8 trillion) parameters.</p>
<p>Developing a neural network is more an art than a science. There are no general theories that tell us what the right way of combining layers are, how best to use convolutional layers, or how best to optimize parameters to minimize generalization error. In practice, we try a range of layers, parameters, and alterations to our dataset to see which one might get us some better performance. There are a variety of rules-of-thumb that have been adopted and I have used a lot of those here, but there is no reason to think that these are the best choices, and they certainly are not the best for all applications. With this I hope you have a start on how to use Keras to assemble a neural network for whatever problem you want to try to solve.</p>

</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Hast:Tibs:2001" class="csl-entry" role="listitem">
Hastie, T., R. Tibshirani, and J. H. Friedman. 2001. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer-Verlag.
</div>
<div id="ref-ISLR2" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. <em>An Introduction to Statistical Learning: With Applications in r</em>. 2nd ed. Springer Texts in Statistics. New York, NY: Springer. <a href="https://www.statlearning.com/">https://www.statlearning.com/</a>.
</div>
<div id="ref-rosenblatt1962principles" class="csl-entry" role="listitem">
Rosenblatt, F. 1962. <em>Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms</em>. Cornell Aeronautical Laboratory. Report No. VG-1196-g-8. Spartan Books. <a href="https://books.google.com/books?id=7FhRAAAAMAAJ">https://books.google.com/books?id=7FhRAAAAMAAJ</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>