<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Greg Ridgeway">
<meta name="dcterms.date" content="2025-01-13">

<title>L8 Boosting and L_1 regularization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="L8-boosting_files/libs/clipboard/clipboard.min.js"></script>
<script src="L8-boosting_files/libs/quarto-html/quarto.js"></script>
<script src="L8-boosting_files/libs/quarto-html/popper.min.js"></script>
<script src="L8-boosting_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="L8-boosting_files/libs/quarto-html/anchor.min.js"></script>
<link href="L8-boosting_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L8-boosting_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="L8-boosting_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="L8-boosting_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L8-boosting_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L8-boosting_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="L8-boosting_files/libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sparse-machine-learning-methods" id="toc-sparse-machine-learning-methods" class="nav-link active" data-scroll-target="#sparse-machine-learning-methods"><span class="header-section-number">1</span> Sparse machine learning methods</a>
  <ul class="collapse">
  <li><a href="#l_1-penalty-and-the-lasso" id="toc-l_1-penalty-and-the-lasso" class="nav-link" data-scroll-target="#l_1-penalty-and-the-lasso"><span class="header-section-number">1.1</span> <span class="math inline">\(L_1\)</span> penalty and the LASSO</a></li>
  <li><a href="#forward-stagewise-selection" id="toc-forward-stagewise-selection" class="nav-link" data-scroll-target="#forward-stagewise-selection"><span class="header-section-number">1.2</span> Forward stagewise selection</a></li>
  </ul></li>
  <li><a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting"><span class="header-section-number">2</span> Boosting</a>
  <ul class="collapse">
  <li><a href="#l_1-regularization-and-decision-trees" id="toc-l_1-regularization-and-decision-trees" class="nav-link" data-scroll-target="#l_1-regularization-and-decision-trees"><span class="header-section-number">2.1</span> <span class="math inline">\(L_1\)</span> regularization and decision trees</a></li>
  <li><a href="#gradient-boosted-models" id="toc-gradient-boosted-models" class="nav-link" data-scroll-target="#gradient-boosted-models"><span class="header-section-number">2.2</span> Gradient boosted models</a></li>
  <li><a href="#gradient-boosting-algorithm-for-squared-error" id="toc-gradient-boosting-algorithm-for-squared-error" class="nav-link" data-scroll-target="#gradient-boosting-algorithm-for-squared-error"><span class="header-section-number">2.3</span> Gradient boosting algorithm for squared error</a></li>
  <li><a href="#gradient-boosting-algorithm-for-negative-bernoulli-log-likelihood-logitboost" id="toc-gradient-boosting-algorithm-for-negative-bernoulli-log-likelihood-logitboost" class="nav-link" data-scroll-target="#gradient-boosting-algorithm-for-negative-bernoulli-log-likelihood-logitboost"><span class="header-section-number">2.4</span> Gradient boosting algorithm for negative Bernoulli log-likelihood (LogitBoost)</a></li>
  <li><a href="#adaboost-algorithm" id="toc-adaboost-algorithm" class="nav-link" data-scroll-target="#adaboost-algorithm"><span class="header-section-number">2.5</span> AdaBoost algorithm</a></li>
  <li><a href="#gradient-boosting-as-an-additive-model" id="toc-gradient-boosting-as-an-additive-model" class="nav-link" data-scroll-target="#gradient-boosting-as-an-additive-model"><span class="header-section-number">2.6</span> Gradient boosting as an additive model</a></li>
  </ul></li>
  <li><a href="#using-the-generalized-boosted-models-gbm3-package" id="toc-using-the-generalized-boosted-models-gbm3-package" class="nav-link" data-scroll-target="#using-the-generalized-boosted-models-gbm3-package"><span class="header-section-number">3</span> Using the Generalized Boosted Models (gbm3) package</a>
  <ul class="collapse">
  <li><a href="#relative-influence" id="toc-relative-influence" class="nav-link" data-scroll-target="#relative-influence"><span class="header-section-number">3.1</span> Relative influence</a></li>
  <li><a href="#partial-dependence-plots" id="toc-partial-dependence-plots" class="nav-link" data-scroll-target="#partial-dependence-plots"><span class="header-section-number">3.2</span> Partial dependence plots</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">4</span> Summary</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="L8-boosting.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">L8 Boosting and <span class="math inline">\(L_1\)</span> regularization</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Greg Ridgeway <a href="mailto:gridge@upenn.edu" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            University of Pennsylvania
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 13, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<!-- In terminal -->
<!-- quarto render L8-boosting.qmd -->
<!-- quarto render L8-boosting.qmd --cache-refresh  -->
<p>Refer to <span class="citation" data-cites="Hast:Tibs:2001">Hastie, Tibshirani, and Friedman (<a href="#ref-Hast:Tibs:2001" role="doc-biblioref">2001</a>)</span> Chapters 3.4, 3.8.1-2, 10, 16.2</p>
<section id="sparse-machine-learning-methods" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Sparse machine learning methods</h1>
<p>One of the major machine learning discoveries of the last 20 years is the development of “sparse” learning methods. These are methods for fitting models in cases where there are a large number of features, perhaps few of them with genuine predictive information about the outcome. This is particularly true of genome-wide association studies (GWAS) in which hundreds of thousands of candidate genes might predict some kind of genetic trait. Large numbers of features also show up in social science studies since many data collection efforts (NELS, NCVS, NSDUH, etc.) include over 1000 measurements on respondents.</p>
<p>The mathematics of sparse learning is tied to an absolute value penalty on coefficients, also known as an <span class="math inline">\(L_1\)</span> penalty or the LASSO (Least Absolute Shrinkage and Selection Operator).</p>
<section id="l_1-penalty-and-the-lasso" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="l_1-penalty-and-the-lasso"><span class="header-section-number">1.1</span> <span class="math inline">\(L_1\)</span> penalty and the LASSO</h2>
<p>Consider our now familiar linear model <span class="math inline">\(f(\mathbf{x}) = \beta'\mathbf{x}\)</span> where we want to minimize squared error. <span class="math display">\[
J(\beta) = \sum_{i=1}^n (y_i-\beta'\mathbf{x}_i)^2
\]</span> Previous we had explored ridge regression that put a squared penalty on the size of the coefficients. <span class="math display">\[
J(\beta) = \sum_{i=1}^n (y_i-\beta'\mathbf{x}_i)^2 + \lambda\sum_{j=1}^d \beta_j^2
\]</span> The LASSO replaces the squared penalty, often called an <span class="math inline">\(L_2\)</span> penalty, with an absolute penalty, often called an <span class="math inline">\(L_1\)</span> penalty <span class="citation" data-cites="LASSO">(<a href="#ref-LASSO" role="doc-biblioref">Tibshirani 1995</a>)</span>. <span class="math display">\[
J(\beta) = \sum_{i=1}^n (y_i-\beta'\mathbf{x}_i)^2 + \lambda\sum_{j=1}^d |\beta_j|
\]</span> In case you were wondering, there is also an <span class="math inline">\(L_0\)</span> penalty that is <span class="math inline">\(\lambda\sum I(\beta_j\neq 0)\)</span>, which simply counts how many coefficients are non-zero. This switch to the absolute penalty, at first, seems arbitrary. It is just another way of measuring the size of the coefficients and penalizing their size just prevents them from getting too large. Setting <span class="math inline">\(\lambda=0\)</span> would give the usual OLS solution while setting <span class="math inline">\(\lambda=\infty\)</span> would set <span class="math inline">\(\beta_0=\bar y\)</span> and set all the other coefficients to 0. This is the same property as we saw with ridge regression. Where the <span class="math inline">\(L_1\)</span> penalty differs from ridge regression is the <em>path</em> the coefficients take between these two end points.</p>
<p>Let’s start with a simulated example. The following code simulates 1,000 observations, each with four independent features generated from a Normal(0,1) distribution. Then the simulation generates the outcome as <span class="math display">\[
\begin{split}
y_i &amp;= -1 + x_{i3} + 2x_{i4} + \epsilon_i \\
\epsilon &amp;\sim N(0, 1.59)
\end{split}
\]</span> <span class="math inline">\(\epsilon\)</span> is random noise added to give a signal-to-noise ratio of 2.0.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20240312</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">rnorm</span>(n), <span class="fu">rnorm</span>(n), <span class="fu">rnorm</span>(n), <span class="fu">rnorm</span>(n))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>betaTrue <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> betaTrue</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>SNR <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> y <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fu">var</span>(y)<span class="sc">/</span>SNR))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The OLS estimates should be roughly <span class="math inline">\(\begin{bmatrix}-1 &amp; 0 &amp; 0 &amp; 1 &amp; 2\end{bmatrix}\)</span>, which they are.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>lm1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~-</span><span class="dv">1</span><span class="sc">+</span>X)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lm1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         X1          X2          X3          X4          X5 
-1.06349997 -0.07795981 -0.05277417  1.05005984  2.00038173 </code></pre>
</div>
</div>
<p>The function <code>L1mse()</code> here computes the mean squared error plus the <span class="math inline">\(L_1\)</span> penalty. Note that the <span class="math inline">\(L_1\)</span> penalty does not include the intercept term, <span class="math inline">\(\beta_0\)</span>. This function also computes the gradient vector and Hessian matrix so that we can optimize using Newton-Raphson.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>L1mse <span class="ot">&lt;-</span> <span class="cf">function</span>(beta, y, X, lambda)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  J <span class="ot">&lt;-</span> <span class="fu">mean</span>((y <span class="sc">-</span> X <span class="sc">%*%</span> beta)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> lambda<span class="sc">*</span><span class="fu">sum</span>(<span class="fu">abs</span>(beta[<span class="sc">-</span><span class="dv">1</span>]))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">attr</span>(J, <span class="st">"gradient"</span>) <span class="ot">&lt;-</span> (<span class="sc">-</span><span class="dv">2</span><span class="sc">/</span><span class="fu">nrow</span>(X))<span class="sc">*</span><span class="fu">t</span>(X)<span class="sc">%*%</span>(y<span class="sc">-</span>X<span class="sc">%*%</span>beta) <span class="sc">+</span> lambda<span class="sc">*</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">sign</span>(beta[<span class="sc">-</span><span class="dv">1</span>]))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">attr</span>(J, <span class="st">"hessian"</span>)  <span class="ot">&lt;-</span> (<span class="dv">2</span><span class="sc">/</span><span class="fu">nrow</span>(X))<span class="sc">*</span><span class="fu">t</span>(X)<span class="sc">%*%</span>X</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(J)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I will start with an initial guess for <span class="math inline">\(\beta\)</span> with the intercept equal to <span class="math inline">\(\bar y\)</span> and all of the other coefficients initialized to 0. Let’s test out the <code>L1mse()</code> function at this starting value for <span class="math inline">\(\beta\)</span> with no <span class="math inline">\(L_1\)</span> penalty, <span class="math inline">\(\lambda=0\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>betaHat <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(betaTrue))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>betaHat[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">L1mse</span>(betaHat, y, X, <span class="at">lambda=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7.733564
attr(,"gradient")
              [,1]
[1,] -1.412204e-16
[2,]  2.317929e-01
[3,]  4.938739e-02
[4,] -2.192318e+00
[5,] -3.997327e+00
attr(,"hessian")
            [,1]        [,2]         [,3]         [,4]          [,5]
[1,]  2.00000000 -0.15469711 0.0378722854  0.077122868  0.0181676407
[2,] -0.15469711  2.02775250 0.0274503061 -0.098435530  0.0106224389
[3,]  0.03787229  0.02745031 1.8954160639  0.051134547  0.0007496225
[4,]  0.07712287 -0.09843553 0.0511345470  2.094271294 -0.0034268939
[5,]  0.01816764  0.01062244 0.0007496225 -0.003426894  2.0010934097</code></pre>
</div>
</div>
<p>Remember that <code>nlm()</code> (non-linear minimization) is a general purpose optimization function. I give it the starting value for <span class="math inline">\(\beta\)</span>, and it will optimize <code>L1mse()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>nlm1 <span class="ot">&lt;-</span> <span class="fu">nlm</span>(L1mse,</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">p=</span>betaHat,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">y=</span>y,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">X=</span>X,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">lambda=</span><span class="dv">0</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>nlm1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$minimum
[1] 2.574103

$estimate
[1] -1.06349997 -0.07795981 -0.05277417  1.05005984  2.00038173

$gradient
[1]  2.282619e-16 -3.351208e-16 -1.241229e-16 -1.848743e-15 -3.725575e-15

$code
[1] 1

$iterations
[1] 1</code></pre>
</div>
</div>
<p>The results show that it converged in 1 iteration, as it should since with <span class="math inline">\(\lambda=0\)</span> the loss function is quadratic and Newton-Raphson will solve it exactly. Also note that the solution is identical to OLS.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>nlm1<span class="sc">$</span>estimate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -1.06349997 -0.07795981 -0.05277417  1.05005984  2.00038173</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lm1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         X1          X2          X3          X4          X5 
-1.06349997 -0.07795981 -0.05277417  1.05005984  2.00038173 </code></pre>
</div>
</div>
<p>What happens when we set <span class="math inline">\(\lambda=0.4\)</span>, penalizing the absolute size of the coefficients.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>nlm1 <span class="ot">&lt;-</span> <span class="fu">nlm</span>(L1mse,</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">p=</span>betaHat,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">y=</span>y,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">X=</span>X,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">lambda=</span><span class="fl">0.4</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">check.analyticals =</span> <span class="cn">FALSE</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>nlm1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$minimum
[1] 3.736787

$estimate
[1] -1.052841e+00 -5.809961e-07 -1.581168e-07  9.334511e-01  1.875751e+00

$gradient
[1] -2.557954e-16 -2.319637e-01 -3.034838e-01  1.520634e-01  1.520634e-01

$code
[1] 2

$iterations
[1] 40</code></pre>
</div>
</div>
<p>Note that <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> have been reduced nearly to 0.0000001, essentially 0, while the other coefficients have been shrunk slightly. This is our first glimpse at why the <span class="math inline">\(L_1\)</span> penalty is special. For certain values of <span class="math inline">\(\lambda\)</span> it will eliminate some (or many) features.</p>
<p>Let’s try a larger <span class="math inline">\(\lambda\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>nlm1 <span class="ot">&lt;-</span> <span class="fu">nlm</span>(L1mse,</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">p=</span>betaHat,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">y=</span>y,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">X=</span>X,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">lambda=</span><span class="dv">4</span>,</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">check.analyticals =</span> <span class="cn">FALSE</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>nlm1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$minimum
[1] 7.733564

$estimate
[1] -0.9998063  0.0000000  0.0000000  0.0000000  0.0000000

$gradient
[1] -1.412204e-16  2.317929e-01  4.938739e-02 -2.192318e+00 -3.997327e+00

$code
[1] 3

$iterations
[1] 1</code></pre>
</div>
</div>
<p>Here we see that when <span class="math inline">\(\lambda=4\)</span> it sets all of the coefficients (except the intercept) to 0. The next block of code explores the path each coefficient takes for <span class="math inline">\(\lambda\)</span> between 0 and 4.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>nIter <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">4</span>, <span class="at">length.out=</span>nIter)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>matBetaHat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span><span class="fu">length</span>(betaHat), <span class="at">ncol=</span>nIter)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>matBetaHat[,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">coef</span>(lm1)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(iIter <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>nIter)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  nlm1 <span class="ot">&lt;-</span> <span class="fu">nlm</span>(L1mse,</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>              <span class="at">p=</span>matBetaHat[,iIter<span class="dv">-1</span>],</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>              <span class="at">typsize =</span> matBetaHat[,iIter<span class="dv">-1</span>],</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span>y,</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>              <span class="at">X=</span>X,</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>              <span class="at">lambda=</span>lambda[iIter],</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>              <span class="at">check.analyticals =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>  matBetaHat[,iIter] <span class="ot">&lt;-</span> nlm1<span class="sc">$</span>estimate</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>L1norm <span class="ot">&lt;-</span> <span class="fu">apply</span>(matBetaHat[<span class="sc">-</span><span class="dv">1</span>,], <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="fu">sum</span>(<span class="fu">abs</span>(x)))</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(L1norm, <span class="fu">rep</span>(<span class="dv">0</span>,nIter), </span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">type=</span><span class="st">"n"</span>,</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.2</span>,<span class="dv">2</span>),</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="fu">expression</span>(L[<span class="dv">1</span>]),</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="fu">expression</span>(beta))</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="fu">nrow</span>(matBetaHat))</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(L1norm, matBetaHat[i,], <span class="at">col=</span>i<span class="dv">-1</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">rep</span>(<span class="fu">max</span>(L1norm), <span class="fu">length</span>(betaTrue)<span class="sc">-</span><span class="dv">1</span>), <span class="fu">coef</span>(lm1)[<span class="sc">-</span><span class="dv">1</span>], </span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>       <span class="at">col=</span><span class="dv">1</span><span class="sc">:</span>(<span class="fu">length</span>(betaTrue)<span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch=</span><span class="dv">15</span>)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">1</span>,<span class="fl">1.2</span>,<span class="fu">expression</span>(beta[<span class="dv">4</span>]))</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">2</span>,<span class="fl">0.7</span>,<span class="fu">expression</span>(beta[<span class="dv">3</span>]))</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">3.1</span>,<span class="fl">0.1</span>,<span class="fu">expression</span>(beta[<span class="dv">2</span>]))</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">3.0</span>,<span class="sc">-</span><span class="fl">0.2</span>,<span class="fu">expression</span>(beta[<span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-nlm1000CoefPath" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nlm1000CoefPath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-nlm1000CoefPath-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nlm1000CoefPath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Path of the coefficients for various values of <span class="math inline">\(\lambda\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice the following:</p>
<ol type="1">
<li>When <span class="math inline">\(\lambda=0\)</span> (right side of the plot), we get the OLS solution</li>
<li>When <span class="math inline">\(\lambda\)</span> is large (left side of the plot), all the coefficients are 0</li>
<li>In between, the coefficients take piecewise linear paths as <span class="math inline">\(\lambda\)</span> (and the <span class="math inline">\(L_1\)</span> penalty) changes</li>
<li>For <span class="math inline">\(0 &lt;L_1 &lt; 0.9\)</span>, only <span class="math inline">\(\beta_4\)</span> is non-zero</li>
<li>For <span class="math inline">\(0.9 &lt;L_1 &lt; 2.9\)</span>, only <span class="math inline">\(\beta_3\)</span> and <span class="math inline">\(\beta_4\)</span> are non-zero</li>
</ol>
<p>The linear model with <span class="math inline">\(L_1\)</span> regularization is built into the <code>glmnet</code> package. <code>glmnet()</code> actually allows the user to mix between ridge regression and the lasso using the <span class="math inline">\(\alpha\)</span> parameter, with <span class="math inline">\(\alpha=1\)</span> indicating all lasso, <span class="math inline">\(\alpha=0\)</span> all ridge regression, and in between a mixture of the two.</p>
<p>Here I ask <code>glmnet()</code> to try 1000 different values for <span class="math inline">\(\lambda\)</span> between 0 and 2 (<span class="math inline">\(\lambda\)</span> is scaled a little differently in <code>glmnet()</code>). Note that I’ve dropped the first column from <code>X</code> since <code>glmnet()</code> will add the intercept term.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>lasso1 <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X[,<span class="sc">-</span><span class="dv">1</span>], y, </span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">family=</span><span class="st">"gaussian"</span>,  <span class="co"># squared error</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">alpha=</span><span class="dv">1</span>,            <span class="co"># 1 - lasso, 0 - ridge regression</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">lambda=</span><span class="fu">seq</span>(<span class="fl">2.02</span>,<span class="dv">0</span>,<span class="at">length.out=</span><span class="dv">1000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s check that no penalty gives us the original OLS estimates.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lasso1, <span class="at">s=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>5 x 1 sparse Matrix of class "dgCMatrix"
                     s1
(Intercept) -1.06349997
V1          -0.07795987
V2          -0.05277413
V3           1.05005984
V4           2.00038173</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lm1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         X1          X2          X3          X4          X5 
-1.06349997 -0.07795981 -0.05277417  1.05005984  2.00038173 </code></pre>
</div>
</div>
<p><code>glmnet</code> has a plot function that will trace out the coefficient paths. Note that the x-axis is the value of <span class="math inline">\(\sum |\beta_j|\)</span> rather than <span class="math inline">\(\lambda\)</span>. The result still shows a piecewise linear coefficient path.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso1)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">rep</span>(<span class="fl">3.18</span>, <span class="fu">length</span>(betaTrue)<span class="sc">-</span><span class="dv">1</span>), <span class="fu">coef</span>(lm1)[<span class="sc">-</span><span class="dv">1</span>], </span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">col=</span><span class="dv">1</span><span class="sc">:</span>(<span class="fu">length</span>(betaTrue)<span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch=</span><span class="dv">15</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-glmnetCoefPath" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-glmnetCoefPath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-glmnetCoefPath-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-glmnetCoefPath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Coefficient path for LASSO penalty from <code>glmnet()</code>
</figcaption>
</figure>
</div>
</div>
</div>
<p>For comparison let’s look at the coefficient paths for ridge regression.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>ridge1 <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X[,<span class="sc">-</span><span class="dv">1</span>], y, </span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">family=</span><span class="st">"gaussian"</span>,  <span class="co"># squared error</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">alpha=</span><span class="dv">0</span>)            <span class="co"># 1 - lasso, 0 - ridge regression</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>                 <span class="co">#lambda=seq(2.02,0,length.out=1000))</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ridge1)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">rep</span>(<span class="fl">3.18</span>, <span class="fu">length</span>(betaTrue)<span class="sc">-</span><span class="dv">1</span>), <span class="fu">coef</span>(lm1)[<span class="sc">-</span><span class="dv">1</span>], </span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">col=</span><span class="dv">1</span><span class="sc">:</span>(<span class="fu">length</span>(betaTrue)<span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch=</span><span class="dv">15</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-glmnetRidge" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-glmnetRidge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-glmnetRidge-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-glmnetRidge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Coefficient path for ridge penalty from <code>glmnet()</code>
</figcaption>
</figure>
</div>
</div>
</div>
<p>The key distinction to pay attention to here is that or every value of <span class="math inline">\(\lambda\)</span> <em>all</em> of the coefficients are different from 0. Ridge regression offers no variable selection and requires the tracking and storage of coefficients for all features used in the prediction model.</p>
</section>
<section id="forward-stagewise-selection" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="forward-stagewise-selection"><span class="header-section-number">1.2</span> Forward stagewise selection</h2>
<p>Setting aside the <span class="math inline">\(L_1\)</span> penalty, let’s try what seems to be a completely different approach. We will initialize all the coefficients to be 0, except <span class="math inline">\(\beta_0=\bar y\)</span>. Then we will take a kind of gradient descent approach. We will consider changing the coefficient of one feature by a small amount, like 0.0001. We will choose which coefficient to change by picking the one that offers the greatest reduction in squared error.</p>
<p>Another way to think of this is that we have an initial model, <span class="math inline">\(f_0(\mathbf{x})=\bar y\)</span>. We would like to improve that prediction model as <span class="math display">\[
f_1(\mathbf{x}) = f_0(\mathbf{x}) + \lambda x_{j^*}
\]</span> where we pick <span class="math inline">\(j^*\)</span> to be the one feature that offers the greatest reduction in squared error. Then we will repeat this again and again like <span class="math display">\[
f_{k+1}(\mathbf{x}) = f_k(\mathbf{x}) + \lambda x_{j^*}
\]</span> always choosing the <span class="math inline">\(x_{j^*}\)</span> that offers the greatest reduction in squared error.</p>
<p>So perhaps after we iterate, say, 8 times our model will look like <span class="math display">\[
\begin{split}
f_8(\mathbf{x}) &amp;= \bar y + \lambda x_4+ \lambda x_4+ \lambda x_1
                      + \lambda x_3+ \lambda x_4+ \lambda x_2+ \lambda x_2
                      + \lambda x_4 \\
  &amp;= \bar y + \lambda x_1 + 2\lambda x_2 + \lambda x_3 + 4\lambda x_4
\end{split}
\]</span> Each iteration is incrementally adding a small amount to each feature’s coefficient. With each iteration those coefficients grow toward the OLS solution.</p>
<p>This is an example of <em>functional gradient descent</em>. With each iteration we are moving our prediction function, <span class="math inline">\(f(\mathbf{x})\)</span>, toward a better function, one that has a smaller squared error.</p>
<p>How do we decide which variable is the next one to add to the model? Consider what we are trying to minimize. <span class="math display">\[
J(\lambda) = \sum_{i=1}^n (y_i-(f_k(\mathbf{x}_i)+\lambda x_{ij}))^2
\]</span> The “directional derivative” of <span class="math inline">\(J\)</span> in the direction of <span class="math inline">\(x_j\)</span> is <span class="math inline">\(\left.\frac{d}{d\lambda} J(\lambda) \right|_{\lambda=0}\)</span>. If we were to nudge <span class="math inline">\(f_k(\mathbf{x})\)</span> in the direction of <span class="math inline">\(x_j\)</span>, the directional derivative tells us the rate at which squared error will change. We are looking for the steepest decline in squared error, a large negative value. The directional derivative is <span class="math display">\[
\begin{split}
\left.\frac{d}{d\lambda} J(\lambda) \right|_{\lambda=0} &amp;=
  \left.\frac{d}{d\lambda} \sum_{i=1}^n (y_i-(f_k(\mathbf{x}_i)+\lambda x_{ij}))^2 \right|_{\lambda=0}  \\
  &amp;=
   \left.\sum_{i=1}^n -2(y_i-(f_k(\mathbf{x}_i)+\lambda x_{ij}))x_{ij}\right|_{\lambda=0} \\
  &amp;= -2\sum_{i=1}^n (y_i-f_k(\mathbf{x}_i))x_{ij}
\end{split}
\]</span> All the information about which variable to pick is in <span class="math inline">\(-(\mathbf{y}-\mathbf{\hat y})'\mathbf{x}_j\)</span> (the 2 is not really important). We can get everything we need by computing <span class="math inline">\(-\mathbf{X'(\mathbf{y}- \mathbf{\hat y})}\)</span>. This expression is also involved in computing the correlation between the columns of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}- \mathbf{\hat y}\)</span>. So you can interpret this as finding the feature that has the largest correlation with the current prediction error (or residuals).</p>
<p>Let’s try this on our simulated data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># reset our betaHat</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>betaHat <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(betaTrue))</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>betaHat[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) </span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># get predicted values</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>fx <span class="ot">&lt;-</span> X <span class="sc">%*%</span> betaHat</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>dirDeriv <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">t</span>(X) <span class="sc">%*%</span> (y<span class="sc">-</span>fx)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>dirDeriv <span class="sc">|&gt;</span> <span class="fu">zapsmall</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]
[1,]     0.0000
[2,]   115.8965
[3,]    24.6937
[4,] -1096.1591
[5,] -1998.6637</code></pre>
</div>
</div>
<p>Note that the fifth value, which is the one associated with <span class="math inline">\(\beta_4\)</span> has the largest negative value. This means if we increment <span class="math inline">\(\hat\beta_4\)</span> by just a little bit we can expect a decrease in squared error that is larger than any decrease we would get by shifting any of the other coefficients.</p>
<p>If the largest directional derivative was a large positive number, it means that subtracting a little from that coefficient would get a large reduction in squared error. So, we need to pay attention to the largest absolute directional derivative and its sign to know which coefficient to adjust and in which direction to adjust it.</p>
<p>Time to run this for real. The next block of code will do 35,000 iterations, compute the directional derivative, find the feature with the largest directional derivative, and adjust the associated coefficient in that direction. Along the way we will generate a plot of the entire coefficient paths.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># reset</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>betaHat <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(betaTrue))</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>betaHat[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) </span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>nIter <span class="ot">&lt;-</span> <span class="dv">35000</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>matBetaHatFS <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span><span class="fu">length</span>(betaHat), <span class="at">ncol=</span>nIter)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>matBetaHatFS[,<span class="dv">1</span>] <span class="ot">&lt;-</span> betaHat</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(iIter <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>nIter)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>  fx <span class="ot">&lt;-</span> X <span class="sc">%*%</span> betaHat</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>  yError <span class="ot">&lt;-</span> y <span class="sc">-</span> fx</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>  dirDeriv <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">t</span>(X[,<span class="sc">-</span><span class="dv">1</span>]) <span class="sc">%*%</span> yError</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>  j <span class="ot">&lt;-</span> dirDeriv <span class="sc">|&gt;</span> <span class="fu">abs</span>() <span class="sc">|&gt;</span> <span class="fu">which.max</span>()</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># -lambda because if dirDeriv&lt;0 then we want to increase beta_j</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># j+1 because betaHat[1] corresponds to be beta_0</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>  betaHat[j<span class="sc">+</span><span class="dv">1</span>] <span class="ot">&lt;-</span> betaHat[j<span class="sc">+</span><span class="dv">1</span>] <span class="sc">-</span> lambda<span class="sc">*</span><span class="fu">sign</span>(dirDeriv[j])</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>  matBetaHatFS[,iIter] <span class="ot">&lt;-</span> betaHat</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span>nIter, <span class="fu">rep</span>(<span class="dv">0</span>,nIter), </span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>     <span class="at">type=</span><span class="st">"n"</span>,</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.2</span>,<span class="dv">2</span>),</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"Iteration"</span>,</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="fu">expression</span>(beta))</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="fu">nrow</span>(matBetaHatFS))</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span>nIter, matBetaHatFS[i,], <span class="at">col=</span>i<span class="dv">-1</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">rep</span>(nIter, <span class="fu">length</span>(betaHat)<span class="sc">-</span><span class="dv">1</span>), <span class="fu">coef</span>(lm1)[<span class="sc">-</span><span class="dv">1</span>], </span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>       <span class="at">col=</span><span class="dv">1</span><span class="sc">:</span>(<span class="fu">length</span>(betaHat)<span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch=</span><span class="dv">15</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-forwardStagewise35000" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-forwardStagewise35000-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-forwardStagewise35000-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-forwardStagewise35000-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Coefficient path from a forward stagewise model fit
</figcaption>
</figure>
</div>
</div>
</div>
<p>Are you not amazed?!?!? The paths of the coefficients traced out as we iterate look a lot like the same paths we got when we did the hard optimization of the <span class="math inline">\(L_1\)</span> penalty. Let’s plot the two on top of each other just for a check.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso1, <span class="at">lwd=</span><span class="dv">5</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>L1norm <span class="ot">&lt;-</span> <span class="fu">apply</span>(matBetaHatFS[<span class="sc">-</span><span class="dv">1</span>,], <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="fu">sum</span>(<span class="fu">abs</span>(x)))</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="fu">nrow</span>(matBetaHatFS))</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(L1norm, matBetaHatFS[i,], <span class="at">col=</span><span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-LASSOCoefPath" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-LASSOCoefPath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-LASSOCoefPath-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-LASSOCoefPath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Coefficient path for LASSO penalty
</figcaption>
</figure>
</div>
</div>
</div>
<p>They are identical! In a groundbreaking paper <span class="citation" data-cites="LeastAngleRegression">Efron et al. (<a href="#ref-LeastAngleRegression" role="doc-biblioref">2004</a>)</span> showed that the entire set of lasso coefficients could be found with this incremental, forward stagewise approach. They went on to show that the coefficient paths are exactly linear and that you do not even have to do this incremental approach. They figured out exactly how long each linear segment would be until the next coefficient became non-zero. They proposed an algorithm that had the same computational complexity as OLS, the least angle regression algorithm (LARS).</p>
<p>Since then, there has been a flurry of research on algorithms to make this approach faster and applicable to more scenarios. For social science, the value of this approach is that it provides a simple method for building predictive models when you have a large number of features, even when features are perfectly correlated.</p>
<p>Returning to the forensic window glass dataset, I am going to load the dataset and create a new feature <code>MgAl</code> that is the sum of <code>Mg</code> and <code>Al</code>. Since there is a linear relationship between this new feature and other variables, this would cause problems for a standard linear regression model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>dGlass <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data"</span>,</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>                  <span class="at">header=</span><span class="cn">FALSE</span>,</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>                  <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">"ID"</span>,<span class="st">"RI"</span>,<span class="st">"Na"</span>,<span class="st">"Mg"</span>,<span class="st">"Al"</span>,<span class="st">"Si"</span>,</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>                                <span class="st">"K"</span>,<span class="st">"Ca"</span>,<span class="st">"Ba"</span>,<span class="st">"Fe"</span>,<span class="st">"type"</span>)) <span class="sc">|&gt;</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="co"># make a new variable that is a linear combination of others</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">MgAl =</span> Mg<span class="sc">+</span>Al,</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">window =</span> <span class="fu">as.numeric</span>(type<span class="sc">==</span><span class="dv">1</span>))</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>RI<span class="sc">+</span>Na<span class="sc">+</span>Mg<span class="sc">+</span>Al<span class="sc">+</span>Si<span class="sc">+</span>K<span class="sc">+</span>Ca<span class="sc">+</span>Ba<span class="sc">+</span>Fe<span class="sc">+</span>MgAl, <span class="at">data=</span>dGlass)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> X[,<span class="sc">-</span><span class="dv">1</span>] <span class="co"># drop the intercept column</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that <span class="math inline">\(\mathbf{X}'\mathbf{X}\)</span> will not be invertible because <code>MgAL</code> is a linear combination of <code>Mg</code> and <code>Al</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in solve.default(t(X) %*% X): system is computationally singular: reciprocal condition number = 4.86532e-19</code></pre>
</div>
</div>
<p>If we try to run <code>lm()</code>, it will still run but gives an <code>NA</code> for <code>MgAl</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(dGlass<span class="sc">$</span>window<span class="sc">~</span>X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = dGlass$window ~ X)

Coefficients:
(Intercept)          XRI          XNa          XMg          XAl          XSi  
  -115.3070      16.3621       0.8402       0.9914       0.6461       0.9305  
         XK          XCa          XBa          XFe        XMgAl  
     0.9305       0.8598       0.9173       0.1036           NA  </code></pre>
</div>
</div>
<p>Let’s give the lasso a try. We do need to select the optimal value for <span class="math inline">\(\lambda\)</span>. Naturally, we do this using 10-fold cross-validation, which conveniently the <code>glmnet</code> package has built in. Here I have set <code>family="binomial"</code> since we have a 0/1 outcome. This will minimize the negative Bernoulli log-likelihood instead of trying to minimize squared error.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20240312</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>lasso1 <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, </span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>                    dGlass<span class="sc">$</span>window, </span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">family=</span><span class="st">"binomial"</span>,  <span class="co"># logistic regression</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">alpha=</span><span class="dv">1</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(lasso1, lambda[<span class="fu">which.min</span>(cvm)]) <span class="sc">|&gt;</span> <span class="fu">log</span>() <span class="sc">|&gt;</span> <span class="fu">round</span>(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -5.1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-cvglmnetGlass" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cvglmnetGlass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-cvglmnetGlass-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cvglmnetGlass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Cross-validated performance of lasso by <span class="math inline">\(\lambda\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
<p>The plot shows a range of values for <span class="math inline">\(\log(\lambda)\)</span> showing that the optimal value for <span class="math inline">\(\lambda\)</span> is 0.006 or on the log scale -5.1. The numbers along the top of the figure count the number of non-zero coefficients for different choices of <span class="math inline">\(\lambda\)</span>.</p>
<p>Within <code>lasso1</code>, the object that <code>cv.glmnet()</code> produces, is a <code>glmnet</code> model fit to the full dataset. That is, in addition to doing cross-validation, <code>cv.glmnet()</code> also runs <code>glmnet()</code> against the entire dataset. That means there is no need to fit another <code>glmnet</code> model after running <code>cv.glmnet()</code>. Just like when we used <code>rpart()</code> to fit decision trees, we simply used <code>prune()</code> to get the tree that we wanted. No need to rerun the algorithm. All the functions like <code>coef()</code> and <code>predict()</code> can run directly from <code>lasso1</code>.</p>
<p>For example, <code>coef()</code> extracts the value of <span class="math inline">\(\hat\beta\)</span> for a specific value of <span class="math inline">\(\lambda\)</span>. With <code>s = "lambda.min"</code> we are asking <code>glmnet</code> to use the <span class="math inline">\(\lambda\)</span> that minimized the cross-validated negative Bernoulli log-likelihood.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lasso1, <span class="at">s =</span> <span class="st">"lambda.min"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>11 x 1 sparse Matrix of class "dgCMatrix"
                      s1
(Intercept) -120.7384861
RI            58.9239738
Na            -0.3893883
Mg             1.2747803
Al            -2.6194133
Si             0.4840770
K              .        
Ca             .        
Ba             .        
Fe            -1.3511375
MgAl           .        </code></pre>
</div>
</div>
<p>We do need to tell the <code>lasso1</code> object exactly which set of coefficients that we want, since it has all sets of coefficients for all choices of <span class="math inline">\(\lambda\)</span>. Simply setting <code>s="lambda.min"</code> will tell <code>coef()</code> to use the <span class="math inline">\(\lambda\)</span> that minimized the cross-validated loss function.</p>
<p>We can also predict using the <code>lasso1</code> object. Setting <code>s = "lambda.min"</code> insists that the predictions use the coefficients associated with the <span class="math inline">\(\lambda\)</span> that minimized the cross-validated Bernoulli log-likelihood. By default, when the <code>glmnet</code> model is fit using <code>family="binomial"</code>, the predicted values will be on the log odds scale, that is <span class="math inline">\(\log\frac{p}{1-p}\)</span>. Setting <code>type="response"</code> transforms the predictions to the probability scale (using the sigmoid function so you do not have to do it yourself).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>pHat <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso1, <span class="at">newx=</span>X, <span class="at">s =</span> <span class="st">"lambda.min"</span>, <span class="at">type=</span><span class="st">"response"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lastly, we can compute the average Bernoulli log-likelihood on the <code>dGlass</code> data if we wish.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">ifelse</span>(dGlass<span class="sc">$</span>window<span class="sc">==</span><span class="dv">1</span>, <span class="fu">log</span>(pHat), <span class="fu">log</span>(<span class="dv">1</span><span class="sc">-</span>pHat)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.4354227</code></pre>
</div>
</div>
<p>Technically, <span class="math inline">\(L_1\)</span>/lasso is not exactly identical to the incremental forward stagewise selection. When features are highly correlated, the two methods’ coefficient paths can diverge from one another. The forward stagewise approach tends to be more stable and more immune to overfitting. See <span class="citation" data-cites="Hast:Tibs:2001">Hastie, Tibshirani, and Friedman (<a href="#ref-Hast:Tibs:2001" role="doc-biblioref">2001</a>)</span> Chapter 16.2.3 for details.</p>
</section>
</section>
<section id="boosting" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Boosting</h1>
<section id="l_1-regularization-and-decision-trees" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="l_1-regularization-and-decision-trees"><span class="header-section-number">2.1</span> <span class="math inline">\(L_1\)</span> regularization and decision trees</h2>
<p>Linear models of the form <span class="math inline">\(\beta'\mathbf{x}\)</span> are very constrained. They do not allow for non-linear relationships between features and outcomes. They cannot adapt to important interaction effects, such as groups often of interest in social science, like young black male, low SES first generation college student, or middle-class elderly rural voter. In order to overcome this constraint, we have to intentionally add non-linear terms and interaction effects. For example, <span class="math display">\[
f(\mathbf{x}) = \beta_0 + \beta_1\mathrm{age} + \beta_2 \mathrm{age}^2 + \beta_3\mathrm{ses} + \beta_4\log(\mathrm{ses}) + \beta_5\mathrm{age}\times\mathrm{ses} + \beta_6\mathrm{age}\times\log(\mathrm{ses}) + \ldots
\]</span> How likely is it that we will pick all the right transformations and all the right interaction effects in the right structural form to get the best predictive model? Ideally, we want the data to tell us what form all these features should have. Which features should not be in the model at all? Which important interaction effects need to be there? Which features have important saturation and threshold effects?</p>
<p>With <span class="math inline">\(L_1\)</span> regularization we can derive a large number of new features, adding new data columns with squared terms and interacted terms. Then we could just lean on the <span class="math inline">\(L_1\)</span> penalty to trim away the ones that are unnecessary for getting good predictive performance. The task remains… how do we enumerate all the possible non-linear terms and interaction terms that we might want to include.</p>
<p>One of the appeals of decision trees is that they can capture both non-linear relationships and interaction effects. They also gracefully handle features of different types (numeric, categorical, ordinal) and even handle missing data well. However, they tend not to have the best predictive performance.</p>
<p>We are going to combine the nice attributes of decision trees with a linear model with <span class="math inline">\(L_1\)</span> regularization and explore what we get. Consider creating a model involving all possible splits on SES in the NELS dataset. There are 2,554 possible single-split decision trees that could come just from using SES (ses&lt;-2.6, ses&lt;-2.5, …, ses&lt;2.5). When we ran <code>rpart()</code>, the algorithm searched over all of these and chose the <em>one</em> that offered the greatest improvement in predictive performance. What if instead we consider a model of the form <span class="math display">\[
f(\mathbf{x}) = \beta_0 + \beta_1T_1(\mathbf{x}) + \beta_2T_2(\mathbf{x}) + \ldots  + \beta_{2554}T_{2554}(\mathbf{x})
\]</span> and used <span class="math inline">\(L_1\)</span> regularization to pick out the splits that are important. Let’s try this idea on the NELS dataset predicting dropout.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"data/nels.RData"</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># enumerate all possible splits (midpoints between adjacent values of SES)</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>sesSplits <span class="ot">&lt;-</span> <span class="fu">unique</span>(nels0<span class="sc">$</span>ses) <span class="sc">|&gt;</span> <span class="fu">sort</span>()</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>sesSplits <span class="ot">&lt;-</span> sesSplits[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">-</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">diff</span>(sesSplits)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then we’ll take each student in the NELS dataset and create 2,554 0/1 features about whether their SES would put them in the left or right branch of the associated decision tree.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">lapply</span>(sesSplits, <span class="cf">function</span>(x) <span class="fu">as.numeric</span>(nels0<span class="sc">$</span>ses <span class="sc">&lt;</span> x)) <span class="sc">|&gt;</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">do.call</span>(cbind, <span class="at">args=</span>_)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 11381  2554</code></pre>
</div>
</div>
<p>For example, <span class="math inline">\(T_3(ses)\)</span> has a split point at -2.3745. So <code>X[,3]</code> will be 1 for all observations with SES less than -2.3745 and 0 otherwise. Let’s check.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>nels0<span class="sc">$</span>ses[X[,<span class="dv">3</span>]<span class="sc">==</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -2.519 -2.414 -2.414 -2.875</code></pre>
</div>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">range</span>(nels0<span class="sc">$</span>ses[X[,<span class="dv">3</span>]<span class="sc">==</span><span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -2.335  2.560</code></pre>
</div>
</div>
<p>Sure enough, when <code>X[,3]==1</code> all SES values are less than the <span class="math inline">\(T_3(ses)\)</span> split point and all the SES values are greater than the <span class="math inline">\(T_3(ses)\)</span> split point when <code>X[,3]==0</code>.</p>
<p>Let’s throw all of these columns into <code>cv.glmnet()</code> to do 10-fold cross-validation to determine how much to penalize <span class="math inline">\(\sum |\beta_j|\)</span> to get the best predictive performance. Note that I have set <code>family=binomial</code> since we have 0/1 outcome. This will use the Bernoulli log-likelihood as the loss function. I have also set this up to run 10-fold cross-validation in parallel since this will take many minutes to run.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20240312</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(doParallel)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>cl <span class="ot">&lt;-</span> <span class="fu">makeCluster</span>(<span class="dv">10</span>)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="fu">registerDoParallel</span>(cl)</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>timeL1 <span class="ot">&lt;-</span> <span class="fu">system.time</span>(</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>  lasso1 <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, </span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>                      nels0<span class="sc">$</span>wave4dropout, </span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>                      <span class="at">family=</span><span class="st">"binomial"</span>,  <span class="co"># logistic regression</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>                      <span class="at">alpha=</span><span class="dv">1</span>,</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>                      <span class="at">parallel =</span> <span class="cn">TRUE</span>)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(timeL1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   user  system elapsed 
 165.65    4.33 1270.59 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">stopCluster</span>(cl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ten-fold cross-validation with a dataset with 11381 students and 2554 features took my computer 21 minutes. That is a long time to figure out the relationship with a single feature.</p>
<p>Let’s plot the cross-validated error to see how well it worked.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso1)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the best coefficients</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>betaHat <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso1, <span class="at">s =</span> <span class="st">"lambda.min"</span>) <span class="sc">|&gt;</span> <span class="fu">as.numeric</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-CVdropout" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-CVdropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-CVdropout-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-CVdropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Cross-validated error for predicting dropout in NELS88
</figcaption>
</figure>
</div>
</div>
</div>
<p><span class="math inline">\(L_1\)</span> regularization drastically cut down the number of terms from 2,554 to 31.</p>
<p>Finally, let’s examine what this model looks like. We can plot the relationship between SES and the predicted probability of dropout.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get predicted probabilities (betahat[1] is the intercept)</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>pHat <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>(betaHat[<span class="dv">1</span>] <span class="sc">+</span> X <span class="sc">%*%</span> betaHat[<span class="sc">-</span><span class="dv">1</span>])))</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="co"># or predict(lasso1, newx=X, s="lambda.min", type="response")</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the relationship between ses and P(dropout|ses)</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="fu">order</span>(nels0<span class="sc">$</span>ses)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nels0<span class="sc">$</span>ses[i], pHat[i], <span class="at">type=</span><span class="st">"l"</span>,</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"SES"</span>, <span class="at">ylab=</span><span class="st">"P(dropout|ses)"</span>)</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="fu">rug</span>(<span class="fu">quantile</span>(nels0<span class="sc">$</span>ses, <span class="at">probs =</span> (<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">/</span><span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-SESvDropoutP" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-SESvDropoutP-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-SESvDropoutP-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-SESvDropoutP-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Relationship between SES and probability of dropout using lasso and all possible splits on SES
</figcaption>
</figure>
</div>
</div>
</div>
<p>We get a picture similar to those we have seen before showing a strong threshold and saturation effect with lower levels of SES associated with higher dropout rates and low dropout rates once SES exceeds 0.</p>
<p>So, this approach seems to “work,” but there are substantial technical hurdles. This was a complicated model to fit to a dataset with 11381 cases using only a single feature. If we consider the NELS dataset and all 14 features that we used when experimenting with decision trees, there are 2631 possible single split trees. To include interaction effects, we need to allow for decision trees with two splits. There are 6,922,161 possible two-split decision trees! To code them as 0/1 columns we would need 2 columns in <code>X</code> to code the three terminal nodes. So, to include all single split trees and all two-split trees we would need to create 13,846,953 columns in <code>X</code>.</p>
<p>Now we are reaching complete impracticality. We cannot possibly include all of them and run <code>glmnet()</code>. We need an alternative route to use <span class="math inline">\(L_1\)</span> regularization to trim them down to a smaller set.</p>
</section>
<section id="gradient-boosted-models" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="gradient-boosted-models"><span class="header-section-number">2.2</span> Gradient boosted models</h2>
<p>As we saw in a previous section, there is an (approximately) equivalent approach to <span class="math inline">\(L_1\)</span> regularization using an incremental forward stagewise selection process. Previously we examined an iterative process with which we would add a small amount of one feature to the current model. <span class="math display">\[
\begin{split}
f_0(\mathbf{x}) &amp;= \bar y \\
f_{k+1}(\mathbf{x}) &amp;= f_k(\mathbf{x}) + \lambda x_{j^*}
\end{split}
\]</span> At each iteration we would choose <span class="math inline">\(x_{j^*}\)</span> to make the directional derivative <span class="math inline">\(-(\mathbf{y}-\mathbf{\hat y})'\mathbf{x}_j\)</span> as small as possible (large negative value). Let’s allow for a little more complexity and find the decision tree that offers the greatest reduction in squared error. <span class="math display">\[
\begin{split}
f_0(\mathbf{x}) &amp;= \bar y \\
f_{k+1}(\mathbf{x}) &amp;= f_k(\mathbf{x}) + \lambda T_{j^*}(\mathbf{x})
\end{split}
\]</span> As before, we can try to find the tree that has the most negative directional derivative. <span class="math display">\[
\begin{split}
\left.\frac{d}{d\lambda} J(\lambda) \right|_{\lambda=0} &amp;=
  \left.\frac{d}{d\lambda} \sum_{i=1}^n (y_i-(f_k(\mathbf{x}_i)+\lambda T_j(\mathbf{x}_i)))^2 \right|_{\lambda=0}  \\
  &amp;=
   \left.\sum_{i=1}^n -2(y_i-(f_k(\mathbf{x}_i)+\lambda T_j(\mathbf{x}_i)))T_j(\mathbf{x}_i))\right|_{\lambda=0} \\
  &amp;= -2\sum_{i=1}^n (y_i-f_k(\mathbf{x}_i))T_j(\mathbf{x}_i)
\end{split}
\]</span> To make the directional derivative large</p>
<ol type="1">
<li>we should try to make the sign of <span class="math inline">\(T_j(\mathbf{x})\)</span> agree with <span class="math inline">\(y_i-f_k(\mathbf{x}_i)\)</span></li>
<li>we should make <span class="math inline">\(T_j(\mathbf{x})\)</span> larger when <span class="math inline">\(y_i-f_k(\mathbf{x}_i)\)</span> is larger</li>
</ol>
<p>These two points suggest that <span class="math inline">\(T_j(\mathbf{x})\)</span> should be the decision tree that best predicts <span class="math inline">\(y-f_k(\mathbf{x})\)</span>, the errors or residuals of the current model. This makes sense. If we have a current model, then what should we add to it to make it better? A model that is good at predicting the current model’s errors.</p>
<p><span class="citation" data-cites="GradientBoost">Friedman (<a href="#ref-GradientBoost" role="doc-biblioref">2001</a>)</span> proposed the general gradient boosted machine algorithm as follows for a generic loss function <span class="math inline">\(J(\mathbf{y},f)\)</span> that we are trying to minimize.</p>
<ol type="1">
<li>Initialize <span class="math inline">\(\hat f_0(\mathbf{x})\)</span> to be a constant, <span class="math inline">\(\hat f_0(\mathbf{x}) = \arg \min_{a} \sum_{i=1}^n J(y_i,a)\)</span></li>
<li>For <span class="math inline">\(k\)</span> in <span class="math inline">\(1,\ldots,K\)</span> do
<ol type="a">
<li>Compute the negative gradient as the working response <span class="math inline">\(z_i = \left.-\frac{\partial}{\partial f(\mathbf{x}_i)} J(y_i,f(\mathbf{x}_i)) \right|_{f(\mathbf{x}_i)=\hat f(\mathbf{x}_i)}\)</span></li>
<li>Fit a decision tree, <span class="math inline">\(T(\mathbf{x})\)</span>, predicting <span class="math inline">\(z_i\)</span> from the covariates <span class="math inline">\(\mathbf{x}_i\)</span></li>
<li>Update the current prediction model as <span class="math inline">\(f_k(\mathbf{x}) \leftarrow \hat f_{k-1}(\mathbf{x}) + \lambda T(\mathbf{x})\)</span></li>
</ol></li>
</ol>
<p>At this point note that the final update closely resembles the gradient descent algorithm. We used gradient descent to improve our estimates of a set of parameters like <span class="math display">\[
\hat\beta \leftarrow \hat\beta - \lambda J'(\hat\beta)
\]</span> The gradient boosting algorithm is also a gradient descent algorithm only it is optimizing an entire prediction function rather than parameters used to tune a prediction model. Since the decision trees, <span class="math inline">\(T(\mathbf{x})\)</span>, are approximating a derivative, we can think of the gradient boosting algorithm as an analogous functional gradient descent. <span class="math display">\[
\begin{split}
\hat f(\mathbf{x}) &amp;\leftarrow \hat f(\mathbf{x}) + \lambda T(\mathbf{x})\\
&amp; \approx \hat f(\mathbf{x}) - \lambda J'(\hat f(\mathbf{x}))
\end{split}
\]</span></p>
</section>
<section id="gradient-boosting-algorithm-for-squared-error" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="gradient-boosting-algorithm-for-squared-error"><span class="header-section-number">2.3</span> Gradient boosting algorithm for squared error</h2>
<p>For squared error we have <span class="math display">\[
J(\mathbf{y}, f) = \sum_{i=1}^n (y_i-f(\mathbf{x}_i))^2
\]</span> Initialize <span class="math display">\[
\begin{split}
\hat f_0(\mathbf{x}) &amp;= \arg \min_a \sum_{i=1}^n (y_i-a)^2 \\
           &amp;= \bar y
\end{split}
\]</span> The working response for squared error is <span class="math display">\[
\begin{split}
z_i &amp;= \left.-\frac{\partial}{\partial f(\mathbf{x}_i)} (y_i-f(\mathbf{x}_i))^2 \right|_{f(\mathbf{x}_i)=\hat f(\mathbf{x}_i)} \\
&amp;= \left. 2(y_i-f(\mathbf{x}_i)) \right|_{f(\mathbf{x}_i)=\hat f(\mathbf{x}_i)} \\
&amp;= 2(y_i-\hat f(\mathbf{x}_i))
\end{split}
\]</span> Normally we ignore any constants (like the 2 here) since they will just get absorbed into the tree or into <span class="math inline">\(\lambda\)</span>.</p>
<p>We have completed the only mathematical work that we needed to do and now we can assemble an algorithm to do the work for us. Let’s take one step of the gbm algorithm, moving from <span class="math inline">\(f_0\)</span> to <span class="math inline">\(f_1\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co"># set lambda to be a small number</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize to the mean</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>fxHat <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">mean</span>(nels0<span class="sc">$</span>wave4dropout), <span class="fu">nrow</span>(nels0))</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the working response</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>nels0<span class="sc">$</span>z <span class="ot">&lt;-</span> nels0<span class="sc">$</span>wave4dropout <span class="sc">-</span> fxHat</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="co"># fit a regression tree to the working response</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>tree1 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(z <span class="sc">~</span> ses,</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>               <span class="at">data=</span>nels0,</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>               <span class="at">method =</span> <span class="st">"anova"</span>,</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>               <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">xval=</span><span class="dv">0</span>, <span class="at">maxdepth =</span> <span class="dv">1</span>))</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a><span class="co"># see which tree got selected first</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">xpd=</span><span class="cn">NA</span>)</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tree1); <span class="fu">text</span>(tree1)</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a><span class="co"># update to get f_1(x)</span></span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>fxHat <span class="ot">&lt;-</span> fxHat <span class="sc">+</span> lambda <span class="sc">*</span> <span class="fu">predict</span>(tree1, <span class="at">newdata=</span>nels0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-GBM1tree" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-GBM1tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-GBM1tree-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-GBM1tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: The decision tree selected in the first iteration
</figcaption>
</figure>
</div>
</div>
</div>
<p>After a single iteration we can take a look at <span class="math inline">\(f_1\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="fu">order</span>(nels0<span class="sc">$</span>ses)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nels0<span class="sc">$</span>ses[i], fxHat[i], <span class="at">type=</span><span class="st">"l"</span>,</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"ses"</span>,</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="fu">expression</span>(f[<span class="dv">1</span>](x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-GBM1iteration" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-GBM1iteration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-GBM1iteration-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-GBM1iteration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: GBM estimate after one iteration
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now let’s let it run for 1,000 total iterations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">1000</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>  nels0<span class="sc">$</span>z <span class="ot">&lt;-</span> nels0<span class="sc">$</span>wave4dropout <span class="sc">-</span> fxHat</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>  tree1 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(z <span class="sc">~</span> ses,</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data=</span>nels0,</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">method =</span> <span class="st">"anova"</span>,</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">xval=</span><span class="dv">0</span>, <span class="at">maxdepth =</span> <span class="dv">1</span>))</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>  fxHat <span class="ot">&lt;-</span> fxHat <span class="sc">+</span> lambda <span class="sc">*</span> <span class="fu">predict</span>(tree1, <span class="at">newdata=</span>nels0)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="fu">order</span>(nels0<span class="sc">$</span>ses)</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nels0<span class="sc">$</span>ses[i], fxHat[i], <span class="at">type=</span><span class="st">"l"</span>,</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"ses"</span>,</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="fu">expression</span>(f[<span class="dv">1000</span>](x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-boostFS1000" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boostFS1000-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-boostFS1000-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boostFS1000-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: GBM estimate after 1,000 iterations
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="gradient-boosting-algorithm-for-negative-bernoulli-log-likelihood-logitboost" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="gradient-boosting-algorithm-for-negative-bernoulli-log-likelihood-logitboost"><span class="header-section-number">2.4</span> Gradient boosting algorithm for negative Bernoulli log-likelihood (LogitBoost)</h2>
<p>If our interest is obtaining the best estimates of the <em>probability</em> of dropout, the Bernoulli log-likelihood is the loss function best designed for good probability estimates. Note that I have the <em>negative</em> Bernoulli log-likelihood so that we are aiming to minimize this loss function. <span class="math display">\[
J(\mathbf{y}, f) = -\sum_{i=1}^n y_if(\mathbf{x}_i) - \log(1+e^{f(\mathbf{x}_i)})
\]</span> Initialize <span class="math display">\[
\begin{split}
\hat f_0(\mathbf{x}) &amp;= \arg \min_a
   -\sum_{i=1}^n y_ia - \log(1+e^a) \\
           &amp;= \log\frac{\bar y}{1-\bar y}
\end{split}
\]</span> The working response for squared error is <span class="math display">\[
\begin{split}
z_i &amp;= \left.-\frac{\partial}{\partial f(\mathbf{x}_i)}
  -\sum_{i=1}^n y_if(\mathbf{x}_i) - \log(1+e^{f(\mathbf{x}_i)})
  \right|_{f(\mathbf{x}_i)=\hat f(\mathbf{x}_i)} \\
&amp;= \left. y_i-\frac{1}{1+e^{-f(\mathbf{x}_i)}}
    \right|_{f(\mathbf{x}_i)=\hat f(\mathbf{x}_i)} \\
&amp;= y_i-\frac{1}{1+e^{-\hat f(\mathbf{x}_i)}} \\
&amp;= y_i-\hat y_i
\end{split}
\]</span> Again, this all the mathematical work that we needed to do to assemble the GBM algorithm. Along the way we have encountered quantities that are not surprising at this point. The baseline log odds are the initial estimate and the working response is just the difference between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(\hat y_i\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set lambda to be a small number</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize to the baseline log odds</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>yBar <span class="ot">&lt;-</span> <span class="fu">mean</span>(nels0<span class="sc">$</span>wave4dropout)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>fxHat <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">log</span>(yBar<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>yBar)), <span class="fu">nrow</span>(nels0))</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the working response</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>nels0<span class="sc">$</span>z <span class="ot">&lt;-</span> nels0<span class="sc">$</span>wave4dropout <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>fxHat))</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="co"># fit a regression tree to the working response</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>tree1 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(z <span class="sc">~</span> ses,</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>               <span class="at">data=</span>nels0,</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>               <span class="at">method =</span> <span class="st">"anova"</span>,</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>               <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">xval=</span><span class="dv">0</span>, <span class="at">maxdepth =</span> <span class="dv">1</span>))</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a><span class="co"># see which tree got selected first</span></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">xpd=</span><span class="cn">NA</span>)</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tree1); <span class="fu">text</span>(tree1)</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a><span class="co"># update to get f_1(x)</span></span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>fxHat <span class="ot">&lt;-</span> fxHat <span class="sc">+</span> lambda <span class="sc">*</span> <span class="fu">predict</span>(tree1, <span class="at">newdata=</span>nels0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-treeModel1LogitBoost" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-treeModel1LogitBoost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-treeModel1LogitBoost-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-treeModel1LogitBoost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: First decision tree selected for LogitBoost
</figcaption>
</figure>
</div>
</div>
</div>
<p>After a single iteration we can take a look at <span class="math inline">\(f_1\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="fu">order</span>(nels0<span class="sc">$</span>ses)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nels0<span class="sc">$</span>ses[i], fxHat[i], <span class="at">type=</span><span class="st">"l"</span>,</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"ses"</span>,</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="fu">expression</span>(f[<span class="dv">1</span>](x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-fx1LogitBoost" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fx1LogitBoost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-fx1LogitBoost-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fx1LogitBoost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: GBM estimate after 1 iteration of the LogitBoost algorithm
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now let’s let it run for 1,000 total iterations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">1000</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>  nels0<span class="sc">$</span>z <span class="ot">&lt;-</span> nels0<span class="sc">$</span>wave4dropout <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>fxHat))</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>  tree1 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(z <span class="sc">~</span> ses,</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data=</span>nels0,</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">method =</span> <span class="st">"anova"</span>,</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">xval=</span><span class="dv">0</span>, <span class="at">maxdepth =</span> <span class="dv">1</span>))</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>  fxHat <span class="ot">&lt;-</span> fxHat <span class="sc">+</span> lambda <span class="sc">*</span> <span class="fu">predict</span>(tree1, <span class="at">newdata=</span>nels0)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="fu">order</span>(nels0<span class="sc">$</span>ses)</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nels0<span class="sc">$</span>ses[i], fxHat[i], <span class="at">type=</span><span class="st">"l"</span>,</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"ses"</span>,</span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="fu">expression</span>(f[<span class="dv">1000</span>](x)),</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"log odds scale"</span>)</span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nels0<span class="sc">$</span>ses[i], <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>fxHat[i])), <span class="at">type=</span><span class="st">"l"</span>,</span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"ses"</span>,</span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="fu">expression</span>(P[<span class="dv">1000</span>](x)),</span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"probability scale"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-boostFSnegBern1000SES" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boostFSnegBern1000SES-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-boostFSnegBern1000SES-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boostFSnegBern1000SES-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Prediction model after 1,000 iterations of the LogitBoost algorithm
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="adaboost-algorithm" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="adaboost-algorithm"><span class="header-section-number">2.5</span> AdaBoost algorithm</h2>
<p><span class="citation" data-cites="FS">Freund and Schapire (<a href="#ref-FS" role="doc-biblioref">1997</a>)</span> proposed the first boosting algorithm, the adaptive boosting or AdaBoost algorithm. Although the paper appeared in 1997, the idea was circulating around for 1-2 years earlier. It got tremendous attention after <span class="citation" data-cites="Elkan">Elkan (<a href="#ref-Elkan" role="doc-biblioref">1997</a>)</span> tried a boosted naïve Bayes classifier and won the Knowledge Discovery and Data Mining Cup in 1997, a prediction contest in which his submission outperformed other extremely well-funded teams.</p>
<p>The computer scientists described the approach algorithmically at the time. That is, they described the recipe for how to do it, but offered only some insight into why it should be better than other existing methods at the time. <span class="citation" data-cites="FHT">Friedman, Hastie, and Tibshirani (<a href="#ref-FHT" role="doc-biblioref">2000</a>)</span> revealed that the algorithm was a gradient descent style algorithm with a particular objective function.</p>
<p>Consider the following representation of the misclassification error rate <span class="math display">\[
J(\mathbf{y},f) = \frac{1}{n}\sum_{i=1}^n I(2y_i-1 \neq \mathrm{sign}(f(\mathbf{x}_i)))
\]</span> <span class="math inline">\(2y_i-1\)</span> simply takes the <span class="math inline">\(\{0,1\}\)</span> outcomes and translates them to be <span class="math inline">\(\{-1,1\}\)</span>. Then all we care about for predictions is the sign on the prediction function <span class="math inline">\(f(\mathbf{x})\)</span>. When <span class="math inline">\(f(\mathbf{x})&gt;0\)</span> then we predict <span class="math inline">\(y=1\)</span> and when <span class="math inline">\(f(\mathbf{x})&lt;0\)</span> we predict <span class="math inline">\(y=0\)</span>… or that <span class="math inline">\(2y-1=-1\)</span>. Another way to represent a classification error instead of <span class="math inline">\(2y_i-1 \neq \mathrm{sign}(f(\mathbf{x}_i))\)</span> is that <span class="math inline">\((2y_i-1)(f(\mathbf{x}_i)) &lt; 0\)</span>. That is, the prediction model and <span class="math inline">\(2y-1\)</span> disagree on signs, then there is a misclassification.</p>
<p><span class="citation" data-cites="RealAdaBoost">Schapire and Singer (<a href="#ref-RealAdaBoost" role="doc-biblioref">1999</a>)</span> noted that there is a differentiable upper bound on the misclassification rate. <span class="math display">\[
\begin{split}
J(\mathbf{y},f) &amp;= \frac{1}{n}\sum_{i=1}^n I(2y_i-1 \neq \mathrm{sign}(f(\mathbf{x}_i))) \\
&amp;\leq \frac{1}{n}\sum_{i=1}^n \exp(-(2y_i-1)f(\mathbf{x}_i))
\end{split}
\]</span> To see why this inequality holds, it is helpful to look at the next figure. Misclassification is represented by the step function in the figure below. Note that the AdaBoost exponential loss function is always larger than the misclassification indicator function. They only intersect at (0,1). So the idea of the AdaBoost algorithm is that, since derivatives and optimization of the misclassification loss function is awkward, use the exponential upper bound and minimize that instead.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>yF <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>, <span class="at">length.out=</span><span class="dv">100</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(yF, <span class="fu">exp</span>(<span class="sc">-</span>yF), <span class="at">type=</span><span class="st">"n"</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">4</span>),</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="fu">expression</span>((<span class="dv">2</span><span class="sc">*</span>y<span class="dv">-1</span>)<span class="sc">*</span><span class="fu">f</span>(x)),</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="fu">expression</span>(<span class="fu">J</span>(f)))</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span>yF)<span class="sc">&lt;</span><span class="dv">4</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(yF[i], <span class="fu">exp</span>(<span class="sc">-</span>yF)[i], <span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(yF, yF<span class="sc">&lt;</span><span class="dv">0</span>, <span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="fu">log</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>yF)) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">-</span><span class="fu">log</span>(<span class="dv">2</span>)) <span class="sc">&lt;</span> <span class="dv">4</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(yF[i], <span class="fu">log</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>yF[i]))<span class="sc">/</span>(<span class="fu">log</span>(<span class="dv">2</span>)), <span class="at">col=</span><span class="st">"red"</span>, <span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(yF, <span class="dv">4</span><span class="sc">*</span>(<span class="dv">1-1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>yF)))<span class="sc">^</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="sc">-</span><span class="fl">1.16</span>,<span class="fl">3.53</span>,<span class="st">"AdaBoost"</span>, <span class="at">adj=</span><span class="dv">0</span>)</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="sc">-</span><span class="fl">1.83</span>,<span class="fl">2.3</span>,<span class="st">"Bernoulli"</span>,<span class="at">adj=</span><span class="dv">0</span>)</span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="sc">-</span><span class="fl">2.0</span>,<span class="fl">3.2</span>,<span class="st">"Squared error"</span>,<span class="at">adj=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-boundsOnMisclassRate" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boundsOnMisclassRate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-boundsOnMisclassRate-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boundsOnMisclassRate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Bounds on the misclassification rate
</figcaption>
</figure>
</div>
</div>
</div>
<p>It also turns out that the negative Bernoulli log-likelihood also is an upper bound on the misclassification rate. You can show that <span class="math display">\[
1-\frac{1}{1+e^{-f(\mathbf{x})}} = \frac{1}{1+e^{f(\mathbf{x})}}
\]</span> So we can write <span class="math display">\[
\begin{split}
-y\log\frac{1}{1+e^{-f(\mathbf{x})}} -
   (1-y)\log\left(1-\frac{1}{1+e^{-f(\mathbf{x})}}\right) &amp;=
  -\log\frac{1}{1+e^{-(2y-1)f(\mathbf{x})}} \\
  &amp;= \log (1+e^{-(2y-1)f(\mathbf{x})})
\end{split}
\]</span> And we can show that, with a little rescaling, this too can be an upperbound on misclassification. <span class="math display">\[
\begin{split}
J(\mathbf{y},f) &amp;= \frac{1}{n}\sum_{i=1}^n I(2y_i-1 \neq \mathrm{sign}(f(\mathbf{x}_i))) \\
&amp;\leq \frac{1}{n}\sum_{i=1}^n \frac{1}{\log 2}\log (1+e^{-(2y_i-1)f(\mathbf{x}_i)})
\end{split}
\]</span> This bound is shown with the red curve in the figure above.</p>
<p>Also squared error can be an upper bound on misclassification. <span class="math display">\[
\begin{split}
J(\mathbf{y},f) &amp;= \frac{1}{n}\sum_{i=1}^n I(2y_i-1 \neq \mathrm{sign}(f(\mathbf{x}_i))) \\
&amp;\leq \frac{1}{n}\sum_{i=1}^n
    4\left(y_i - \frac{1}{1+e^{-f(\mathbf{x}_i)}}\right)^2 \\
&amp;\leq \frac{1}{n}\sum_{i=1}^n
    4\left(1 - \frac{1}{1+e^{-(2y_i-1)f(\mathbf{x}_i)}}\right)^2
\end{split}
\]</span> The figure shows the squared error bound in blue.</p>
</section>
<section id="gradient-boosting-as-an-additive-model" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="gradient-boosting-as-an-additive-model"><span class="header-section-number">2.6</span> Gradient boosting as an additive model</h2>
<p>In our previous experiments with boosting we just tried out the method with a single variable and trees with single splits. I introduced it this way to keep it simpler and so that we could easily visualize the result. I will continue to use only single split decision trees but will introduce additional variables.</p>
<p>Because we are using only single split decision trees, each tree will be a function of only a single variable. As a result, the prediction function will be of the form <span class="math display">\[
\begin{split}
f(\mathbf{x}) =&amp; \beta_0 +\beta_1 T_1(\mathrm{ses})+\beta_2 T_2(\mathrm{pctFreeLunch})+
        \beta_3 T_3(\mathrm{ses})+\beta_4 T_4(\mathrm{urbanicity})+ \\
      &amp;  \beta_5 T_5(\mathrm{ses})+\beta_6 T_6(\mathrm{ses})+\beta_7 T_7(\mathrm{pctFreeLunch})+\beta_8 T_8(\mathrm{urbanicity}) \\
    &amp;  + \ldots + \beta_{1000} T_{1000}(\mathrm{famIncome})
\end{split}
\]</span> Once we are finished with our boosting iterations, we can reorder the terms like <span class="math display">\[
\begin{split}
f(\mathbf{x}) &amp;= \beta_0 + \\
   &amp;\quad \beta_1 T_1(\mathrm{ses})+\beta_3 T_3(\mathrm{ses})+
     \beta_5 T_5(\mathrm{ses})+\beta_6 T_6(\mathrm{ses}) + \\
   &amp;\quad \beta_2 T_2(\mathrm{pctFreeLunch})+\beta_7 T_7(\mathrm{pctFreeLunch}) +\\
   &amp;\quad \beta_4 T_4(\mathrm{urbanicity})+ \beta_8 T_8(\mathrm{urbanicity}) +\\
   &amp;\quad  \ldots + \\
   &amp;\quad \beta_{1000} T_{1000}(\mathrm{famIncome}) \\
   &amp;= \beta_0 + f_1(\mathrm{ses}) + f_2(\mathrm{pctFreeLunch}) + \\
   &amp;\quad f_3(\mathrm{urbanicity}) +\ldots + f_{14}(\mathrm{famIncome}) \\
\end{split}
\]</span> This is known as an <em>additive</em> model. It is not a linear model in that each of the functions <span class="math inline">\(f_j(x)\)</span> are not restricted to be linear, but each non-linear function gets added together to form the final prediction function.</p>
<p>As before we will initialize <span class="math inline">\(\lambda\)</span> to be our small step size or learning rate and <span class="math inline">\(\beta_0\)</span> to be the baseline log odds.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set lambda to be a small number</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize to the baseline log odds</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>yBar <span class="ot">&lt;-</span> <span class="fu">mean</span>(nels0<span class="sc">$</span>wave4dropout)</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="fu">log</span>(yBar<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>yBar))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Instead of having just one column contain the predictions, I will make a matrix with 14 columns to keep track of the predictions for each of the 14 student features that we will use in the analysis.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>fxHatM <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span><span class="fu">nrow</span>(nels0), <span class="at">ncol=</span><span class="dv">14</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(fxHatM) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"typeSchool"</span>,<span class="st">"urbanicity"</span>,<span class="st">"region"</span>,<span class="st">"pctMinor"</span>,</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"pctFreeLunch"</span>,<span class="st">"female"</span>,<span class="st">"race"</span>,<span class="st">"ses"</span>,<span class="st">"parentEd"</span>,</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"famSize"</span>,<span class="st">"famStruct"</span>,<span class="st">"parMarital"</span>,<span class="st">"famIncome"</span>,</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"langHome"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We will run the same boosting algorithm except now allowing 14 student features to be included in the prediction model. Also, we will be tracking which variable gets split and updating only that column of <code>fxHatM</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3000</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute current prediction function</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>  fxHat <span class="ot">&lt;-</span> beta0 <span class="sc">+</span> <span class="fu">rowSums</span>(fxHatM)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute working response</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>  nels0<span class="sc">$</span>z <span class="ot">&lt;-</span> nels0<span class="sc">$</span>wave4dropout <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>fxHat))</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>  tree1 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(z<span class="sc">~</span>typeSchool<span class="sc">+</span>urbanicity<span class="sc">+</span>region<span class="sc">+</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>                   pctMinor<span class="sc">+</span>pctFreeLunch<span class="sc">+</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>                   female<span class="sc">+</span>race<span class="sc">+</span>ses<span class="sc">+</span>parentEd<span class="sc">+</span>famSize<span class="sc">+</span>famStruct<span class="sc">+</span>parMarital<span class="sc">+</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>                   famIncome<span class="sc">+</span>langHome,</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">method=</span><span class="st">"anova"</span>,</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data=</span>nels0,</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>                 <span class="at">control=</span><span class="fu">rpart.control</span>(<span class="at">cp=</span><span class="fl">0.0</span>, <span class="at">xval=</span><span class="dv">0</span>, <span class="at">maxdepth=</span><span class="dv">1</span>))</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># which variable got split?</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>  varSplit <span class="ot">&lt;-</span> <span class="fu">rownames</span>(tree1<span class="sc">$</span>splits)[<span class="dv">1</span>]</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># update only the component associated with the split variable</span></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>  fxHatM[,varSplit] <span class="ot">&lt;-</span> fxHatM[,varSplit] <span class="sc">+</span> lambda <span class="sc">*</span> <span class="fu">predict</span>(tree1,<span class="at">newdata=</span>nels0)</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since we have all of the individual components separately, we can plot them out separately to see what patterns the gradient boosted machine learning algorithm uncovered.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">2</span>), <span class="at">mai=</span><span class="fu">c</span>(<span class="fl">0.52</span>,<span class="fl">0.25</span>,<span class="fl">0.25</span>,<span class="fl">0.25</span>))</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>j <span class="ot">&lt;-</span> <span class="fu">apply</span>(fxHatM, <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="fu">any</span>(x<span class="sc">!=</span><span class="dv">0</span>))</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>ylim <span class="ot">&lt;-</span> <span class="fu">range</span>(fxHatM)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(xj <span class="cf">in</span> <span class="fu">names</span>(j[j]))</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>  i <span class="ot">&lt;-</span> <span class="fu">order</span>(nels0[,xj])</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(nels0[i,xj], fxHatM[i,xj], <span class="at">type=</span><span class="st">"l"</span>,</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab=</span>xj,</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab=</span><span class="fu">expression</span>(<span class="fu">f</span>(x)),</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">main=</span><span class="st">""</span>,</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim=</span>ylim)</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-additiveDecompFx" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-additiveDecompFx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-additiveDecompFx-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-additiveDecompFx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Decomposition of functions predicting droput
</figcaption>
</figure>
</div>
</div>
</div>
<p>SES clearly has the largest impact on dropout prediction. The other factors, such as family structure, race, and family size have modest effects on the predictions.</p>
</section>
</section>
<section id="using-the-generalized-boosted-models-gbm3-package" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Using the Generalized Boosted Models (gbm3) package</h1>
<p>I wrote the initial version of <code>gbm</code> in 1999 at the time when the use of boosting was really taking off. Since then, numerous collaborators have added new features, new loss functions, and made it run in parallel. The current version of <code>gbm</code> has been downloaded almost 3 million times and about 25,000 times per month. Over 2,000 published scientific articles have used <code>gbm</code>.</p>
<p>There are newer implementations worth evaluating. <code>lightgbm</code> was developed at Microsoft Research <span class="citation" data-cites="LightGBM:2017">(<a href="#ref-LightGBM:2017" role="doc-biblioref">Ke et al. 2017</a>)</span>. <code>xgboost</code> (extreme gradient boosting) started at the University of Washington and now has the backing of Nvidia and Intel <span class="citation" data-cites="XGBoost">(<a href="#ref-XGBoost" role="doc-biblioref">Chen and Guestrin 2016</a>)</span>. A few benefits of these newer implementations include the use of GPUs and large distributed systems, permitting sparse dataset representations, and availability from multiple environments (R, Python, Julia, etc.).</p>
<p>You are welcome to try <code>lightgbm</code> and <code>xgboost</code>. We are going to use the <code>gbm3</code>. You will need to install <code>gbm3</code> from GitHub. You only need to install from GitHub when first using <code>gbm3</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>remotes<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">"gbm-developers/gbm3"</span>, </span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>                         <span class="at">build_vignettes =</span> <span class="cn">TRUE</span>, <span class="at">force =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then load the <code>gbm3</code> package any time you want to use it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s predict dropout from 14 of the student features in the NELS dataset. Although I ignored them previously to keep the discussion simple, I have included the sampling weights here, which is the correct approach for these data. Since the outcome is 0/1 we set <code>distribution=gbm_dist("Bernoulli")</code>.</p>
<p>We will run 3,000 iterations and set <span class="math inline">\(\lambda=0.003\)</span> (shrinkage). The number of iterations and <span class="math inline">\(\lambda\)</span> go together. Generally smaller values of <span class="math inline">\(\lambda\)</span> result in better predictive performance. However, models fit with smaller values of <span class="math inline">\(\lambda\)</span> will require more iterations. Cutting <span class="math inline">\(\lambda\)</span> by a factor of 2 generally requires doubling the number of iterations. I generally try to run about 3,000 iterations and set <span class="math inline">\(\lambda\)</span> as small as I can but still get the best number of iterations to be right around 3,000.</p>
<p><span class="citation" data-cites="StochasticBoost">Friedman (<a href="#ref-StochasticBoost" role="doc-biblioref">2002</a>)</span> showed that fitting the trees to a randomly sampled half of the dataset at each iteration doubles the computation speed <em>and</em> results in improved predictive performance. Setting <code>bag_fraction=0.5</code> implements this. Some have experimented with randomly sampling features to consider at each iteration as well, but we will consider all of them by setting <code>num_features = 14</code>.</p>
<p><code>gbmt()</code> allows you to give it both a training and test set, but here we are going to ask <code>gbmt()</code> to learn from the entire NELS dataset by setting <code>num_train = nrow(nels0)</code>.</p>
<p>We are going to allow up to three-way interactions by setting <code>interaction_depth = 3</code>. This limits the number of splits that any of the decision trees can have to 3. So, every new tree added to the prediction function involves at most 3 distinct student features.</p>
<p>Lastly, we will do 10-fold cross-validation and use up to 12 cores to fit the model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20240316</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>gbm1 <span class="ot">&lt;-</span> <span class="fu">gbmt</span>(wave4dropout<span class="sc">~</span>typeSchool<span class="sc">+</span>urbanicity<span class="sc">+</span>region<span class="sc">+</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>                   pctMinor<span class="sc">+</span>pctFreeLunch<span class="sc">+</span>female<span class="sc">+</span>race<span class="sc">+</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>                   ses<span class="sc">+</span>parentEd<span class="sc">+</span>famSize<span class="sc">+</span>famStruct<span class="sc">+</span>parMarital<span class="sc">+</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>                   famIncome<span class="sc">+</span>langHome,</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">data=</span>nels0,</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>             <span class="at">weights=</span>nels0<span class="sc">$</span>F4QWT,</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">distribution=</span><span class="fu">gbm_dist</span>(<span class="st">"Bernoulli"</span>),</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">train_params =</span> <span class="fu">training_params</span>(</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">num_trees =</span> <span class="dv">3000</span>,         <span class="co"># number of trees</span></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>               <span class="at">shrinkage =</span> <span class="fl">0.003</span>,        <span class="co"># lambda</span></span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>               <span class="at">bag_fraction =</span> <span class="fl">0.5</span>,       <span class="co"># fit trees to random subsample</span></span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>               <span class="at">num_train =</span> <span class="fu">nrow</span>(nels0),</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>               <span class="at">min_num_obs_in_node =</span> <span class="dv">10</span>,</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>               <span class="at">interaction_depth =</span> <span class="dv">3</span>,    <span class="co"># number of splits</span></span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>               <span class="at">num_features =</span> <span class="dv">14</span>),       <span class="co"># number of features</span></span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a>             <span class="at">cv_folds=</span><span class="dv">10</span>,</span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a>             <span class="at">par_details=</span><span class="fu">gbmParallel</span>(<span class="at">num_threads=</span><span class="dv">12</span>),</span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>             <span class="at">is_verbose =</span> <span class="cn">FALSE</span>)         <span class="co"># don't print progress</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once complete, we can check in on the 10-fold cross-validation to determine the optimal number of iterations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>bestNTree <span class="ot">&lt;-</span> <span class="fu">gbmt_performance</span>(gbm1, <span class="at">method=</span><span class="st">"cv"</span>) </span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(bestNTree)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-gbmCVperf" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gbmCVperf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-gbmCVperf-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gbmCVperf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Training and cross-validated performance by number of iterations
</figcaption>
</figure>
</div>
</div>
</div>
<p>The figure shows that the optimal number of iterations occurs after 2014 iterations. The green line shows the cross-validated negative Bernoulli log-likelihood, while the black line shows the negative Bernoulli likelihood on the training dataset. The performance will always look better and better on the training dataset with each iteration, but this is because the model is very flexible and can start fitting unusual random quirks of the training dataset. Because the green curve is quite flat, the specific choice for the optimal number of iterations is not very sensitive. Choosing any value between 1500 and 3000 in this example would still result in a very reasonable model fit.</p>
<p>To use the model to predict on a dataset, there is a <code>predict()</code> method for the GBM object.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>pHat <span class="ot">&lt;-</span> <span class="fu">predict</span>(gbm1, <span class="at">newdata=</span>nels0, <span class="at">n.trees =</span> bestNTree, <span class="at">type =</span> <span class="st">"response"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It may take some effort to determine a good choice for <code>shrinkage</code>. Remember that these models tend to work best when <code>shrinkage</code> (that is, <span class="math inline">\(\lambda\)</span>) is small. But setting it too small means that the model will learn very slowly and require a lot more iterations (trees) to get to the optimal number of iterations. You might run 3,000 iterations and the performance plot still shows that the cross-validated error is still declining. You have two choices 1) increase the number of iterations, which will require more computation time, or 2) increase <code>shrinkage</code> so that the model learns a little faster. In general, small datasets will require small values of <code>shrinkage</code> (like 0.001) and larger datasets can get good performance with larger values of <code>shrinkage</code> like 0.1. Let’s try running this with a <code>shrinkage</code> that is too small.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20240316</span>)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>gbm2 <span class="ot">&lt;-</span> <span class="fu">gbmt</span>(wave4dropout<span class="sc">~</span>typeSchool<span class="sc">+</span>urbanicity<span class="sc">+</span>region<span class="sc">+</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>                   pctMinor<span class="sc">+</span>pctFreeLunch<span class="sc">+</span>female<span class="sc">+</span>race<span class="sc">+</span></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>                   ses<span class="sc">+</span>parentEd<span class="sc">+</span>famSize<span class="sc">+</span>famStruct<span class="sc">+</span>parMarital<span class="sc">+</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>                   famIncome<span class="sc">+</span>langHome,</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">data=</span>nels0,</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>             <span class="at">weights=</span>nels0<span class="sc">$</span>F4QWT,</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">distribution=</span><span class="fu">gbm_dist</span>(<span class="st">"Bernoulli"</span>),</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">train_params =</span> <span class="fu">training_params</span>(</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">num_trees =</span> <span class="dv">3000</span>,         <span class="co"># number of trees</span></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>               <span class="at">shrinkage =</span> <span class="fl">0.0003</span>,        <span class="co"># lambda</span></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>               <span class="at">bag_fraction =</span> <span class="fl">0.5</span>,       <span class="co"># fit trees to random subsample</span></span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>               <span class="at">num_train =</span> <span class="fu">nrow</span>(nels0),</span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a>               <span class="at">min_num_obs_in_node =</span> <span class="dv">10</span>,</span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>               <span class="at">interaction_depth =</span> <span class="dv">3</span>,    <span class="co"># number of splits</span></span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>               <span class="at">num_features =</span> <span class="dv">14</span>),       <span class="co"># number of features</span></span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a>             <span class="at">cv_folds=</span><span class="dv">10</span>,</span>
<span id="cb70-18"><a href="#cb70-18" aria-hidden="true" tabindex="-1"></a>             <span class="at">par_details=</span><span class="fu">gbmParallel</span>(<span class="at">num_threads=</span><span class="dv">12</span>),</span>
<span id="cb70-19"><a href="#cb70-19" aria-hidden="true" tabindex="-1"></a>             <span class="at">is_verbose =</span> <span class="cn">FALSE</span>)         <span class="co"># don't print progress</span></span>
<span id="cb70-20"><a href="#cb70-20" aria-hidden="true" tabindex="-1"></a><span class="fu">gbmt_performance</span>(gbm2,<span class="at">method=</span><span class="st">"cv"</span>) <span class="sc">|&gt;</span> <span class="fu">plot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-gbmCVperf2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gbmCVperf2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-gbmCVperf2-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gbmCVperf2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Training and cross-validated performance by number of iterations with <span class="math inline">\(\lambda\)</span> set too small
</figcaption>
</figure>
</div>
</div>
</div>
<p>Here you see that the GBM model with the lowest cross-validated error requires 3,000 iterations, but that the loss function is still declining at 3,000 iterations. Clearly more iterations are needed to reach that minimum… or we can increase <code>shrinkage</code> so that <code>gbmt()</code> takes larger learning steps at each iteration so it learns a little faster.</p>
<p>What happens if we err in the other direction and have <code>gbmt()</code> try to learn quickly with few iterations. Here I have set <span class="math inline">\(\lambda=0.03\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20240316</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>gbm3 <span class="ot">&lt;-</span> <span class="fu">gbmt</span>(wave4dropout<span class="sc">~</span>typeSchool<span class="sc">+</span>urbanicity<span class="sc">+</span>region<span class="sc">+</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>                   pctMinor<span class="sc">+</span>pctFreeLunch<span class="sc">+</span>female<span class="sc">+</span>race<span class="sc">+</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>                   ses<span class="sc">+</span>parentEd<span class="sc">+</span>famSize<span class="sc">+</span>famStruct<span class="sc">+</span>parMarital<span class="sc">+</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>                   famIncome<span class="sc">+</span>langHome,</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">data=</span>nels0,</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>             <span class="at">weights=</span>nels0<span class="sc">$</span>F4QWT,</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">distribution=</span><span class="fu">gbm_dist</span>(<span class="st">"Bernoulli"</span>),</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">train_params =</span> <span class="fu">training_params</span>(</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">num_trees =</span> <span class="dv">3000</span>,         <span class="co"># number of trees</span></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>               <span class="at">shrinkage =</span> <span class="fl">0.03</span>,        <span class="co"># lambda</span></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>               <span class="at">bag_fraction =</span> <span class="fl">0.5</span>,       <span class="co"># fit trees to random subsample</span></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>               <span class="at">num_train =</span> <span class="fu">nrow</span>(nels0),</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>               <span class="at">min_num_obs_in_node =</span> <span class="dv">10</span>,</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>               <span class="at">interaction_depth =</span> <span class="dv">3</span>,    <span class="co"># number of splits</span></span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>               <span class="at">num_features =</span> <span class="dv">14</span>),       <span class="co"># number of features</span></span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>             <span class="at">cv_folds=</span><span class="dv">10</span>,</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>             <span class="at">par_details=</span><span class="fu">gbmParallel</span>(<span class="at">num_threads=</span><span class="dv">12</span>),</span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>             <span class="at">is_verbose =</span> <span class="cn">FALSE</span>)         <span class="co"># don't print progress</span></span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a><span class="fu">gbmt_performance</span>(gbm3,<span class="at">method=</span><span class="st">"cv"</span>) <span class="sc">|&gt;</span> <span class="fu">plot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-gbmCVperf3" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gbmCVperf3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-gbmCVperf3-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gbmCVperf3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Training and cross-validated performance by number of iterations with <span class="math inline">\(\lambda\)</span> set too large
</figcaption>
</figure>
</div>
</div>
</div>
<p>It does indeed reach a minimum quickly with many fewer iterations than the last two models we fit. However, it does not perform nearly as well. The best model we fit stored in <code>gbm1</code> had a <span class="math inline">\(\lambda=0.003\)</span> and had an average negative Bernoulli log-likelihood of 0.3927299. The best performance we could get out of the fast learning model with <span class="math inline">\(\lambda=0.03\)</span> is 0.3911301. Therefore, when fitting gradient boosted models, set <code>shrinkage</code> to be as small as possible while still being able to fit the model in a reasonable amount of computer time.</p>
<section id="relative-influence" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="relative-influence"><span class="header-section-number">3.1</span> Relative influence</h2>
<p><span class="citation" data-cites="GradientBoost">Friedman (<a href="#ref-GradientBoost" role="doc-biblioref">2001</a>)</span> also developed an extension of a variable’s “relative influence”. If our loss function is squared error, then every split included in a decision tree is reducing that squared error. We can look across all the splits on SES, for example, in every tree and total up all the associated reductions in squared error. This would give us an idea about how much of the reduction in squared error is attributable to splits on SES. We would do the same process for all of the features used in the model.</p>
<p>The relative influence of a feature <span class="math inline">\(x_j\)</span> is <span class="math display">\[
\widehat{RI}_j = \sum_{\mathrm{splits~on~}x_j} I_t^2
\]</span> where <span class="math inline">\(I_t^2\)</span> is the empirical improvement in tree <span class="math inline">\(t\)</span> by splitting on <span class="math inline">\(x_j\)</span> at that point. <code>gbmt()</code> normalizes the relative influence so that they sum to 100 so that they represent the fraction of predictive performance that is attributable to each feature.</p>
<p>Here we see that SES is the primary predictor of dropout with family features (structure, income, parent’s marital status, and size) mattering to a more modest degree.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fl">0.1</span><span class="sc">+</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">7</span>,<span class="dv">4</span>,<span class="dv">2</span>))</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>relInf <span class="ot">&lt;-</span> <span class="fu">summary</span>(gbm1, <span class="at">num_trees=</span>bestNTree)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-relInf" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-relInf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-relInf-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-relInf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Relative influence of student features on dropout risk
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can also show the table with the same information that is in the figure.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>relInf</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                      var    rel_inf
ses                   ses 42.7479024
famStruct       famStruct 11.7912548
famIncome       famIncome  8.7639783
famSize           famSize  6.4373210
parMarital     parMarital  6.3303950
region             region  5.6741448
race                 race  5.1268490
pctFreeLunch pctFreeLunch  4.3914581
pctMinor         pctMinor  3.2686709
parentEd         parentEd  2.6661309
urbanicity     urbanicity  1.7141224
female             female  0.7765825
langHome         langHome  0.3111900
typeSchool     typeSchool  0.0000000</code></pre>
</div>
</div>
<p>Since we fit this model to minimize the negative Bernoulli log-likelihood, these numbers approximate the percentage reduction in that loss function attributable to each student feature.</p>
</section>
<section id="partial-dependence-plots" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="partial-dependence-plots"><span class="header-section-number">3.2</span> Partial dependence plots</h2>
<p>The relative influence describes which features are the most important in the prediction function, but does not describe their shape or even whether the relationship is positive or negative. The partial dependence plot offers some insight into these relationships.</p>
<p>Let’s say we are interested in one specific feature <span class="math inline">\(x_j\)</span>. We will call all the other features <span class="math inline">\(\mathbf{x}_{-j}\)</span>. GBM then produces a prediction function <span class="math inline">\(\hat f(x_j,\mathbf{x}_{-j})\)</span>. Partial dependence fixes a value for <span class="math inline">\(x_j\)</span> and then averages out over all the combinations of <span class="math inline">\(\mathbf{x}_{-j}\)</span> observed in the dataset. <span class="math display">\[
\hat f_j(x) = \frac{1}{n} \sum_{i=1}^n \hat f(x_j=x, \mathbf{x}_{-j}=\mathbf{x}_{i,-j})
\]</span> If we limit the trees to have at most a single split, then we get an additive model like we saw before and the partial dependence plots can be computed by just predicting using the decision trees that split on <span class="math inline">\(x_j\)</span>. In our NELS example we allowed for up to three-way interactions (three splits) so that in most trees <span class="math inline">\(x_j\)</span> will be interacted with other student features.</p>
<p><code>gbm3</code> has a <code>plot()</code> method that will show partial dependence plots. See <code>?plot.GBMFit</code> for details about the fitting and various options. Here are the partial dependence plots for the top nine most important predictors of dropout.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>), <span class="at">mai=</span><span class="fu">c</span>(<span class="fl">0.7</span>,<span class="fl">0.6</span>,<span class="fl">0.1</span>,<span class="fl">0.1</span>))</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(xj <span class="cf">in</span> relInf<span class="sc">$</span>var[<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>])</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(gbm1, <span class="at">var_index =</span> xj, <span class="at">num_trees =</span> bestNTree)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-partialDependenceDropout" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-partialDependenceDropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-partialDependenceDropout-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-partialDependenceDropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: Partial dependence of student features on dropout risk
</figcaption>
</figure>
</div>
</div>
</div>
<p>Again, SES shows strong threshold and saturation effects. You can extract the specific numbers used in the plots to get more details.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gbm1, <span class="at">var_index =</span> <span class="st">"famStruct"</span>, <span class="at">return_grid =</span> <span class="cn">TRUE</span>, <span class="at">num_trees =</span> bestNTree)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         famStruct         y
1              Dad -3.057205
2  Dad &amp; Fem Guard -2.856864
3              Mom -3.275981
4        Mom &amp; Dad -3.369352
5 Mom &amp; Male Guard -3.147131
6            Other -3.146190</code></pre>
</div>
</div>
<p>The highs and lows in the plots seem to be striking, but note that the vertical scale is on the log odds scale. For example, the range of values for <code>pctMinor</code> is from -3.38 to -3.32, a difference of 0.06. This implies that the odds of dropout for the lowest category is 6% smaller (<span class="math inline">\(e^{-0.06}-1=-0.058\)</span>) than the dropout rate for the highest category. To avoid overinterpreting the changes in the vertical scale, the following plot makes the vertical scales identical in all nine figures.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>), <span class="at">mai=</span><span class="fu">c</span>(<span class="fl">0.7</span>,<span class="fl">0.6</span>,<span class="fl">0.1</span>,<span class="fl">0.1</span>))</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>ylim <span class="ot">&lt;-</span> <span class="fu">plot</span>(gbm1, <span class="at">var_index=</span><span class="st">"ses"</span>, <span class="at">return_grid =</span> <span class="cn">TRUE</span>, </span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">num_trees =</span> bestNTree)<span class="sc">$</span>y <span class="sc">|&gt;</span> </span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>        <span class="fu">range</span>()</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(xj <span class="cf">in</span> relInf<span class="sc">$</span>var[<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>])</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(gbm1, <span class="at">var_index =</span> xj, <span class="at">ylim=</span>ylim, <span class="at">num_trees =</span> bestNTree)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-partialDependenceDropout2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-partialDependenceDropout2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-partialDependenceDropout2-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-partialDependenceDropout2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: Partial dependence of student features on dropout risk, common vertical scale
</figcaption>
</figure>
</div>
</div>
</div>
<p><code>plot()</code> also allows the user to examine two-way relationships. The same partial dependence idea holds, but just fixes two feature values at a time. <span class="math display">\[
\hat f_{j_1j_2}(x_1,x_2) = \frac{1}{n} \sum_{i=1}^n \hat f(x_{j_1}=x_1, x_{j_2}=x_2, \mathbf{x}_{-j_1,-j_2}=\mathbf{x}_{i,(-j_1,-j_2)})
\]</span> We can examine whether how the relationship between SES and dropouts varies by family structure.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gbm1, <span class="at">var_index =</span> <span class="fu">c</span>(<span class="st">"ses"</span>,<span class="st">"famStruct"</span>), <span class="at">num_trees =</span> bestNTree)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-partialDependenceDropout3" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-partialDependenceDropout3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L8-boosting_files/figure-html/fig-partialDependenceDropout3-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-partialDependenceDropout3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: Partial dependence of the SES and family structure interaction on dropout risk
</figcaption>
</figure>
</div>
</div>
</div>
<p><code>gbm</code> can also compute Friedman-Popescu’s <span class="math inline">\(H\)</span>-statistic to measure the size of an interaction effect for these complex non-linear models <span class="citation" data-cites="FriedmanPopescu2008">(<a href="#ref-FriedmanPopescu2008" role="doc-biblioref">Friedman and Popescu 2008</a>)</span>. <span class="math display">\[
H_{jk}^2= \frac{\sum_{i=1}^n \left(\hat f_{jk}(x_{ij},x_{ik}) - \hat f_{j}(x_j) - \hat f_k(x_k)\right)^2}
               {\sum_{i=1}^n \hat f_{jk}^2(x_{ij},x_{ik})}
\]</span> This captures the fraction of variance of <span class="math inline">\(\hat f_{jk}(x_{ij},x_{ik})\)</span> that is not captured by <span class="math inline">\(\hat f_{j}(x_j) + \hat f_k(x_k)\)</span>. Values near 0 mean no/little interaction effect. Note that <code>interact()</code> returns the value <span class="math inline">\(H\)</span> and not <span class="math inline">\(H^2\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>gbm1<span class="sc">$</span>variables<span class="sc">$</span>var_names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "typeSchool"   "urbanicity"   "region"       "pctMinor"     "pctFreeLunch"
 [6] "female"       "race"         "ses"          "parentEd"     "famSize"     
[11] "famStruct"    "parMarital"   "famIncome"    "langHome"    </code></pre>
</div>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="fu">interact</span>(gbm1, <span class="at">data=</span>nels0, <span class="at">var_indices =</span> <span class="fu">c</span>(<span class="dv">8</span>,<span class="dv">11</span>), </span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">num_trees =</span> bestNTree)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.07980591</code></pre>
</div>
</div>
</section>
</section>
<section id="summary" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Summary</h1>
<p>We have covered key concepts for “sparse” machine learning, focusing on <span class="math inline">\(L_1\)</span> regularization and boosting algorithms.</p>
<ol type="1">
<li><strong><span class="math inline">\(L_1\)</span> Regularization and Sparse Models</strong>:
<ul>
<li><span class="math inline">\(L_1\)</span> regularization penalizes the absolute size of coefficients to promote the selection of only the most important features</li>
<li>LASSO eliminates irrelevant features while preserving predictive accuracy</li>
<li>The coefficient paths appear to be linear segments the weight on the <span class="math inline">\(L_1\)</span> penalty (<span class="math inline">\(\lambda\)</span>) changes</li>
</ul></li>
<li><strong>Forward Stagewise Selection</strong>:
<ul>
<li>We can incrementally build models by iteratively adding features that most reduce the prediction error</li>
<li>The coefficient path was exactly the same as the LASSO, but with a much more computational efficient approach. This equivalency is best exemplified with the Least Angle Regression (LARS) algorithm</li>
</ul></li>
<li><strong>Boosting Algorithms</strong>:
<ul>
<li>Gradient boosting is a flexible method using regression trees as basis functions and using a forward stagewise approach to get <span class="math inline">\(L_1\)</span>-like sparsity</li>
<li><strong>LogitBoost</strong> uses the Bernoulli log-likelihood for classification tasks</li>
<li><strong>AdaBoost</strong> was the original boosting algorithm that minimized an exponential loss function acting as an upper bound on misclassification rates</li>
</ul></li>
<li><strong>Generalized Boosted Models (gbm3)</strong>:
<ul>
<li>Used the <code>gbm3</code> package to test out boosting on real datasets</li>
<li>Use cross-validation to select the shrinkage and the number of iterations</li>
<li>Measured the relative influence of individual features</li>
<li>Examine partial dependence plots to show relationships between features and the outcome</li>
</ul></li>
</ol>
<p>Since about 2005, the use of <span class="math inline">\(L_1\)</span> penalties has become a widespread method for fitting machine learning data involving a large collection of candidate predictors. Boosting is one of the best off-the-shelf prediction models, requiring little to no tuning to get strong predictive performance. More recent implementations are worth exploring such as <code>xgboost</code>.</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-XGBoost" class="csl-entry" role="listitem">
Chen, Tianqi, and Carlos Guestrin. 2016. <span>“XGBoost: A Scalable Tree Boosting System.”</span> In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 785–94. KDD ’16. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/2939672.2939785">https://doi.org/10.1145/2939672.2939785</a>.
</div>
<div id="ref-LeastAngleRegression" class="csl-entry" role="listitem">
Efron, Brad, Trevor Hastie, Iain Johnstone, and Rob Tibshirani. 2004. <span>“Least Angle Regression.”</span> <em>Annals of Statistics</em> 32 (2): 407–99.
</div>
<div id="ref-Elkan" class="csl-entry" role="listitem">
Elkan, C. 1997. <span>“Boosting and Na<span>ı̈</span>ve <span>Bayes</span> Learning.”</span> CS97-557. University of California, San Diego.
</div>
<div id="ref-FS" class="csl-entry" role="listitem">
Freund, Y., and R. Schapire. 1997. <span>“A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.”</span> <em>Journal of Computer and System Sciences</em> 55 (1): 119–39.
</div>
<div id="ref-GradientBoost" class="csl-entry" role="listitem">
Friedman, J. H. 2001. <span>“Greedy Function Approximation: A Gradient Boosting Machine.”</span> <em>The Annals of Statistics</em> 29 (5): 1189–1232.
</div>
<div id="ref-StochasticBoost" class="csl-entry" role="listitem">
———. 2002. <span>“Stochastic Gradient Boosting.”</span> <em>Computational Statistics and Data Analysis</em> 38 (4): 367–78.
</div>
<div id="ref-FHT" class="csl-entry" role="listitem">
Friedman, J. H., T. Hastie, and R. Tibshirani. 2000. <span>“Additive Logistic Regression: A Statistical View of Boosting (with Discussion).”</span> <em>Annals of Statistics</em> 28 (2): 337–74.
</div>
<div id="ref-FriedmanPopescu2008" class="csl-entry" role="listitem">
Friedman, J. H., and Bogdan E. Popescu. 2008. <span>“<span class="nocase">Predictive learning via rule ensembles</span>.”</span> <em>The Annals of Applied Statistics</em> 2 (3): 916–54. <a href="https://doi.org/10.1214/07-AOAS148">https://doi.org/10.1214/07-AOAS148</a>.
</div>
<div id="ref-Hast:Tibs:2001" class="csl-entry" role="listitem">
Hastie, T., R. Tibshirani, and J. H. Friedman. 2001. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer-Verlag.
</div>
<div id="ref-LightGBM:2017" class="csl-entry" role="listitem">
Ke, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. <span>“LightGBM: A Highly Efficient Gradient Boosting Decision Tree.”</span> <em>Advances in Neural Information Processing Systems</em> 30: 3149–57.
</div>
<div id="ref-RealAdaBoost" class="csl-entry" role="listitem">
Schapire, Robert E., and Yoram Singer. 1999. <span>“Improved Boosting Algorithms Using Confidence-Rated Predictions.”</span> <em>Machine Learning</em> 37 (3): 297–336.
</div>
<div id="ref-LASSO" class="csl-entry" role="listitem">
Tibshirani, Rob. 1995. <span>“Regression Selection and Shrinkage via the Lasso.”</span> <em>Journal of the Royal Statistical Society, Series B</em> 57: 267–88.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>